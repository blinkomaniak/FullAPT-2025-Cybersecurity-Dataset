# Remote Server Setup Guide for Dataset Evaluation Pipeline

This guide provides the exact folder structure needed to run the dataset evaluation pipeline smoothly on a remote server. This setup supports all model types (SAE, LSTM-SAE, Isolation Forest, etc.).

## ğŸ“‚ Complete Directory Structure

```
research/                              # Project root directory
â”œâ”€â”€ analysis/                          # ML pipeline scripts and configurations
â”‚   â”œâ”€â”€ sae/                           # SAE pipeline scripts
â”‚   â”‚   â”œâ”€â”€ 01-aggregation.py
â”‚   â”‚   â”œâ”€â”€ 02-encoding-bigbase-sae.py
â”‚   â”‚   â”œâ”€â”€ 02-encoding-unraveled-sae.py
â”‚   â”‚   â”œâ”€â”€ 03-training-sae.py
â”‚   â”‚   â”œâ”€â”€ 04-model-evaluation-sae.py
â”‚   â”‚   â”œâ”€â”€ 05-result-vis.py
â”‚   â”‚   â”œâ”€â”€ run_sae_pipeline.py        # Pipeline orchestrator
â”‚   â”‚   â”œâ”€â”€ run_sae.sh                 # Shell wrapper
â”‚   â”‚   â”œâ”€â”€ README_PIPELINE.md
â”‚   â”‚   â””â”€â”€ PATH_VERIFICATION_REPORT.md
â”‚   â”œâ”€â”€ lstm-sae/                      # LSTM-SAE pipeline scripts
â”‚   â”‚   â”œâ”€â”€ 01-aggregation.py
â”‚   â”‚   â”œâ”€â”€ 02-encoding-lstm-sae.py
â”‚   â”‚   â”œâ”€â”€ 03-training-lstm-sae.py
â”‚   â”‚   â”œâ”€â”€ 04-model-evaluation-lstm-sae.py
â”‚   â”‚   â”œâ”€â”€ 05-result-vis-lstm-sae.py
â”‚   â”‚   â”œâ”€â”€ run_lstm_sae_pipeline.py
â”‚   â”‚   â””â”€â”€ smart_early_stopping.py
â”‚   â”œâ”€â”€ gru-sae/                       # GRU-SAE pipeline scripts
â”‚   â”‚   â”œâ”€â”€ 01-aggregation.py
â”‚   â”‚   â”œâ”€â”€ 02-encoding-gru-sae.py
â”‚   â”‚   â”œâ”€â”€ 03-training-gru-sae.py
â”‚   â”‚   â”œâ”€â”€ 04-model-evaluation-gru-sae.py
â”‚   â”‚   â”œâ”€â”€ 05-result-vis-gru-sae.py
â”‚   â”‚   â”œâ”€â”€ run_gru_sae_pipeline.py
â”‚   â”‚   â””â”€â”€ smart_early_stopping.py
â”‚   â”œâ”€â”€ iforest/                       # Isolation Forest pipeline scripts
â”‚   â”‚   â”œâ”€â”€ 01-aggregation.py
â”‚   â”‚   â”œâ”€â”€ 02-encoding-bigbase-iforest.py
â”‚   â”‚   â”œâ”€â”€ 02-encoding-unraveled-iforest.py
â”‚   â”‚   â”œâ”€â”€ 03-training-iforest.py
â”‚   â”‚   â”œâ”€â”€ 04-model-evaluation-iforest.py
â”‚   â”‚   â”œâ”€â”€ 05-feature-space-iforest.py
â”‚   â”‚   â”œâ”€â”€ run_iforest_pipeline.py
â”‚   â”‚   â””â”€â”€ run_iforest.sh
â”‚   â”œâ”€â”€ config/                        # Configuration files
â”‚   â”‚   â”œâ”€â”€ schema-bigbase-sae-v1.json
â”‚   â”‚   â”œâ”€â”€ schema-bigbase-sae-v2.json
â”‚   â”‚   â”œâ”€â”€ schema-bigbase-lstm-sae-v1.json
â”‚   â”‚   â”œâ”€â”€ schema-bigbase-lstm-sae-v2.json
â”‚   â”‚   â”œâ”€â”€ schema-bigbase-gru-sae-v1.json
â”‚   â”‚   â”œâ”€â”€ schema-bigbase-gru-sae-v2.json
â”‚   â”‚   â”œâ”€â”€ schema-unraveled-sae-v1.json
â”‚   â”‚   â”œâ”€â”€ schema-unraveled-lstm-sae-v1.json
â”‚   â”‚   â”œâ”€â”€ schema-unraveled-gru-sae-v1.json
â”‚   â”‚   â”œâ”€â”€ encoding-bigbase-sae-v1.json
â”‚   â”‚   â”œâ”€â”€ encoding-bigbase-sae-v2.json
â”‚   â”‚   â”œâ”€â”€ encoding-bigbase-lstm-sae-v1.json
â”‚   â”‚   â”œâ”€â”€ encoding-bigbase-lstm-sae-v2.json
â”‚   â”‚   â”œâ”€â”€ encoding-bigbase-gru-sae-v1.json
â”‚   â”‚   â”œâ”€â”€ encoding-bigbase-gru-sae-v2.json
â”‚   â”‚   â”œâ”€â”€ encoding-bigbase-iforest-v1.json
â”‚   â”‚   â”œâ”€â”€ encoding-unraveled-sae-v1.json
â”‚   â”‚   â”œâ”€â”€ encoding-unraveled-lstm-sae-v1.json
â”‚   â”‚   â”œâ”€â”€ encoding-unraveled-gru-sae-v1.json
â”‚   â”‚   â””â”€â”€ encoding-unraveled-iforest-v1.json
â”‚   â”œâ”€â”€ comparative-evaluation/        # Cross-model analysis scripts
â”‚   â”‚   â”œâ”€â”€ comparative_analysis.py
â”‚   â”‚   â”œâ”€â”€ dataset_quality_analyzer.py
â”‚   â”‚   â”œâ”€â”€ roc_comparison_generator.py
â”‚   â”‚   â””â”€â”€ simple_quality_analyzer.py
â”‚   â”œâ”€â”€ utils/                         # Utility modules
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ paths.py                   # Path management utilities
â”‚   â”‚   â”œâ”€â”€ config.py                  # Configuration loading
â”‚   â”‚   â””â”€â”€ encoding.py                # Data preprocessing utilities
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ datasets/                          # Raw cybersecurity datasets (REQUIRED - must be uploaded)
â”‚   â”œâ”€â”€ bigbase/                       # Windows event logs dataset
â”‚   â”‚   â”œâ”€â”€ dataset-01.csv
â”‚   â”‚   â”œâ”€â”€ dataset-02.csv
â”‚   â”‚   â”œâ”€â”€ ...
â”‚   â”‚   â”œâ”€â”€ dataset-50.csv             # 50 CSV files total
â”‚   â”‚   â””â”€â”€ reports/                   # Caldera attack reports
â”‚   â”‚       â”œâ”€â”€ cal-report-01.json
â”‚   â”‚       â”œâ”€â”€ entry_params-01.json
â”‚   â”‚       â””â”€â”€ ...                    # Additional report files
â”‚   â”œâ”€â”€ unraveled/                     # Multi-modal cybersecurity data
â”‚   â”‚   â”œâ”€â”€ host-logs/                 # Host log data
â”‚   â”‚   â”‚   â”œâ”€â”€ audit/
â”‚   â”‚   â”‚   â”œâ”€â”€ auth/
â”‚   â”‚   â”‚   â”œâ”€â”€ filebeat/
â”‚   â”‚   â”‚   â”œâ”€â”€ syslog/
â”‚   â”‚   â”‚   â””â”€â”€ windows/
â”‚   â”‚   â”œâ”€â”€ network-flows/             # Network flow data by weeks/days
â”‚   â”‚   â”‚   â”œâ”€â”€ Week1_Day1-2_05262021-05272021/
â”‚   â”‚   â”‚   â”œâ”€â”€ Week1_Day3_05282021/
â”‚   â”‚   â”‚   â”œâ”€â”€ Week1_Day4_05292021/
â”‚   â”‚   â”‚   â””â”€â”€ ...                    # Additional week/day directories
â”‚   â”‚   â””â”€â”€ nids/                      # Network intrusion detection data
â”‚   â”‚       â””â”€â”€ all_snort
â”‚   â””â”€â”€ dapt2020/                      # DAPT2020 network flow dataset
â”‚       â”œâ”€â”€ README.md
â”‚       â”œâ”€â”€ enp0s3-monday-pvt.pcap_Flow.csv
â”‚       â”œâ”€â”€ enp0s3-monday.pcap_Flow.csv
â”‚       â”œâ”€â”€ util.py
â”‚       â””â”€â”€ ...                        # Additional PCAP flow files
â”œâ”€â”€ data/                              # Pipeline outputs (auto-created)
â”‚   â”œâ”€â”€ processed/                     # Stage 1 outputs
â”‚   â”‚   â”œâ”€â”€ bigbase-sae-v1.parquet
â”‚   â”‚   â””â”€â”€ unraveled-sae-v1.parquet
â”‚   â””â”€â”€ encoded/                       # Stage 2 outputs
â”‚       â”œâ”€â”€ bigbase-sae-v1/
â”‚       â”‚   â”œâ”€â”€ X_train_encoded.npz
â”‚       â”‚   â”œâ”€â”€ X_val_encoded.npz
â”‚       â”‚   â”œâ”€â”€ X_test_encoded.npz
â”‚       â”‚   â”œâ”€â”€ y_train.npy
â”‚       â”‚   â”œâ”€â”€ y_val.npy
â”‚       â”‚   â”œâ”€â”€ y_test.npy
â”‚       â”‚   â””â”€â”€ column_transformer.joblib
â”‚       â””â”€â”€ unraveled-sae-v1/
â”‚           â””â”€â”€ [same structure as bigbase]
â”œâ”€â”€ models/                            # Stage 3 outputs (auto-created)
â”‚   â””â”€â”€ (model files created by pipeline scripts)
â”œâ”€â”€ artifacts/                         # Analysis outputs (auto-created)
â”‚   â”œâ”€â”€ eval/                          # Model evaluation results
â”‚   â”‚   â”œâ”€â”€ sae/
â”‚   â”‚   â”œâ”€â”€ lstm-sae/
â”‚   â”‚   â”œâ”€â”€ gru-sae/
â”‚   â”‚   â””â”€â”€ iforest/
â”‚   â”œâ”€â”€ plots/                         # Visualizations
â”‚   â”œâ”€â”€ metadata/                      # Dataset metadata
â”‚   â”œâ”€â”€ comparative/                   # Cross-model analysis
â”‚   â”œâ”€â”€ quality_analysis/              # Dataset quality reports
â”‚   â””â”€â”€ roc_comparison/                # ROC curve comparisons
â”œâ”€â”€ docs/                              # Documentation files
â”œâ”€â”€ pdfs/                              # Research papers and reports
â”œâ”€â”€ others/                            # Miscellaneous files
â”œâ”€â”€ data-raw/                          # Raw data collection artifacts
â”œâ”€â”€ dataset-venv/                      # Python virtual environment
â”œâ”€â”€ CLAUDE.md                          # Project documentation for Claude Code
â””â”€â”€ REMOTE_SERVER_SETUP.md             # This file
```

## ğŸš€ Setup Instructions

### 1. Create Base Directory Structure
```bash
# Create the main project directory
mkdir -p dataset-eval
cd dataset-eval

# Create required subdirectories that won't be auto-created
mkdir -p analysis/{sae,lstm-sae,gru-sae,iforest,config,utils,comparative-evaluation}
mkdir -p datasets/{bigbase,unraveled,dapt2020}
mkdir -p data/{processed,encoded} models artifacts/{eval,plots,metadata,comparative,quality_analysis,roc_comparison}
mkdir -p docs pdfs others data-raw

# Create empty __init__.py files for Python imports
touch analysis/__init__.py
touch analysis/utils/__init__.py
```

### 2. Upload Code Files
Transfer these files to their respective directories:

**Pipeline Scripts** â†’ `analysis/sae/` (or other model directories):
- `01-aggregation.py`
- `02-encoding-bigbase-sae.py`  
- `02-encoding-unraveled-sae.py`
- `03-training-sae.py`
- `04-model-evaluation-sae.py`
- `05-result-vis.py`
- `run_sae_pipeline.py`
- `run_sae.sh`
- `README_PIPELINE.md`

**Configuration Files** â†’ `analysis/config/`:
- Schema configs: `schema-bigbase-{model}-v{version}.json`
- Schema configs: `schema-unraveled-{model}-v{version}.json`
- Encoding configs: `encoding-bigbase-{model}-v{version}.json`
- Encoding configs: `encoding-unraveled-{model}-v{version}.json`
- Where {model} = sae, lstm-sae, gru-sae, iforest
- Where {version} = v1, v2, etc.

**Utility Modules** â†’ `analysis/utils/`:
- `paths.py`
- `config.py`
- `encoding.py`
- `__init__.py`

**Project Documentation** â†’ root directory:
- `CLAUDE.md`
- `REMOTE_SERVER_SETUP.md`

### 3. Upload Raw Datasets (CRITICAL)
This is the most important step - your raw CSV data must be placed exactly here:

**Bigbase Dataset** â†’ `datasets/bigbase/`:
```bash
datasets/bigbase/
â”œâ”€â”€ dataset-01.csv
â”œâ”€â”€ dataset-02.csv
â”œâ”€â”€ ...
â””â”€â”€ dataset-50.csv
```

**Unraveled Dataset** â†’ `datasets/unraveled/`:
```bash
datasets/unraveled/
â”œâ”€â”€ host-logs/
â”‚   â”œâ”€â”€ audit/
â”‚   â”œâ”€â”€ auth/
â”‚   â”œâ”€â”€ filebeat/
â”‚   â”œâ”€â”€ syslog/
â”‚   â””â”€â”€ windows/
â”œâ”€â”€ network-flows/
â”‚   â”œâ”€â”€ Week1_Day1-2_05262021-05272021/
â”‚   â”œâ”€â”€ Week1_Day3_05282021/
â”‚   â””â”€â”€ [additional week/day directories]
â””â”€â”€ nids/
```

**DAPT2020 Dataset** â†’ `datasets/dapt2020/`:
```bash
datasets/dapt2020/
â”œâ”€â”€ README.md
â”œâ”€â”€ enp0s3-monday-pvt.pcap_Flow.csv
â”œâ”€â”€ enp0s3-monday.pcap_Flow.csv
â””â”€â”€ [additional PCAP flow files]
```

### 4. Make Scripts Executable
```bash
cd analysis/sae/
chmod +x run_sae.sh run_sae_pipeline.py
```

## âœ… Validation Checklist

Before running the pipeline, verify these critical requirements:

### Required Directories with Content:
- [ ] `datasets/bigbase/` contains 50 CSV files (dataset-01.csv to dataset-50.csv) + reports/
- [ ] `datasets/unraveled/` contains host-logs/, network-flows/, nids/ subdirectories
- [ ] `datasets/dapt2020/` contains PCAP flow CSV files + util.py
- [ ] `analysis/sae/` contains all 5 pipeline scripts + orchestrator + shell wrapper
- [ ] `analysis/lstm-sae/` contains LSTM-SAE pipeline scripts + orchestrator + early stopping
- [ ] `analysis/gru-sae/` contains GRU-SAE pipeline scripts + orchestrator + early stopping
- [ ] `analysis/iforest/` contains Isolation Forest pipeline scripts + orchestrator
- [ ] `analysis/comparative-evaluation/` contains cross-model analysis scripts
- [ ] `analysis/config/` contains all model-specific JSON configuration files
- [ ] `analysis/utils/` contains paths.py, config.py, encoding.py

### Docker Environment:
- [ ] Docker container has all required packages (tensorflow, scikit-learn, pandas, numpy, etc.)
- [ ] Container has access to project directory

### File Permissions:
- [ ] Pipeline scripts are executable (chmod +x)

## ğŸš¦ Quick Start Test

Once everything is set up, test with:

```bash
# Navigate to SAE directory
cd analysis/sae/

# Test pipeline in Docker container
./run_sae.sh bigbase v1

# Or test other models
cd ../lstm-sae/
python run_lstm_sae_pipeline.py --dataset unraveled --version v1

cd ../iforest/
./run_iforest.sh bigbase v1
```

## ğŸ“ Notes

- **Auto-created directories**: `data/`, `models/`, `artifacts/` are created by setup script
- **Multi-model support**: This structure supports SAE, LSTM-SAE, GRU-SAE, and Isolation Forest models
- **Critical uploads**: Only `datasets/` and code files need manual upload
- **Docker environment**: All Python dependencies should be pre-installed in the container
- **Pipeline orchestration**: Each model has its own pipeline orchestrator (SAE has shell wrapper)
- **Configuration files**: Include model-specific suffixes (-sae, -lstm-sae, -gru-sae, -iforest) for clarity

## ğŸ› Common Issues

1. **"Raw data directory not found"**: Check that datasets are in exact paths shown above
2. **"Module not found"**: Ensure script runs from project root and Docker container has Python dependencies
3. **"Permission denied"**: Run `chmod +x` on shell scripts (run_sae.sh)
4. **Shell script syntax errors**: Ensure proper bash comment syntax (# not """)
5. **"Config file not found"**: Use exact config file names with model suffixes (-sae, -lstm)

## ğŸ“Š Expected Storage Requirements

- **Raw datasets**: 2-5 GB (depending on dataset size)
- **Processed data**: 1-3 GB per model/dataset combination
- **Trained models**: 100-500 MB per model
- **Evaluation artifacts**: 50-200 MB per model
- **Logs**: 10-50 MB per pipeline run
- **Total**: 10-20 GB for complete pipeline with multiple models