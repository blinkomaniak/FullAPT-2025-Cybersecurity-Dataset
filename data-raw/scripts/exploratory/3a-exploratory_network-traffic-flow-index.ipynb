{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Traffic Flow Data Exploratory Analysis\n",
    "\n",
    "This notebook provides comprehensive exploratory analysis of the network traffic JSONL file to understand its structure, data distribution, and characteristics before implementing the main processing logic in notebook #3.\n",
    "\n",
    "**Objective**: Examine the network traffic flow data structure through random sampling and statistical analysis to inform optimal processing strategies.\n",
    "\n",
    "**Target File**: `-ds-logs-network_traffic-flow-default-2025-05-04-000001.jsonl`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Initial Setup: Logging System and Output Organization\n",
    "\n",
    "**What this section does**: Sets up a comprehensive logging system to capture all analysis outputs in organized files for easy inspection and review.\n",
    "\n",
    "**Simple explanation**: \n",
    "- Creates output directories specifically for 3a network-flows analysis\n",
    "- Generates timestamped log files so you can track when analysis was run\n",
    "- Sets up automatic output capture so everything gets saved to files\n",
    "- Organizes outputs in `outputs/3a-network-flows/` directory structure\n",
    "\n",
    "**Paraphrasing tips**:\n",
    "- \"What type of analysis am I running?\" ‚Üí Sets `ANALYSIS_TYPE = \"3a-network-flows\"`\n",
    "- \"Where will my results be saved?\" ‚Üí Creates organized folder structure in `outputs/3a-network-flows/`\n",
    "- \"How can I review what happened?\" ‚Üí All print statements get saved to timestamped log files\n",
    "\n",
    "**Key insight**: This creates a professional logging infrastructure that makes it easy to review analysis results later and compare with 3b structure consistency analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Analysis type: 3a-network-flows\n",
      "üìÅ Output directory: outputs/3a-network-flows\n",
      "üìä Log file: outputs/3a-network-flows/3a-network-flows_exploratory_analysis_20250629_064647.log\n",
      "üíæ Data file: outputs/3a-network-flows/3a-network-flows_sample_data_20250629_064647.json\n",
      "‚úÖ Logging system initialized for 3a network-flows exploratory analysis!\n",
      "üìÇ All outputs will be organized in: outputs/3a-network-flows/\n",
      "üîç Use 'with capture_output(\"Section Name\", section_number):' to log outputs\n",
      "üíæ Use 'save_data_structure(data, \"description\", \"section\")' for complex data\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "from datetime import datetime\n",
    "from contextlib import contextmanager\n",
    "\n",
    "# Define analysis type and create organized output structure\n",
    "ANALYSIS_TYPE = \"3a-network-flows\"  # This will be \"sysmon\" for sysmon analysis\n",
    "outputs_base_dir = \"outputs\"\n",
    "analysis_outputs_dir = f\"{outputs_base_dir}/{ANALYSIS_TYPE}\"\n",
    "\n",
    "# Create output directories\n",
    "if not os.path.exists(outputs_base_dir):\n",
    "    os.makedirs(outputs_base_dir)\n",
    "    print(f\"‚úÖ Created base outputs directory: {outputs_base_dir}\")\n",
    "\n",
    "if not os.path.exists(analysis_outputs_dir):\n",
    "    os.makedirs(analysis_outputs_dir)\n",
    "    print(f\"‚úÖ Created analysis outputs directory: {analysis_outputs_dir}\")\n",
    "\n",
    "# Generate timestamped filenames with descriptive names\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "log_filename = f\"{analysis_outputs_dir}/{ANALYSIS_TYPE}_exploratory_analysis_{timestamp}.log\"\n",
    "data_filename = f\"{analysis_outputs_dir}/{ANALYSIS_TYPE}_sample_data_{timestamp}.json\"\n",
    "\n",
    "print(f\"üìù Analysis type: {ANALYSIS_TYPE}\")\n",
    "print(f\"üìÅ Output directory: {analysis_outputs_dir}\")\n",
    "print(f\"üìä Log file: {log_filename}\")\n",
    "print(f\"üíæ Data file: {data_filename}\")\n",
    "\n",
    "# Initialize log file with header\n",
    "with open(log_filename, 'w', encoding='utf-8') as log_file:\n",
    "    log_file.write(f\"NETWORK TRAFFIC FLOW EXPLORATORY ANALYSIS\\n\")\n",
    "    log_file.write(f\"Analysis Type: {ANALYSIS_TYPE.upper()}\\n\")\n",
    "    log_file.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    log_file.write(f\"Target File: -ds-logs-network_traffic-flow-default-2025-05-04-000001.jsonl\\n\")\n",
    "    log_file.write(f\"{'='*80}\\n\\n\")\n",
    "\n",
    "# Initialize data file with metadata structure\n",
    "with open(data_filename, 'w', encoding='utf-8') as data_file:\n",
    "    json.dump({\"analysis_metadata\": {\"timestamp\": timestamp, \"analysis_type\": ANALYSIS_TYPE, \"samples\": []}}, data_file, indent=2)\n",
    "\n",
    "class LogCapture:\n",
    "    def __init__(self, log_filename):\n",
    "        self.log_filename = log_filename\n",
    "        self.original_stdout = sys.stdout\n",
    "    \n",
    "    def write(self, text):\n",
    "        self.original_stdout.write(text)\n",
    "        self.original_stdout.flush()\n",
    "        \n",
    "        with open(self.log_filename, 'a', encoding='utf-8') as log_file:\n",
    "            log_file.write(text)\n",
    "            log_file.flush()\n",
    "    \n",
    "    def flush(self):\n",
    "        self.original_stdout.flush()\n",
    "\n",
    "log_capture = LogCapture(log_filename)\n",
    "\n",
    "def log_section(section_name, section_number=None):\n",
    "    header = f\"\\n{'='*80}\\n\"\n",
    "    if section_number:\n",
    "        header += f\"SECTION {section_number}: {section_name.upper()}\\n\"\n",
    "    else:\n",
    "        header += f\"{section_name.upper()}\\n\"\n",
    "    header += f\"{'='*80}\\n\\n\"\n",
    "    \n",
    "    with open(log_filename, 'a', encoding='utf-8') as log_file:\n",
    "        log_file.write(header)\n",
    "    \n",
    "    print(header.strip())\n",
    "\n",
    "def save_data_structure(data, description, section_name):\n",
    "    \"\"\"\n",
    "    Save complex data structures to organized JSON file for detailed inspection.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(data_filename, 'r', encoding='utf-8') as f:\n",
    "            existing_data = json.load(f)\n",
    "        \n",
    "        new_entry = {\n",
    "            \"section\": section_name,\n",
    "            \"description\": description,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"data\": data\n",
    "        }\n",
    "        existing_data[\"analysis_metadata\"][\"samples\"].append(new_entry)\n",
    "        \n",
    "        with open(data_filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(existing_data, f, indent=2, default=str)\n",
    "        \n",
    "        print(f\"üíæ Saved data structure: {description}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Warning: Could not save data structure - {str(e)}\")\n",
    "\n",
    "@contextmanager\n",
    "def capture_output(section_name, section_number=None):\n",
    "    \"\"\"\n",
    "    Context manager to capture both console output and logging with organized section headers.\n",
    "    \"\"\"\n",
    "    log_section(section_name, section_number)\n",
    "    \n",
    "    original_stdout = sys.stdout\n",
    "    sys.stdout = log_capture\n",
    "    \n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        sys.stdout = original_stdout\n",
    "        \n",
    "        with open(log_filename, 'a', encoding='utf-8') as log_file:\n",
    "            log_file.write(f\"\\n{'-'*60} END SECTION {'-'*60}\\n\\n\")\n",
    "\n",
    "print(\"‚úÖ Logging system initialized for 3a network-flows exploratory analysis!\")\n",
    "print(f\"üìÇ All outputs will be organized in: {analysis_outputs_dir}/\")\n",
    "print(f\"üîç Use 'with capture_output(\\\"Section Name\\\", section_number):' to log outputs\")\n",
    "print(f\"üíæ Use 'save_data_structure(data, \\\"description\\\", \\\"section\\\")' for complex data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Import essential Python libraries for data exploration, JSON processing, and statistical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json  # For parsing JSONL records\n",
    "import random  # For random sampling of large datasets\n",
    "import os  # For file system operations\n",
    "from collections import Counter, defaultdict  # For frequency analysis and data aggregation\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "from pprint import pprint  # For pretty-printing complex data structures\n",
    "import sys  # For system-specific parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. File Path Setup and Validation\n",
    "\n",
    "Define the target JSONL file path and verify its existence before proceeding with the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ File found: -ds-logs-network_traffic-flow-default-2025-05-04-000001.jsonl\n",
      "üìä File size: 2309.17 MB (2,421,341,652 bytes)\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the network traffic JSONL file\n",
    "jsonl_file_path = '-ds-logs-network_traffic-flow-default-2025-05-04-000001.jsonl'\n",
    "\n",
    "# Verify that the file exists in the current directory\n",
    "if os.path.exists(jsonl_file_path):\n",
    "    # Get file size in MB for initial assessment\n",
    "    file_size_bytes = os.path.getsize(jsonl_file_path)\n",
    "    file_size_mb = file_size_bytes / (1024 * 1024)\n",
    "    print(f\"‚úÖ File found: {jsonl_file_path}\")\n",
    "    print(f\"üìä File size: {file_size_mb:.2f} MB ({file_size_bytes:,} bytes)\")\n",
    "else:\n",
    "    print(f\"‚ùå File not found: {jsonl_file_path}\")\n",
    "    print(\"üìÅ Available files in current directory:\")\n",
    "    # List all JSONL files in current directory for debugging\n",
    "    for file in os.listdir('.'):\n",
    "        if file.endswith('.jsonl'):\n",
    "            print(f\"   - {file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initial Data Loading and Line Count\n",
    "\n",
    "Count total number of records in the JSONL file to understand dataset size and plan sampling strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples_to_analyze = 200_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SECTION 3: INITIAL DATA LOADING AND LINE COUNT\n",
      "================================================================================\n",
      "üìà Total records in dataset: 1,090,212\n",
      "üéØ Sampling strategy: Will analyze 200000 random samples\n"
     ]
    }
   ],
   "source": [
    "if 'capture_output' in globals():\n",
    "    with capture_output(\"Initial Data Loading and Line Count\", 3):\n",
    "        total_records = 0\n",
    "        try:\n",
    "            with open(jsonl_file_path, 'r', encoding='utf-8') as f:\n",
    "                for line_num, line in enumerate(f, 1):\n",
    "                    if line.strip():\n",
    "                        total_records += 1\n",
    "                \n",
    "            print(f\"üìà Total records in dataset: {total_records:,}\")\n",
    "            print(f\"üéØ Sampling strategy: Will analyze {min(num_samples_to_analyze, total_records)} random samples\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error reading file: {str(e)}\")\n",
    "            total_records = 0\n",
    "else:\n",
    "    total_records = 0\n",
    "    try:\n",
    "        with open(jsonl_file_path, 'r', encoding='utf-8') as f:\n",
    "            for line_num, line in enumerate(f, 1):\n",
    "                if line.strip():\n",
    "                    total_records += 1\n",
    "            \n",
    "        print(f\"üìà Total records in dataset: {total_records:,}\")\n",
    "        print(f\"üéØ Sampling strategy: Will analyze {min(num_samples_to_analyze, total_records)} random samples\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error reading file: {str(e)}\")\n",
    "        total_records = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Random Sampling Function\n",
    "\n",
    "Implement efficient random sampling to select representative records from the large JSONL dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_samples(file_path, sample_size=num_samples_to_analyze, max_records=None):\n",
    "    \"\"\"\n",
    "    Extract random samples from JSONL file using reservoir sampling algorithm.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the JSONL file\n",
    "        sample_size (int): Number of samples to collect\n",
    "        max_records (int): Maximum records to read (None for all)\n",
    "    \n",
    "    Returns:\n",
    "        list: List of parsed JSON objects\n",
    "    \"\"\"\n",
    "    samples = []  # List to store selected samples\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line_num, line in enumerate(f, 1):\n",
    "                # Skip empty lines\n",
    "                if not line.strip():\n",
    "                    continue\n",
    "                \n",
    "                # Stop if we've reached the maximum record limit\n",
    "                if max_records and line_num > max_records:\n",
    "                    break\n",
    "                \n",
    "                try:\n",
    "                    # Parse the JSON record\n",
    "                    record = json.loads(line)\n",
    "                    \n",
    "                    # Reservoir sampling: Always add first 'sample_size' records\n",
    "                    if len(samples) < sample_size:\n",
    "                        samples.append(record)\n",
    "                    else:\n",
    "                        # Randomly replace existing samples with decreasing probability\n",
    "                        replace_index = random.randint(0, line_num - 1)\n",
    "                        if replace_index < sample_size:\n",
    "                            samples[replace_index] = record\n",
    "                            \n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"‚ö†Ô∏è  JSON decode error on line {line_num}: {str(e)}\")\n",
    "                    continue\n",
    "                    \n",
    "        print(f\"‚úÖ Successfully collected {len(samples)} random samples\")\n",
    "        return samples\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during sampling: {str(e)}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Set random seed for reproducible sampling\n# random.seed(42)  # Fixed seed - always gives same samples\nrandom.seed()  # Use system time for true randomness each run\n\n# Collect random samples from the dataset\nsample_size = min(100, total_records)  # Sample up to 100 records or total if smaller\nrandom_samples = get_random_samples(jsonl_file_path, sample_size=sample_size)\n\nif random_samples:\n    print(f\"üìä Analysis ready with {len(random_samples)} samples\")\n    print(f\"üé≤ Random seed: {random.getstate()[1][0]} (for debugging)\")\n    print(f\"üîÑ Note: Using system time for true randomness - results will vary between runs\")\nelse:\n    print(\"‚ùå No samples collected - cannot proceed with analysis\")",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully collected 200000 random samples\n",
      "üìä Analysis ready with 200000 samples\n",
      "üé≤ Random seed: 42 (for reproducible results)\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducible sampling\n",
    "random.seed()\n",
    "\n",
    "# Collect random samples from the dataset\n",
    "sample_size = min(num_samples_to_analyze, total_records)  # Sample up to 100 records or total if smaller\n",
    "random_samples = get_random_samples(jsonl_file_path, sample_size=sample_size)\n",
    "\n",
    "if random_samples:\n",
    "    print(f\"üìä Analysis ready with {len(random_samples)} samples\")\n",
    "    print(f\"üé≤ Random seed: 42 (for reproducible results)\")\n",
    "else:\n",
    "    print(\"‚ùå No samples collected - cannot proceed with analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Basic Data Structure Analysis\n",
    "\n",
    "Examine the fundamental structure of network traffic records to understand data organization and hierarchy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SECTION 6: BASIC DATA STRUCTURE ANALYSIS\n",
      "================================================================================\n",
      "üìã BASIC DATA STRUCTURE ANALYSIS\n",
      "==================================================\n",
      "üîç Data type: <class 'dict'>\n",
      "üìè Number of top-level fields: 11\n",
      "üóùÔ∏è  Top-level fields:\n",
      "    1. agent                          (dict)\n",
      "    2. destination                    (dict)\n",
      "    3. elastic_agent                  (dict)\n",
      "    4. network_traffic                (dict)\n",
      "    5. source                         (dict)\n",
      "    6. network                        (dict)\n",
      "    7. @timestamp                     (str)\n",
      "    8. ecs                            (dict)\n",
      "    9. data_stream                    (dict)\n",
      "   10. host                           (dict)\n",
      "   11. event                          (dict)\n",
      "üìÑ Complete first sample structure:\n",
      "------------------------------\n",
      "{'@timestamp': '2025-05-04T12:39:15.847Z',\n",
      " 'agent': {'ephemeral_id': 'fdaf0b65-7a0d-49a7-90a6-5f011b283f17',\n",
      "           'id': 'dea09884-b943-42b8-a03b-2e87109e5297',\n",
      "           'name': 'theblock',\n",
      "           'type': 'packetbeat',\n",
      "           'version': '8.18.0'},\n",
      " 'data_stream': {'dataset': 'network_traffic.flow', 'namespace': 'default', 'type': 'logs'},\n",
      " 'destination': {'bytes': 7087,\n",
      "                 'ip': '10.2.0.20',\n",
      "                 'mac': '08-00-27-26-33-A5',\n",
      "                 'packets': 26,\n",
      "                 'port': 9200},\n",
      " 'ecs': {'version': '8.11.0'},\n",
      " 'elastic_agent': {'id': 'dea09884-b943-42b8-a03b-2e87109e5297',\n",
      "                   'snapshot': False,\n",
      "                   'version': '8.18.0'},\n",
      " 'event': {'action': 'network_flow',\n",
      "           'agent_id_status': 'verified',\n",
      "           'category': ['network'],\n",
      "           'duration': 16204538900,\n",
      "           'end': '2025-05-04T12:38:32.745Z',\n",
      "           'ingested': '2025-05-04T12:39:21Z',\n",
      "           'kind': 'event',\n",
      "           'start': '2025-05-04T12:38:16.540Z',\n",
      "           'type': ['connection']},\n",
      " 'host': {'architecture': 'x86_64',\n",
      "          'hostname': 'theblock',\n",
      "          'id': 'a978ae42-a0af-4cc7-9ec6-49eeab15067d',\n",
      "          'ip': ['fe80::ac69:cdb8:409a:9db8', '10.1.0.5'],\n",
      "          'mac': ['08-00-27-F9-13-75'],\n",
      "          'name': 'theblock',\n",
      "          'os': {'build': '17763.107',\n",
      "                 'family': 'windows',\n",
      "                 'kernel': '10.0.17763.107 (WinBuild.160101.0800)',\n",
      "                 'name': 'Windows 10 Pro for Workstations',\n",
      "                 'platform': 'windows',\n",
      "                 'type': 'windows',\n",
      "                 'version': '10.0'}},\n",
      " 'network': {'bytes': 68032,\n",
      "             'community_id': '1:p/eP4xM4zgi1RF3mnJg2GvG310c=',\n",
      "             'packets': 69,\n",
      "             'transport': 'tcp',\n",
      "             'type': 'ipv4'},\n",
      " 'network_traffic': {'flow': {'final': False,\n",
      "                              'id': 'EQQA////DP//////FP8BAAEIACcmM6UIACf5E3UKAgAUCgEABfAj3vg'}},\n",
      " 'source': {'bytes': 60945,\n",
      "            'ip': '10.1.0.5',\n",
      "            'mac': '08-00-27-F9-13-75',\n",
      "            'packets': 43,\n",
      "            'port': 63710}}\n",
      "üíæ Saved data structure: Complete First Sample Structure\n"
     ]
    }
   ],
   "source": [
    "if random_samples:\n",
    "    if 'capture_output' in globals() and 'save_data_structure' in globals():\n",
    "        with capture_output(\"Basic Data Structure Analysis\", 6):\n",
    "            first_sample = random_samples[0]\n",
    "            \n",
    "            print(\"üìã BASIC DATA STRUCTURE ANALYSIS\")\n",
    "            print(\"=\" * 50)\n",
    "            \n",
    "            print(f\"üîç Data type: {type(first_sample)}\")\n",
    "            print(f\"üìè Number of top-level fields: {len(first_sample)}\")\n",
    "            \n",
    "            print(f\"üóùÔ∏è  Top-level fields:\")\n",
    "            for i, key in enumerate(first_sample.keys(), 1):\n",
    "                field_type = type(first_sample[key]).__name__\n",
    "                print(f\"   {i:2d}. {key:<30} ({field_type})\")\n",
    "            \n",
    "            print(f\"üìÑ Complete first sample structure:\")\n",
    "            print(\"-\" * 30)\n",
    "            pprint(first_sample, depth=3, width=100)\n",
    "            \n",
    "            save_data_structure(first_sample, \"Complete First Sample Structure\", \"Basic Data Structure Analysis\")\n",
    "    else:\n",
    "        first_sample = random_samples[0]\n",
    "        \n",
    "        print(\"üìã BASIC DATA STRUCTURE ANALYSIS\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        print(f\"üîç Data type: {type(first_sample)}\")\n",
    "        print(f\"üìè Number of top-level fields: {len(first_sample)}\")\n",
    "        \n",
    "        print(f\"üóùÔ∏è  Top-level fields:\")\n",
    "        for i, key in enumerate(first_sample.keys(), 1):\n",
    "            field_type = type(first_sample[key]).__name__\n",
    "            print(f\"   {i:2d}. {key:<30} ({field_type})\")\n",
    "        \n",
    "        print(f\"üìÑ Complete first sample structure:\")\n",
    "        print(\"-\" * 30)\n",
    "        pprint(first_sample, depth=3, width=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Field Distribution Analysis\n",
    "\n",
    "Analyze the frequency and distribution of fields across all samples to identify consistent vs. optional fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SECTION 7: FIELD DISTRIBUTION ANALYSIS\n",
      "================================================================================\n",
      "üìä FIELD DISTRIBUTION ANALYSIS\n",
      "==================================================\n",
      "üìà Field frequency analysis (200000 samples):\n",
      "Field Name                     Count    Percentage   Primary Type   \n",
      "----------------------------------------------------------------------\n",
      "agent                          200000     100.0%     dict           \n",
      "destination                    200000     100.0%     dict           \n",
      "elastic_agent                  200000     100.0%     dict           \n",
      "network_traffic                200000     100.0%     dict           \n",
      "source                         200000     100.0%     dict           \n",
      "network                        200000     100.0%     dict           \n",
      "@timestamp                     200000     100.0%     str            \n",
      "ecs                            200000     100.0%     dict           \n",
      "data_stream                    200000     100.0%     dict           \n",
      "host                           200000     100.0%     dict           \n",
      "event                          200000     100.0%     dict           \n",
      "process                        125679      62.8%     dict           \n",
      "‚úÖ Always present fields (11):\n",
      "   - agent\n",
      "   - destination\n",
      "   - elastic_agent\n",
      "   - network_traffic\n",
      "   - source\n",
      "   - network\n",
      "   - @timestamp\n",
      "   - ecs\n",
      "   - data_stream\n",
      "   - host\n",
      "   - event\n",
      "‚ùì Optional fields (1):\n",
      "   - process                        (present in 62.8% of samples)\n",
      "üíæ Saved data structure: Field Distribution Analysis Results\n"
     ]
    }
   ],
   "source": [
    "if random_samples:\n",
    "    if 'capture_output' in globals() and 'save_data_structure' in globals():\n",
    "        with capture_output(\"Field Distribution Analysis\", 7):\n",
    "            field_counter = Counter()\n",
    "            field_types = defaultdict(Counter)\n",
    "            \n",
    "            print(\"üìä FIELD DISTRIBUTION ANALYSIS\")\n",
    "            print(\"=\" * 50)\n",
    "            \n",
    "            for sample_idx, sample in enumerate(random_samples):\n",
    "                for field_name, field_value in sample.items():\n",
    "                    field_counter[field_name] += 1\n",
    "                    field_type = type(field_value).__name__\n",
    "                    field_types[field_name][field_type] += 1\n",
    "            \n",
    "            total_samples = len(random_samples)\n",
    "            \n",
    "            print(f\"üìà Field frequency analysis ({total_samples} samples):\")\n",
    "            print(f\"{'Field Name':<30} {'Count':<8} {'Percentage':<12} {'Primary Type':<15}\")\n",
    "            print(\"-\" * 70)\n",
    "            \n",
    "            for field_name, count in field_counter.most_common():\n",
    "                percentage = (count / total_samples) * 100\n",
    "                primary_type = field_types[field_name].most_common(1)[0][0]\n",
    "                print(f\"{field_name:<30} {count:<8} {percentage:>7.1f}%     {primary_type:<15}\")\n",
    "            \n",
    "            always_present = [field for field, count in field_counter.items() if count == total_samples]\n",
    "            optional_fields = [field for field, count in field_counter.items() if count < total_samples]\n",
    "            \n",
    "            print(f\"‚úÖ Always present fields ({len(always_present)}):\")\n",
    "            for field in always_present:\n",
    "                print(f\"   - {field}\")\n",
    "            \n",
    "            print(f\"‚ùì Optional fields ({len(optional_fields)}):\")\n",
    "            for field in optional_fields:\n",
    "                presence = (field_counter[field] / total_samples) * 100\n",
    "                print(f\"   - {field:<30} (present in {presence:.1f}% of samples)\")\n",
    "            \n",
    "            save_data_structure({\n",
    "                \"field_frequency\": dict(field_counter.most_common()),\n",
    "                \"always_present_fields\": always_present,\n",
    "                \"optional_fields\": optional_fields\n",
    "            }, \"Field Distribution Analysis Results\", \"Field Distribution Analysis\")\n",
    "    else:\n",
    "        field_counter = Counter()\n",
    "        field_types = defaultdict(Counter)\n",
    "        \n",
    "        print(\"üìä FIELD DISTRIBUTION ANALYSIS\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        for sample_idx, sample in enumerate(random_samples):\n",
    "            for field_name, field_value in sample.items():\n",
    "                field_counter[field_name] += 1\n",
    "                field_type = type(field_value).__name__\n",
    "                field_types[field_name][field_type] += 1\n",
    "        \n",
    "        total_samples = len(random_samples)\n",
    "        \n",
    "        print(f\"üìà Field frequency analysis ({total_samples} samples):\")\n",
    "        print(f\"{'Field Name':<30} {'Count':<8} {'Percentage':<12} {'Primary Type':<15}\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        for field_name, count in field_counter.most_common():\n",
    "            percentage = (count / total_samples) * 100\n",
    "            primary_type = field_types[field_name].most_common(1)[0][0]\n",
    "            print(f\"{field_name:<30} {count:<8} {percentage:>7.1f}%     {primary_type:<15}\")\n",
    "        \n",
    "        always_present = [field for field, count in field_counter.items() if count == total_samples]\n",
    "        optional_fields = [field for field, count in field_counter.items() if count < total_samples]\n",
    "        \n",
    "        print(f\"‚úÖ Always present fields ({len(always_present)}):\")\n",
    "        for field in always_present:\n",
    "            print(f\"   - {field}\")\n",
    "        \n",
    "        print(f\"‚ùì Optional fields ({len(optional_fields)}):\")\n",
    "        for field in optional_fields:\n",
    "            presence = (field_counter[field] / total_samples) * 100\n",
    "            print(f\"   - {field:<30} (present in {presence:.1f}% of samples)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Data Type Analysis\n",
    "\n",
    "Examine data types in detail to understand field characteristics and identify potential processing requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ DETAILED DATA TYPE ANALYSIS\n",
      "==================================================\n",
      "\n",
      "üìù Field: agent\n",
      "------------------------------\n",
      "Type distribution:\n",
      "   dict            200000 (100.0%)\n",
      "Sample values:\n",
      "   1. dict with keys: ['name', 'id', 'ephemeral_id']...\n",
      "   2. dict with keys: ['name', 'id', 'ephemeral_id']...\n",
      "   3. dict with keys: ['name', 'id', 'ephemeral_id']...\n",
      "   4. dict with keys: ['name', 'id', 'ephemeral_id']...\n",
      "   5. dict with keys: ['name', 'id', 'type']...\n",
      "\n",
      "üìù Field: destination\n",
      "------------------------------\n",
      "Type distribution:\n",
      "   dict            200000 (100.0%)\n",
      "Sample values:\n",
      "   1. dict with keys: ['port', 'bytes', 'ip']...\n",
      "   2. dict with keys: ['port', 'ip', 'mac']...\n",
      "   3. dict with keys: ['port', 'ip', 'mac']...\n",
      "   4. dict with keys: ['port', 'bytes', 'ip']...\n",
      "   5. dict with keys: ['port', 'bytes', 'ip']...\n",
      "\n",
      "üìù Field: elastic_agent\n",
      "------------------------------\n",
      "Type distribution:\n",
      "   dict            200000 (100.0%)\n",
      "Sample values:\n",
      "   1. dict with keys: ['id', 'version', 'snapshot']...\n",
      "   2. dict with keys: ['id', 'version', 'snapshot']...\n",
      "   3. dict with keys: ['id', 'version', 'snapshot']...\n",
      "   4. dict with keys: ['id', 'version', 'snapshot']...\n",
      "   5. dict with keys: ['id', 'version', 'snapshot']...\n",
      "\n",
      "üìù Field: network_traffic\n",
      "------------------------------\n",
      "Type distribution:\n",
      "   dict            200000 (100.0%)\n",
      "Sample values:\n",
      "   1. dict with keys: ['flow']...\n",
      "   2. dict with keys: ['flow']...\n",
      "   3. dict with keys: ['flow']...\n",
      "   4. dict with keys: ['flow']...\n",
      "   5. dict with keys: ['flow']...\n",
      "\n",
      "üìù Field: source\n",
      "------------------------------\n",
      "Type distribution:\n",
      "   dict            200000 (100.0%)\n",
      "Sample values:\n",
      "   1. dict with keys: ['port', 'bytes', 'ip']...\n",
      "   2. dict with keys: ['port', 'bytes', 'ip']...\n",
      "   3. dict with keys: ['port', 'bytes', 'ip']...\n",
      "   4. dict with keys: ['port', 'bytes', 'ip']...\n",
      "   5. dict with keys: ['process', 'port', 'bytes']...\n",
      "\n",
      "üìù Field: network\n",
      "------------------------------\n",
      "Type distribution:\n",
      "   dict            200000 (100.0%)\n",
      "Sample values:\n",
      "   1. dict with keys: ['community_id', 'bytes', 'transport']...\n",
      "   2. dict with keys: ['community_id', 'bytes', 'transport']...\n",
      "   3. dict with keys: ['community_id', 'bytes', 'transport']...\n",
      "   4. dict with keys: ['community_id', 'bytes', 'transport']...\n",
      "   5. dict with keys: ['community_id', 'bytes', 'transport']...\n",
      "\n",
      "üìù Field: @timestamp\n",
      "------------------------------\n",
      "Type distribution:\n",
      "   str             200000 (100.0%)\n",
      "Sample values:\n",
      "   1. \"(24 chars): 2025-05-04T12:39:15.847Z\"\n",
      "   2. \"(24 chars): 2025-05-04T11:30:08.613Z\"\n",
      "   3. \"(24 chars): 2025-05-04T11:30:08.613Z\"\n",
      "   4. \"(24 chars): 2025-05-04T12:27:14.936Z\"\n",
      "   5. \"(24 chars): 2025-05-04T11:30:08.613Z\"\n",
      "\n",
      "üìù Field: ecs\n",
      "------------------------------\n",
      "Type distribution:\n",
      "   dict            200000 (100.0%)\n",
      "Sample values:\n",
      "   1. dict with keys: ['version']...\n",
      "   2. dict with keys: ['version']...\n",
      "   3. dict with keys: ['version']...\n",
      "   4. dict with keys: ['version']...\n",
      "   5. dict with keys: ['version']...\n",
      "\n",
      "üìù Field: data_stream\n",
      "------------------------------\n",
      "Type distribution:\n",
      "   dict            200000 (100.0%)\n",
      "Sample values:\n",
      "   1. dict with keys: ['namespace', 'type', 'dataset']...\n",
      "   2. dict with keys: ['namespace', 'type', 'dataset']...\n",
      "   3. dict with keys: ['namespace', 'type', 'dataset']...\n",
      "   4. dict with keys: ['namespace', 'type', 'dataset']...\n",
      "   5. dict with keys: ['namespace', 'type', 'dataset']...\n",
      "\n",
      "üìù Field: host\n",
      "------------------------------\n",
      "Type distribution:\n",
      "   dict            200000 (100.0%)\n",
      "Sample values:\n",
      "   1. dict with keys: ['hostname', 'os', 'ip']...\n",
      "   2. dict with keys: ['hostname', 'os', 'ip']...\n",
      "   3. dict with keys: ['hostname', 'os', 'ip']...\n",
      "   4. dict with keys: ['hostname', 'os', 'ip']...\n",
      "   5. dict with keys: ['hostname', 'os', 'ip']...\n",
      "\n",
      "üìù Field: event\n",
      "------------------------------\n",
      "Type distribution:\n",
      "   dict            200000 (100.0%)\n",
      "Sample values:\n",
      "   1. dict with keys: ['duration', 'agent_id_status', 'ingested']...\n",
      "   2. dict with keys: ['duration', 'agent_id_status', 'ingested']...\n",
      "   3. dict with keys: ['duration', 'agent_id_status', 'ingested']...\n",
      "   4. dict with keys: ['duration', 'agent_id_status', 'ingested']...\n",
      "   5. dict with keys: ['duration', 'agent_id_status', 'ingested']...\n",
      "\n",
      "üìù Field: process\n",
      "------------------------------\n",
      "Type distribution:\n",
      "   dict            125679 (100.0%)\n",
      "Sample values:\n",
      "   1. dict with keys: ['args', 'parent', 'start']...\n",
      "\n",
      "üèóÔ∏è  Complex fields requiring special processing:\n",
      "   - agent                          (nested_object)\n",
      "   - destination                    (nested_object)\n",
      "   - elastic_agent                  (nested_object)\n",
      "   - network_traffic                (nested_object)\n",
      "   - source                         (nested_object)\n",
      "   - network                        (nested_object)\n",
      "   - ecs                            (nested_object)\n",
      "   - data_stream                    (nested_object)\n",
      "   - host                           (nested_object)\n",
      "   - event                          (nested_object)\n",
      "   - process                        (nested_object)\n"
     ]
    }
   ],
   "source": [
    "if random_samples:\n",
    "    print(\"üî¨ DETAILED DATA TYPE ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Analyze complex fields (nested objects, arrays)\n",
    "    complex_fields = {}  # Store analysis of nested structures\n",
    "    \n",
    "    for field_name in field_counter.keys():\n",
    "        print(f\"\\nüìù Field: {field_name}\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        # Get type distribution for this field\n",
    "        type_dist = field_types[field_name]\n",
    "        print(f\"Type distribution:\")\n",
    "        for data_type, count in type_dist.most_common():\n",
    "            percentage = (count / field_counter[field_name]) * 100\n",
    "            print(f\"   {data_type:<15} {count:>4} ({percentage:5.1f}%)\")\n",
    "        \n",
    "        # Analyze sample values for each field\n",
    "        sample_values = []\n",
    "        for sample in random_samples[:5]:  # Show first 5 samples\n",
    "            if field_name in sample:\n",
    "                value = sample[field_name]\n",
    "                \n",
    "                # Handle different data types appropriately\n",
    "                if isinstance(value, dict):\n",
    "                    # For dictionaries, show structure\n",
    "                    sample_values.append(f\"dict with keys: {list(value.keys())[:3]}...\")\n",
    "                    complex_fields[field_name] = 'nested_object'\n",
    "                elif isinstance(value, list):\n",
    "                    # For lists, show length and sample items\n",
    "                    sample_values.append(f\"list[{len(value)}]: {value[:2]}...\")\n",
    "                    complex_fields[field_name] = 'array'\n",
    "                elif isinstance(value, str):\n",
    "                    # For strings, show truncated value\n",
    "                    truncated = value[:50] + \"...\" if len(value) > 50 else value\n",
    "                    sample_values.append(f'\"({len(value)} chars): {truncated}\"')\n",
    "                else:\n",
    "                    # For primitives, show actual value\n",
    "                    sample_values.append(str(value))\n",
    "        \n",
    "        print(f\"Sample values:\")\n",
    "        for i, val in enumerate(sample_values, 1):\n",
    "            print(f\"   {i}. {val}\")\n",
    "    \n",
    "    # Summary of complex fields\n",
    "    if complex_fields:\n",
    "        print(f\"\\nüèóÔ∏è  Complex fields requiring special processing:\")\n",
    "        for field, complexity in complex_fields.items():\n",
    "            print(f\"   - {field:<30} ({complexity})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Nested Structure Deep Dive\n",
    "\n",
    "Examine nested objects and arrays to understand the full data hierarchy and identify all available fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SECTION 9: NESTED STRUCTURE DEEP DIVE\n",
      "================================================================================\n",
      "üóÇÔ∏è  NESTED STRUCTURE DEEP DIVE\n",
      "==================================================\n",
      "‚úì Analyzed sample 1/10\n",
      "‚úì Analyzed sample 2/10\n",
      "‚úì Analyzed sample 3/10\n",
      "‚úì Analyzed sample 4/10\n",
      "‚úì Analyzed sample 5/10\n",
      "‚úì Analyzed sample 6/10\n",
      "‚úì Analyzed sample 7/10\n",
      "‚úì Analyzed sample 8/10\n",
      "‚úì Analyzed sample 9/10\n",
      "‚úì Analyzed sample 10/10\n",
      "üìã Complete field hierarchy (87 unique paths):\n",
      "------------------------------------------------------------\n",
      "üå≥ @timestamp:\n",
      "  ‚îú‚îÄ @timestamp (str)\n",
      "üå≥ agent:\n",
      "  ‚îú‚îÄ agent (dict)\n",
      "    ‚îú‚îÄ ephemeral_id (str)\n",
      "    ‚îú‚îÄ id (str)\n",
      "    ‚îú‚îÄ name (str)\n",
      "    ‚îú‚îÄ type (str)\n",
      "    ‚îú‚îÄ version (str)\n",
      "üå≥ data_stream:\n",
      "  ‚îú‚îÄ data_stream (dict)\n",
      "    ‚îú‚îÄ dataset (str)\n",
      "    ‚îú‚îÄ namespace (str)\n",
      "    ‚îú‚îÄ type (str)\n",
      "üå≥ destination:\n",
      "  ‚îú‚îÄ destination (dict)\n",
      "    ‚îú‚îÄ bytes (int)\n",
      "    ‚îú‚îÄ ip (str)\n",
      "    ‚îú‚îÄ mac (str)\n",
      "    ‚îú‚îÄ packets (int)\n",
      "    ‚îú‚îÄ port (int)\n",
      "üå≥ ecs:\n",
      "  ‚îú‚îÄ ecs (dict)\n",
      "    ‚îú‚îÄ version (str)\n",
      "üå≥ elastic_agent:\n",
      "  ‚îú‚îÄ elastic_agent (dict)\n",
      "    ‚îú‚îÄ id (str)\n",
      "    ‚îú‚îÄ snapshot (bool)\n",
      "    ‚îú‚îÄ version (str)\n",
      "üå≥ event:\n",
      "  ‚îú‚îÄ event (dict)\n",
      "    ‚îú‚îÄ action (str)\n",
      "    ‚îú‚îÄ agent_id_status (str)\n",
      "    ‚îú‚îÄ category (list)\n",
      "    ‚îú‚îÄ category[0] (str)\n",
      "    ‚îú‚îÄ duration (int)\n",
      "    ‚îú‚îÄ end (str)\n",
      "    ‚îú‚îÄ ingested (str)\n",
      "    ‚îú‚îÄ kind (str)\n",
      "    ‚îú‚îÄ start (str)\n",
      "    ‚îú‚îÄ type (list)\n",
      "    ‚îú‚îÄ type[0] (str)\n",
      "üå≥ host:\n",
      "  ‚îú‚îÄ host (dict)\n",
      "    ‚îú‚îÄ architecture (str)\n",
      "    ‚îú‚îÄ hostname (str)\n",
      "    ‚îú‚îÄ id (str)\n",
      "    ‚îú‚îÄ ip (list)\n",
      "    ‚îú‚îÄ ip[0] (str)\n",
      "    ‚îú‚îÄ mac (list)\n",
      "    ‚îú‚îÄ mac[0] (str)\n",
      "    ‚îú‚îÄ name (str)\n",
      "    ‚îú‚îÄ os (dict)\n",
      "      ‚îú‚îÄ build (str)\n",
      "      ‚îú‚îÄ family (str)\n",
      "      ‚îú‚îÄ kernel (str)\n",
      "      ‚îú‚îÄ name (str)\n",
      "      ‚îú‚îÄ platform (str)\n",
      "      ‚îú‚îÄ type (str)\n",
      "      ‚îú‚îÄ version (str)\n",
      "üå≥ network:\n",
      "  ‚îú‚îÄ network (dict)\n",
      "    ‚îú‚îÄ bytes (int)\n",
      "    ‚îú‚îÄ community_id (str)\n",
      "    ‚îú‚îÄ packets (int)\n",
      "    ‚îú‚îÄ transport (str)\n",
      "    ‚îú‚îÄ type (str)\n",
      "üå≥ network_traffic:\n",
      "  ‚îú‚îÄ network_traffic (dict)\n",
      "    ‚îú‚îÄ flow (dict)\n",
      "      ‚îú‚îÄ final (bool)\n",
      "      ‚îú‚îÄ id (str)\n",
      "üå≥ process:\n",
      "  ‚îú‚îÄ process (dict)\n",
      "    ‚îú‚îÄ args (list)\n",
      "    ‚îú‚îÄ args[0] (str)\n",
      "    ‚îú‚îÄ executable (str)\n",
      "    ‚îú‚îÄ name (str)\n",
      "    ‚îú‚îÄ parent (dict)\n",
      "      ‚îú‚îÄ pid (int)\n",
      "    ‚îú‚îÄ pid (int)\n",
      "    ‚îú‚îÄ start (str)\n",
      "    ‚îú‚îÄ working_directory (str)\n",
      "üå≥ source:\n",
      "  ‚îú‚îÄ source (dict)\n",
      "    ‚îú‚îÄ bytes (int)\n",
      "    ‚îú‚îÄ ip (str)\n",
      "    ‚îú‚îÄ mac (str)\n",
      "    ‚îú‚îÄ packets (int)\n",
      "    ‚îú‚îÄ port (int)\n",
      "    ‚îú‚îÄ process (dict)\n",
      "      ‚îú‚îÄ args (list)\n",
      "      ‚îú‚îÄ args[0] (str)\n",
      "      ‚îú‚îÄ executable (str)\n",
      "      ‚îú‚îÄ name (str)\n",
      "      ‚îú‚îÄ pid (int)\n",
      "      ‚îú‚îÄ ppid (int)\n",
      "      ‚îú‚îÄ start (str)\n",
      "      ‚îú‚îÄ working_directory (str)\n",
      "üíæ Saved data structure: Complete Field Hierarchy Analysis\n"
     ]
    }
   ],
   "source": [
    "def analyze_nested_structure(obj, path=\"\", max_depth=3, current_depth=0):\n",
    "    paths = set()\n",
    "    \n",
    "    if current_depth >= max_depth:\n",
    "        return paths\n",
    "    \n",
    "    if isinstance(obj, dict):\n",
    "        for key, value in obj.items():\n",
    "            new_path = f\"{path}.{key}\" if path else key\n",
    "            paths.add(f\"{new_path} ({type(value).__name__})\")\n",
    "            paths.update(analyze_nested_structure(value, new_path, max_depth, current_depth + 1))\n",
    "            \n",
    "    elif isinstance(obj, list) and obj:\n",
    "        first_item = obj[0]\n",
    "        array_path = f\"{path}[0]\"\n",
    "        paths.add(f\"{array_path} ({type(first_item).__name__})\")\n",
    "        paths.update(analyze_nested_structure(first_item, array_path, max_depth, current_depth + 1))\n",
    "    \n",
    "    return paths\n",
    "\n",
    "if random_samples:\n",
    "    if 'capture_output' in globals() and 'save_data_structure' in globals():\n",
    "        with capture_output(\"Nested Structure Deep Dive\", 9):\n",
    "            print(\"üóÇÔ∏è  NESTED STRUCTURE DEEP DIVE\")\n",
    "            print(\"=\" * 50)\n",
    "            \n",
    "            all_paths = set()\n",
    "            \n",
    "            for i, sample in enumerate(random_samples[:10]):\n",
    "                sample_paths = analyze_nested_structure(sample, max_depth=4)\n",
    "                all_paths.update(sample_paths)\n",
    "                print(f\"‚úì Analyzed sample {i+1}/10\")\n",
    "            \n",
    "            print(f\"üìã Complete field hierarchy ({len(all_paths)} unique paths):\")\n",
    "            print(\"-\" * 60)\n",
    "            \n",
    "            sorted_paths = sorted(all_paths)\n",
    "            \n",
    "            grouped_paths = defaultdict(list)\n",
    "            for path in sorted_paths:\n",
    "                top_level = path.split('.')[0].split(' ')[0]\n",
    "                grouped_paths[top_level].append(path)\n",
    "            \n",
    "            for top_field, paths in grouped_paths.items():\n",
    "                print(f\"üå≥ {top_field}:\")\n",
    "                for path in paths:\n",
    "                    indent = \"  \" * (path.count('.') + 1)\n",
    "                    field_name = path.split(' ')[0]\n",
    "                    field_type = path.split('(')[1].replace(')', '') if '(' in path else 'unknown'\n",
    "                    print(f\"{indent}‚îú‚îÄ {field_name.split('.')[-1]} ({field_type})\")\n",
    "            \n",
    "            save_data_structure({\n",
    "                \"total_unique_paths\": len(all_paths),\n",
    "                \"grouped_field_hierarchy\": {k: v for k, v in grouped_paths.items()},\n",
    "                \"all_paths\": sorted_paths\n",
    "            }, \"Complete Field Hierarchy Analysis\", \"Nested Structure Deep Dive\")\n",
    "    else:\n",
    "        print(\"üóÇÔ∏è  NESTED STRUCTURE DEEP DIVE\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        all_paths = set()\n",
    "        \n",
    "        for i, sample in enumerate(random_samples[:10]):\n",
    "            sample_paths = analyze_nested_structure(sample, max_depth=4)\n",
    "            all_paths.update(sample_paths)\n",
    "            print(f\"‚úì Analyzed sample {i+1}/10\")\n",
    "        \n",
    "        print(f\"üìã Complete field hierarchy ({len(all_paths)} unique paths):\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        sorted_paths = sorted(all_paths)\n",
    "        \n",
    "        grouped_paths = defaultdict(list)\n",
    "        for path in sorted_paths:\n",
    "            top_level = path.split('.')[0].split(' ')[0]\n",
    "            grouped_paths[top_level].append(path)\n",
    "        \n",
    "        for top_field, paths in grouped_paths.items():\n",
    "            print(f\"üå≥ {top_field}:\")\n",
    "            for path in paths:\n",
    "                indent = \"  \" * (path.count('.') + 1)\n",
    "                field_name = path.split(' ')[0]\n",
    "                field_type = path.split('(')[1].replace(')', '') if '(' in path else 'unknown'\n",
    "                print(f\"{indent}‚îú‚îÄ {field_name.split('.')[-1]} ({field_type})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Data Quality Assessment\n",
    "\n",
    "Evaluate data quality by checking for missing values, empty fields, and potential data inconsistencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç DATA QUALITY ASSESSMENT\n",
      "==================================================\n",
      "üìä Data completeness analysis:\n",
      "Field Name                     Empty Count  Completeness    Notes               \n",
      "--------------------------------------------------------------------------------\n",
      "agent                          0              100.0%       good                \n",
      "destination                    0              100.0%       good                \n",
      "elastic_agent                  0              100.0%       good                \n",
      "network_traffic                0              100.0%       good                \n",
      "source                         0              100.0%       good                \n",
      "network                        0              100.0%       good                \n",
      "@timestamp                     0              100.0%       avg len: 24.0       \n",
      "ecs                            0              100.0%       good                \n",
      "data_stream                    0              100.0%       good                \n",
      "host                           0              100.0%       good                \n",
      "event                          0              100.0%       good                \n",
      "process                        0              100.0%       good                \n",
      "\n",
      "üè∑Ô∏è  Potential categorical fields (‚â§50 unique values):\n"
     ]
    }
   ],
   "source": [
    "if random_samples:\n",
    "    print(\"üîç DATA QUALITY ASSESSMENT\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Initialize quality metrics\n",
    "    quality_stats = {\n",
    "        'empty_values': defaultdict(int),  # Count of empty/null values per field\n",
    "        'field_lengths': defaultdict(list),  # String/array lengths per field\n",
    "        'unique_values': defaultdict(set),  # Unique values for categorical analysis\n",
    "        'total_samples': len(random_samples)\n",
    "    }\n",
    "    \n",
    "    # Analyze each sample for quality metrics\n",
    "    for sample_idx, sample in enumerate(random_samples):\n",
    "        for field_name, field_value in sample.items():\n",
    "            \n",
    "            # Check for empty/null values\n",
    "            if field_value is None or field_value == \"\" or field_value == []:\n",
    "                quality_stats['empty_values'][field_name] += 1\n",
    "            \n",
    "            # Track field lengths for strings and arrays\n",
    "            if isinstance(field_value, (str, list)):\n",
    "                quality_stats['field_lengths'][field_name].append(len(field_value))\n",
    "            \n",
    "            # Collect unique values for potential categorical fields\n",
    "            if isinstance(field_value, (str, int, float, bool)) and not isinstance(field_value, bool):\n",
    "                # Limit unique value tracking to avoid memory issues\n",
    "                if len(quality_stats['unique_values'][field_name]) < 50:\n",
    "                    quality_stats['unique_values'][field_name].add(str(field_value))\n",
    "    \n",
    "    # Report data quality findings\n",
    "    total_samples = quality_stats['total_samples']\n",
    "    \n",
    "    print(f\"üìä Data completeness analysis:\")\n",
    "    print(f\"{'Field Name':<30} {'Empty Count':<12} {'Completeness':<15} {'Notes':<20}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for field_name in field_counter.keys():\n",
    "        empty_count = quality_stats['empty_values'][field_name]\n",
    "        present_count = field_counter[field_name]\n",
    "        completeness = ((present_count - empty_count) / present_count) * 100 if present_count > 0 else 0\n",
    "        \n",
    "        # Generate quality notes\n",
    "        notes = []\n",
    "        if empty_count > 0:\n",
    "            notes.append(f\"{empty_count} empty\")\n",
    "        if field_name in quality_stats['field_lengths']:\n",
    "            lengths = quality_stats['field_lengths'][field_name]\n",
    "            if lengths:\n",
    "                avg_len = sum(lengths) / len(lengths)\n",
    "                notes.append(f\"avg len: {avg_len:.1f}\")\n",
    "        \n",
    "        notes_str = \", \".join(notes) if notes else \"good\"\n",
    "        \n",
    "        print(f\"{field_name:<30} {empty_count:<12} {completeness:>7.1f}%       {notes_str:<20}\")\n",
    "    \n",
    "    # Identify potential categorical fields\n",
    "    print(f\"\\nüè∑Ô∏è  Potential categorical fields (‚â§50 unique values):\")\n",
    "    for field_name, unique_vals in quality_stats['unique_values'].items():\n",
    "        if len(unique_vals) <= 20:  # Show fields with few unique values\n",
    "            unique_count = len(unique_vals)\n",
    "            sample_values = list(unique_vals)[:5]  # Show first 5 unique values\n",
    "            print(f\"   - {field_name:<25} ({unique_count:2d} unique): {sample_values}\")\n",
    "            if len(unique_vals) > 5:\n",
    "                print(f\"     {' ' * 27} ... and {len(unique_vals) - 5} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Sample Record Display\n",
    "\n",
    "Display a few complete sample records with formatted output for manual inspection and verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SECTION 11: SAMPLE RECORD DISPLAY\n",
      "================================================================================\n",
      "üìÑ SAMPLE RECORD DISPLAY\n",
      "==================================================\n",
      "Sample Record #1:\n",
      "----------------------------------------\n",
      "üíæ Saved data structure: Sample Record #1\n",
      "üèóÔ∏è  agent:\n",
      "      name: theblock\n",
      "      id: dea09884-b943-42b8-a03b-2e87109e5297\n",
      "      ephemeral_id: fdaf0b65-7a0d-49a7-90a6-5f011b283f17\n",
      "      type: packetbeat\n",
      "      version: 8.18.0\n",
      "üèóÔ∏è  destination:\n",
      "      port: 9200\n",
      "      bytes: 7087\n",
      "      ip: 10.2.0.20\n",
      "      packets: 26\n",
      "      mac: 08-00-27-26-33-A5\n",
      "üèóÔ∏è  elastic_agent:\n",
      "      id: dea09884-b943-42b8-a03b-2e87109e5297\n",
      "      version: 8.18.0\n",
      "      snapshot: False\n",
      "üèóÔ∏è  network_traffic:\n",
      "      flow: {'final': False, 'id': 'EQQA////DP//////FP8BAAEIACcmM6UIACf5E3UKAgAUCgEABfAj3vg'}\n",
      "üèóÔ∏è  source:\n",
      "      port: 63710\n",
      "      bytes: 60945\n",
      "      ip: 10.1.0.5\n",
      "      packets: 43\n",
      "      mac: 08-00-27-F9-13-75\n",
      "üèóÔ∏è  network:\n",
      "      community_id: 1:p/eP4xM4zgi1RF3mnJg2GvG310c=\n",
      "      bytes: 68032\n",
      "      transport: tcp\n",
      "      type: ipv4\n",
      "      packets: 69\n",
      "üìù @timestamp: 2025-05-04T12:39:15.847Z\n",
      "üèóÔ∏è  ecs:\n",
      "      version: 8.11.0\n",
      "üèóÔ∏è  data_stream:\n",
      "      namespace: default\n",
      "      type: logs\n",
      "      dataset: network_traffic.flow\n",
      "üèóÔ∏è  host:\n",
      "      hostname: theblock\n",
      "      os: {'build': '17763.107', 'kernel': '10.0.17763.107 (WinBuild.160101.0800)', 'name': 'Windows 10 Pro fo...\n",
      "      ip: ['fe80::ac69:cdb8:409a:9db8', '10.1.0.5']\n",
      "      name: theblock\n",
      "      id: a978ae42-a0af-4cc7-9ec6-49eeab15067d\n",
      "      mac: ['08-00-27-F9-13-75']\n",
      "      architecture: x86_64\n",
      "üèóÔ∏è  event:\n",
      "      duration: 16204538900\n",
      "      agent_id_status: verified\n",
      "      ingested: 2025-05-04T12:39:21Z\n",
      "      kind: event\n",
      "      start: 2025-05-04T12:38:16.540Z\n",
      "      action: network_flow\n",
      "      end: 2025-05-04T12:38:32.745Z\n",
      "      category: ['network']\n",
      "      type: ['connection']\n",
      "\n",
      "Sample Record #2:\n",
      "----------------------------------------\n",
      "üíæ Saved data structure: Sample Record #2\n",
      "üèóÔ∏è  agent:\n",
      "      name: diskjockey\n",
      "      id: 15638b02-c77f-4aed-9566-1c5c68f1c0b3\n",
      "      ephemeral_id: bb3f2762-d059-48c3-8da2-f6b7c9630c2a\n",
      "      type: packetbeat\n",
      "      version: 8.18.0\n",
      "üèóÔ∏è  elastic_agent:\n",
      "      id: 15638b02-c77f-4aed-9566-1c5c68f1c0b3\n",
      "      version: 8.18.0\n",
      "      snapshot: False\n",
      "üèóÔ∏è  destination:\n",
      "      port: 5355\n",
      "      ip: 224.0.0.252\n",
      "      mac: 01-00-5E-00-00-FC\n",
      "üèóÔ∏è  network_traffic:\n",
      "      flow: {'final': False, 'id': 'EQIA////DP////8U//8BAAEBAF4AAPwIACdXBSvgAAD8CgEABOsU8Os'}\n",
      "üèóÔ∏è  source:\n",
      "      port: 60400\n",
      "      bytes: 166\n",
      "      ip: 10.1.0.4\n",
      "      mac: 08-00-27-57-05-2B\n",
      "      packets: 2\n",
      "üèóÔ∏è  network:\n",
      "      community_id: 1:3LGZ/at8r1OIXFF7p9YZCxSArn0=\n",
      "      bytes: 166\n",
      "      transport: udp\n",
      "      type: ipv4\n",
      "      packets: 2\n",
      "üìù @timestamp: 2025-05-04T11:30:08.613Z\n",
      "üèóÔ∏è  ecs:\n",
      "      version: 8.11.0\n",
      "üèóÔ∏è  data_stream:\n",
      "      namespace: default\n",
      "      type: logs\n",
      "      dataset: network_traffic.flow\n",
      "üèóÔ∏è  host:\n",
      "      hostname: diskjockey\n",
      "      os: {'build': '17763.3650', 'kernel': '10.0.17763.3650 (WinBuild.160101.0800)', 'name': 'Windows Server ...\n",
      "      ip: ['10.1.0.4']\n",
      "      name: diskjockey\n",
      "      id: acb80d05-87f8-427b-9b79-726fd9bed256\n",
      "      mac: ['08-00-27-57-05-2B']\n",
      "      architecture: x86_64\n",
      "üèóÔ∏è  event:\n",
      "      duration: 500000600\n",
      "      agent_id_status: verified\n",
      "      ingested: 2025-05-04T11:30:11Z\n",
      "      kind: event\n",
      "      start: 2025-05-04T11:29:51.890Z\n",
      "      action: network_flow\n",
      "      end: 2025-05-04T11:29:52.380Z\n",
      "      category: ['network']\n",
      "      type: ['connection']\n",
      "\n",
      "Sample Record #3:\n",
      "----------------------------------------\n",
      "üíæ Saved data structure: Sample Record #3\n",
      "üèóÔ∏è  agent:\n",
      "      name: diskjockey\n",
      "      id: 15638b02-c77f-4aed-9566-1c5c68f1c0b3\n",
      "      ephemeral_id: bb3f2762-d059-48c3-8da2-f6b7c9630c2a\n",
      "      type: packetbeat\n",
      "      version: 8.18.0\n",
      "üèóÔ∏è  destination:\n",
      "      port: 5355\n",
      "      ip: 224.0.0.252\n",
      "      mac: 01-00-5E-00-00-FC\n",
      "üèóÔ∏è  elastic_agent:\n",
      "      id: 15638b02-c77f-4aed-9566-1c5c68f1c0b3\n",
      "      version: 8.18.0\n",
      "      snapshot: False\n",
      "üèóÔ∏è  network_traffic:\n",
      "      flow: {'final': False, 'id': 'EQIA////DP////8U//8BAAEBAF4AAPwIACdXBSvgAAD8CgEABOsUbuY'}\n",
      "üèóÔ∏è  source:\n",
      "      port: 58990\n",
      "      bytes: 168\n",
      "      ip: 10.1.0.4\n",
      "      packets: 2\n",
      "      mac: 08-00-27-57-05-2B\n",
      "üèóÔ∏è  network:\n",
      "      community_id: 1:sqWOLWq7fU1bcT/GgYiI1n9rnHg=\n",
      "      bytes: 168\n",
      "      transport: udp\n",
      "      type: ipv4\n",
      "      packets: 2\n",
      "üìù @timestamp: 2025-05-04T11:30:08.613Z\n",
      "üèóÔ∏è  ecs:\n",
      "      version: 8.11.0\n",
      "üèóÔ∏è  data_stream:\n",
      "      namespace: default\n",
      "      type: logs\n",
      "      dataset: network_traffic.flow\n",
      "üèóÔ∏è  host:\n",
      "      hostname: diskjockey\n",
      "      os: {'build': '17763.3650', 'kernel': '10.0.17763.3650 (WinBuild.160101.0800)', 'name': 'Windows Server ...\n",
      "      ip: ['10.1.0.4']\n",
      "      name: diskjockey\n",
      "      id: acb80d05-87f8-427b-9b79-726fd9bed256\n",
      "      mac: ['08-00-27-57-05-2B']\n",
      "      architecture: x86_64\n",
      "üèóÔ∏è  event:\n",
      "      duration: 422012100\n",
      "      agent_id_status: verified\n",
      "      ingested: 2025-05-04T11:30:11Z\n",
      "      kind: event\n",
      "      start: 2025-05-04T11:29:58.901Z\n",
      "      action: network_flow\n",
      "      end: 2025-05-04T11:29:59.305Z\n",
      "      category: ['network']\n",
      "      type: ['connection']\n",
      "\n",
      "üíæ Complete sample records saved to: outputs/3a-network-flows/3a-network-flows_sample_data_20250629_064647.json\n",
      "üìù All output logged to: outputs/3a-network-flows/3a-network-flows_exploratory_analysis_20250629_064647.log\n"
     ]
    }
   ],
   "source": [
    "if random_samples:\n",
    "    if 'capture_output' in globals() and 'save_data_structure' in globals():\n",
    "        with capture_output(\"Sample Record Display\", 11):\n",
    "            print(\"üìÑ SAMPLE RECORD DISPLAY\")\n",
    "            print(\"=\" * 50)\n",
    "            \n",
    "            num_samples_to_show = min(3, len(random_samples))\n",
    "            \n",
    "            for i in range(num_samples_to_show):\n",
    "                print(f\"Sample Record #{i+1}:\")\n",
    "                print(\"-\" * 40)\n",
    "                \n",
    "                sample = random_samples[i]\n",
    "                \n",
    "                save_data_structure(sample, f\"Sample Record #{i+1}\", \"Sample Record Display\")\n",
    "                \n",
    "                for field_name, field_value in sample.items():\n",
    "                    if isinstance(field_value, dict):\n",
    "                        print(f\"üèóÔ∏è  {field_name}:\")\n",
    "                        for nested_key, nested_val in field_value.items():\n",
    "                            nested_str = str(nested_val)[:100] + \"...\" if len(str(nested_val)) > 100 else str(nested_val)\n",
    "                            print(f\"      {nested_key}: {nested_str}\")\n",
    "                    elif isinstance(field_value, list):\n",
    "                        print(f\"üìã {field_name} (array[{len(field_value)}]):\")\n",
    "                        for j, item in enumerate(field_value[:3]):\n",
    "                            item_str = str(item)[:100] + \"...\" if len(str(item)) > 100 else str(item)\n",
    "                            print(f\"      [{j}]: {item_str}\")\n",
    "                        if len(field_value) > 3:\n",
    "                            print(f\"      ... and {len(field_value) - 3} more items\")\n",
    "                    else:\n",
    "                        value_str = str(field_value)\n",
    "                        if len(value_str) > 150:\n",
    "                            value_str = value_str[:150] + \"...\"\n",
    "                        print(f\"üìù {field_name}: {value_str}\")\n",
    "                \n",
    "                print()\n",
    "        \n",
    "        print(f\"üíæ Complete sample records saved to: {data_filename}\")\n",
    "        print(f\"üìù All output logged to: {log_filename}\")\n",
    "    \n",
    "    else:\n",
    "        print(\"üìÑ SAMPLE RECORD DISPLAY\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        num_samples_to_show = min(3, len(random_samples))\n",
    "        \n",
    "        for i in range(num_samples_to_show):\n",
    "            print(f\"Sample Record #{i+1}:\")\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "            sample = random_samples[i]\n",
    "            \n",
    "            for field_name, field_value in sample.items():\n",
    "                if isinstance(field_value, dict):\n",
    "                    print(f\"üèóÔ∏è  {field_name}:\")\n",
    "                    for nested_key, nested_val in field_value.items():\n",
    "                        nested_str = str(nested_val)[:100] + \"...\" if len(str(nested_val)) > 100 else str(nested_val)\n",
    "                        print(f\"      {nested_key}: {nested_str}\")\n",
    "                elif isinstance(field_value, list):\n",
    "                    print(f\"üìã {field_name} (array[{len(field_value)}]):\")\n",
    "                    for j, item in enumerate(field_value[:3]):\n",
    "                        item_str = str(item)[:100] + \"...\" if len(str(item)) > 100 else str(item)\n",
    "                        print(f\"      [{j}]: {item_str}\")\n",
    "                    if len(field_value) > 3:\n",
    "                        print(f\"      ... and {len(field_value) - 3} more items\")\n",
    "                else:\n",
    "                    value_str = str(field_value)\n",
    "                    if len(value_str) > 150:\n",
    "                        value_str = value_str[:150] + \"...\"\n",
    "                    print(f\"üìù {field_name}: {value_str}\")\n",
    "            \n",
    "            print()\n",
    "        \n",
    "        print(\"‚ÑπÔ∏è  Note: Logging system not available - outputs not saved to files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Analysis Summary and Recommendations\n",
    "\n",
    "Provide a comprehensive summary of findings and recommendations for processing the network traffic data in notebook #3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SECTION 12: ANALYSIS SUMMARY AND RECOMMENDATIONS\n",
      "================================================================================\n",
      "üìä ANALYSIS SUMMARY AND RECOMMENDATIONS\n",
      "============================================================\n",
      "üîç DATASET CHARACTERISTICS:\n",
      "   ‚Ä¢ Total records in file: ~1,090,212\n",
      "   ‚Ä¢ Samples analyzed: 200000\n",
      "   ‚Ä¢ Total unique fields: 12\n",
      "   ‚Ä¢ Always present fields: 11\n",
      "   ‚Ä¢ Optional fields: 1\n",
      "üí° PROCESSING RECOMMENDATIONS:\n",
      "   üß† Memory Management:\n",
      "      - Use batch processing for large dataset (1,090,212 records)\n",
      "      - Consider chunked reading with pandas or custom iterator\n",
      "      - Implement progress tracking for long operations\n",
      "   üèóÔ∏è  Complex Field Handling:\n",
      "      - agent: Flatten nested structure or extract key fields\n",
      "      - destination: Flatten nested structure or extract key fields\n",
      "      - elastic_agent: Flatten nested structure or extract key fields\n",
      "      - network_traffic: Flatten nested structure or extract key fields\n",
      "      - source: Flatten nested structure or extract key fields\n",
      "      - network: Flatten nested structure or extract key fields\n",
      "      - ecs: Flatten nested structure or extract key fields\n",
      "      - data_stream: Flatten nested structure or extract key fields\n",
      "      - host: Flatten nested structure or extract key fields\n",
      "      - event: Flatten nested structure or extract key fields\n",
      "      - process: Flatten nested structure or extract key fields\n",
      "   üìÑ CSV Conversion Strategy:\n",
      "      - Include all 11 always-present fields\n",
      "      - Handle 1 optional fields with default values\n",
      "      - Implement error logging for malformed records\n",
      "      - Use appropriate data types for each field\n",
      "‚úÖ NEXT STEPS:\n",
      "   1. Review findings above\n",
      "   2. Update notebook #3 with optimized processing logic\n",
      "   3. Implement batch processing for memory efficiency\n",
      "   4. Add comprehensive error handling and logging\n",
      "   5. Test with full dataset after validation\n",
      "üéØ Ready to proceed to notebook #3 optimization!\n",
      "üíæ Saved data structure: Complete Analysis Summary\n"
     ]
    }
   ],
   "source": [
    "if random_samples:\n",
    "    if 'capture_output' in globals() and 'save_data_structure' in globals():\n",
    "        with capture_output(\"Analysis Summary and Recommendations\", 12):\n",
    "            print(\"üìä ANALYSIS SUMMARY AND RECOMMENDATIONS\")\n",
    "            print(\"=\" * 60)\n",
    "            \n",
    "            total_fields = len(field_counter)\n",
    "            always_present_fields = [f for f, c in field_counter.items() if c == len(random_samples)]\n",
    "            optional_fields = [f for f, c in field_counter.items() if c < len(random_samples)]\n",
    "            \n",
    "            print(f\"üîç DATASET CHARACTERISTICS:\")\n",
    "            print(f\"   ‚Ä¢ Total records in file: ~{total_records:,}\")\n",
    "            print(f\"   ‚Ä¢ Samples analyzed: {len(random_samples)}\")\n",
    "            print(f\"   ‚Ä¢ Total unique fields: {total_fields}\")\n",
    "            print(f\"   ‚Ä¢ Always present fields: {len(always_present_fields)}\")\n",
    "            print(f\"   ‚Ä¢ Optional fields: {len(optional_fields)}\")\n",
    "            \n",
    "            print(f\"üí° PROCESSING RECOMMENDATIONS:\")\n",
    "            \n",
    "            if total_records > 100000:\n",
    "                print(f\"   üß† Memory Management:\")\n",
    "                print(f\"      - Use batch processing for large dataset ({total_records:,} records)\")\n",
    "                print(f\"      - Consider chunked reading with pandas or custom iterator\")\n",
    "                print(f\"      - Implement progress tracking for long operations\")\n",
    "            \n",
    "            if complex_fields:\n",
    "                print(f\"   üèóÔ∏è  Complex Field Handling:\")\n",
    "                for field, complexity in complex_fields.items():\n",
    "                    if complexity == 'nested_object':\n",
    "                        print(f\"      - {field}: Flatten nested structure or extract key fields\")\n",
    "                    elif complexity == 'array':\n",
    "                        print(f\"      - {field}: Consider array length, join elements, or extract features\")\n",
    "            \n",
    "            high_empty_fields = [f for f, c in quality_stats['empty_values'].items() if c > len(random_samples) * 0.1]\n",
    "            if high_empty_fields:\n",
    "                print(f\"   üö® Data Quality:\")\n",
    "                for field in high_empty_fields:\n",
    "                    empty_pct = (quality_stats['empty_values'][field] / field_counter[field]) * 100\n",
    "                    print(f\"      - {field}: {empty_pct:.1f}% empty values - consider exclusion or imputation\")\n",
    "            \n",
    "            print(f\"   üìÑ CSV Conversion Strategy:\")\n",
    "            print(f\"      - Include all {len(always_present_fields)} always-present fields\")\n",
    "            print(f\"      - Handle {len(optional_fields)} optional fields with default values\")\n",
    "            print(f\"      - Implement error logging for malformed records\")\n",
    "            print(f\"      - Use appropriate data types for each field\")\n",
    "            \n",
    "            print(f\"‚úÖ NEXT STEPS:\")\n",
    "            print(f\"   1. Review findings above\")\n",
    "            print(f\"   2. Update notebook #3 with optimized processing logic\")\n",
    "            print(f\"   3. Implement batch processing for memory efficiency\")\n",
    "            print(f\"   4. Add comprehensive error handling and logging\")\n",
    "            print(f\"   5. Test with full dataset after validation\")\n",
    "            \n",
    "            print(f\"üéØ Ready to proceed to notebook #3 optimization!\")\n",
    "            \n",
    "            save_data_structure({\n",
    "                \"dataset_characteristics\": {\n",
    "                    \"total_records\": total_records,\n",
    "                    \"samples_analyzed\": len(random_samples),\n",
    "                    \"total_unique_fields\": total_fields,\n",
    "                    \"always_present_fields\": always_present_fields,\n",
    "                    \"optional_fields\": optional_fields\n",
    "                },\n",
    "                \"processing_recommendations\": {\n",
    "                    \"memory_management_needed\": total_records > 100000,\n",
    "                    \"complex_fields_count\": len(complex_fields) if 'complex_fields' in globals() else 0,\n",
    "                    \"high_empty_fields\": high_empty_fields if 'quality_stats' in globals() else []\n",
    "                }\n",
    "            }, \"Complete Analysis Summary\", \"Analysis Summary and Recommendations\")\n",
    "    else:\n",
    "        print(\"üìä ANALYSIS SUMMARY AND RECOMMENDATIONS\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        total_fields = len(field_counter)\n",
    "        always_present_fields = [f for f, c in field_counter.items() if c == len(random_samples)]\n",
    "        optional_fields = [f for f, c in field_counter.items() if c < len(random_samples)]\n",
    "        \n",
    "        print(f\"üîç DATASET CHARACTERISTICS:\")\n",
    "        print(f\"   ‚Ä¢ Total records in file: ~{total_records:,}\")\n",
    "        print(f\"   ‚Ä¢ Samples analyzed: {len(random_samples)}\")\n",
    "        print(f\"   ‚Ä¢ Total unique fields: {total_fields}\")\n",
    "        print(f\"   ‚Ä¢ Always present fields: {len(always_present_fields)}\")\n",
    "        print(f\"   ‚Ä¢ Optional fields: {len(optional_fields)}\")\n",
    "        \n",
    "        print(f\"üí° PROCESSING RECOMMENDATIONS:\")\n",
    "        \n",
    "        if total_records > 100000:\n",
    "            print(f\"   üß† Memory Management:\")\n",
    "            print(f\"      - Use batch processing for large dataset ({total_records:,} records)\")\n",
    "            print(f\"      - Consider chunked reading with pandas or custom iterator\")\n",
    "            print(f\"      - Implement progress tracking for long operations\")\n",
    "        \n",
    "        if 'complex_fields' in globals() and complex_fields:\n",
    "            print(f\"   üèóÔ∏è  Complex Field Handling:\")\n",
    "            for field, complexity in complex_fields.items():\n",
    "                if complexity == 'nested_object':\n",
    "                    print(f\"      - {field}: Flatten nested structure or extract key fields\")\n",
    "                elif complexity == 'array':\n",
    "                    print(f\"      - {field}: Consider array length, join elements, or extract features\")\n",
    "        \n",
    "        if 'quality_stats' in globals():\n",
    "            high_empty_fields = [f for f, c in quality_stats['empty_values'].items() if c > len(random_samples) * 0.1]\n",
    "            if high_empty_fields:\n",
    "                print(f\"   üö® Data Quality:\")\n",
    "                for field in high_empty_fields:\n",
    "                    empty_pct = (quality_stats['empty_values'][field] / field_counter[field]) * 100\n",
    "                    print(f\"      - {field}: {empty_pct:.1f}% empty values - consider exclusion or imputation\")\n",
    "        \n",
    "        print(f\"   üìÑ CSV Conversion Strategy:\")\n",
    "        print(f\"      - Include all {len(always_present_fields)} always-present fields\")\n",
    "        print(f\"      - Handle {len(optional_fields)} optional fields with default values\")\n",
    "        print(f\"      - Implement error logging for malformed records\")\n",
    "        print(f\"      - Use appropriate data types for each field\")\n",
    "        \n",
    "        print(f\"‚úÖ NEXT STEPS:\")\n",
    "        print(f\"   1. Review findings above\")\n",
    "        print(f\"   2. Update notebook #3 with optimized processing logic\")\n",
    "        print(f\"   3. Implement batch processing for memory efficiency\")\n",
    "        print(f\"   4. Add comprehensive error handling and logging\")\n",
    "        print(f\"   5. Test with full dataset after validation\")\n",
    "        \n",
    "        print(f\"üéØ Ready to proceed to notebook #3 optimization!\")\n",
    "else:\n",
    "    print(\"‚ùå No data available for analysis - check file path and permissions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Output Files Summary\n",
    "\n",
    "Summary of all generated output files for easy access and inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ OUTPUT FILES GENERATED\n",
      "==================================================\n",
      "üìÇ Analysis directory: outputs/3a-network-flows\n",
      "üè∑Ô∏è  Analysis type: 3a-network-flows\n",
      "\n",
      "üìù Log file: 3a-network-flows_exploratory_analysis_20250629_055400.log\n",
      "   Size: 18,069 bytes\n",
      "   Purpose: Contains all console output from analysis sections\n",
      "   Usage: View complete analysis results and debugging info\n",
      "\n",
      "üìù Log file: 3a-network-flows_exploratory_analysis_20250629_063341.log\n",
      "   Size: 18,069 bytes\n",
      "   Purpose: Contains all console output from analysis sections\n",
      "   Usage: View complete analysis results and debugging info\n",
      "\n",
      "üìù Log file: 3a-network-flows_exploratory_analysis_20250629_064000.log\n",
      "   Size: 18,170 bytes\n",
      "   Purpose: Contains all console output from analysis sections\n",
      "   Usage: View complete analysis results and debugging info\n",
      "\n",
      "üìù Log file: 3a-network-flows_exploratory_analysis_20250629_064223.log\n",
      "   Size: 20,233 bytes\n",
      "   Purpose: Contains all console output from analysis sections\n",
      "   Usage: View complete analysis results and debugging info\n",
      "\n",
      "üìù Log file: 3a-network-flows_exploratory_analysis_20250629_064647.log\n",
      "   Size: 16,575 bytes\n",
      "   Purpose: Contains all console output from analysis sections\n",
      "   Usage: View complete analysis results and debugging info\n",
      "\n",
      "üìä Data file: 3a-network-flows_sample_data_20250629_055400.json\n",
      "   Size: 24,765 bytes\n",
      "   Purpose: Contains complete sample data structures\n",
      "   Usage: Detailed inspection of individual records\n",
      "\n",
      "üìä Data file: 3a-network-flows_sample_data_20250629_063341.json\n",
      "   Size: 24,765 bytes\n",
      "   Purpose: Contains complete sample data structures\n",
      "   Usage: Detailed inspection of individual records\n",
      "\n",
      "üìä Data file: 3a-network-flows_sample_data_20250629_064000.json\n",
      "   Size: 22,849 bytes\n",
      "   Purpose: Contains complete sample data structures\n",
      "   Usage: Detailed inspection of individual records\n",
      "\n",
      "üìä Data file: 3a-network-flows_sample_data_20250629_064223.json\n",
      "   Size: 30,215 bytes\n",
      "   Purpose: Contains complete sample data structures\n",
      "   Usage: Detailed inspection of individual records\n",
      "\n",
      "üìä Data file: 3a-network-flows_sample_data_20250629_064647.json\n",
      "   Size: 20,451 bytes\n",
      "   Purpose: Contains complete sample data structures\n",
      "   Usage: Detailed inspection of individual records\n",
      "\n",
      "üîç HOW TO USE THE OUTPUT FILES:\n",
      "   1. Copy log file contents to view complete analysis output\n",
      "   2. Copy JSON file contents to examine detailed sample structures\n",
      "   3. Use these files to provide complete analysis results for review\n",
      "   4. Compare with outputs from 3b structure consistency analysis\n",
      "üìã TO ADD LOGGING TO OTHER CELLS:\n",
      "   ‚Ä¢ Wrap code with: with capture_output(\"Section Name\", section_number):\n",
      "   ‚Ä¢ Save data with: save_data_structure(data, \"description\", \"section\")\n",
      "   ‚Ä¢ Both console output and files will be updated automatically\n",
      "‚úÖ 3a exploratory analysis complete! Check outputs/3a-network-flows/ directory for all output files.\n",
      "üîó Related: Check outputs/network-flows/ for 3b structure consistency analysis outputs.\n"
     ]
    }
   ],
   "source": [
    "print(\"üìÅ OUTPUT FILES GENERATED\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if os.path.exists(analysis_outputs_dir):\n",
    "    output_files = os.listdir(analysis_outputs_dir)\n",
    "    \n",
    "    print(f\"üìÇ Analysis directory: {analysis_outputs_dir}\")\n",
    "    print(f\"üè∑Ô∏è  Analysis type: {ANALYSIS_TYPE}\")\n",
    "    print()\n",
    "    \n",
    "    for file in sorted(output_files):\n",
    "        file_path = os.path.join(analysis_outputs_dir, file)\n",
    "        file_size = os.path.getsize(file_path)\n",
    "        \n",
    "        if file.endswith('.log'):\n",
    "            print(f\"üìù Log file: {file}\")\n",
    "            print(f\"   Size: {file_size:,} bytes\")\n",
    "            print(f\"   Purpose: Contains all console output from analysis sections\")\n",
    "            print(f\"   Usage: View complete analysis results and debugging info\")\n",
    "            \n",
    "        elif file.endswith('.json'):\n",
    "            print(f\"üìä Data file: {file}\")\n",
    "            print(f\"   Size: {file_size:,} bytes\")\n",
    "            print(f\"   Purpose: Contains complete sample data structures\")\n",
    "            print(f\"   Usage: Detailed inspection of individual records\")\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    print(\"üîç HOW TO USE THE OUTPUT FILES:\")\n",
    "    print(f\"   1. Copy log file contents to view complete analysis output\")\n",
    "    print(f\"   2. Copy JSON file contents to examine detailed sample structures\")\n",
    "    print(f\"   3. Use these files to provide complete analysis results for review\")\n",
    "    print(f\"   4. Compare with outputs from 3b structure consistency analysis\")\n",
    "    \n",
    "    print(f\"üìã TO ADD LOGGING TO OTHER CELLS:\")\n",
    "    print(f\"   ‚Ä¢ Wrap code with: with capture_output(\\\"Section Name\\\", section_number):\")\n",
    "    print(f\"   ‚Ä¢ Save data with: save_data_structure(data, \\\"description\\\", \\\"section\\\")\")\n",
    "    print(f\"   ‚Ä¢ Both console output and files will be updated automatically\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No analysis output directory found - logging system may not have initialized\")\n",
    "    print(f\"Expected directory: {analysis_outputs_dir}\")\n",
    "\n",
    "print(f\"‚úÖ 3a exploratory analysis complete! Check {analysis_outputs_dir}/ directory for all output files.\")\n",
    "print(f\"üîó Related: Check outputs/network-flows/ for 3b structure consistency analysis outputs.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataset-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}