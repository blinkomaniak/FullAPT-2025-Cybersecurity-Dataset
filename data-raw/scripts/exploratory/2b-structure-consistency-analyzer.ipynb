{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîç SYSMON STRUCTURE CONSISTENCY ANALYZER\n",
    "\n",
    "This notebook performs comprehensive structure consistency analysis on Windows Sysmon events stored in JSONL format from Elasticsearch. The analysis focuses on understanding XML structure patterns, field consistency, and schema variation across different EventIDs.\n",
    "\n",
    "**Target File**: `-ds-logs-windows-sysmon_operational-default-2025-05-04-000001.jsonl`  \n",
    "**Analysis Type**: 2B-SYSMON  \n",
    "**Purpose**: Analyze structure consistency and schema patterns for robust CSV conversion\n",
    "\n",
    "**Key Analysis Areas**:\n",
    "- XML structure fingerprinting and pattern detection\n",
    "- EventID-specific schema consistency analysis\n",
    "- Field co-occurrence and dependency patterns\n",
    "- Structure variation analysis and outlier detection\n",
    "- Schema evolution and consistency metrics\n",
    "- Processing pipeline recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import hashlib\n",
    "import random\n",
    "import os\n",
    "from datetime import datetime\n",
    "from collections import defaultdict, Counter\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Analysis Configuration and Logging Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SYSMON STRUCTURE CONSISTENCY ANALYSIS\n",
      "Analysis Type: 2B-SYSMON\n",
      "Generated: 2025-06-29 11:36:37\n",
      "Target File: -ds-logs-windows-sysmon_operational-default-2025-05-04-000001.jsonl\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Analysis Configuration\n",
    "ANALYSIS_TYPE = \"2b-sysmon\"\n",
    "SAMPLE_SIZE = 200_000  # Number of samples to analyze\n",
    "TARGET_FILE = \"-ds-logs-windows-sysmon_operational-default-2025-05-04-000001.jsonl\"\n",
    "\n",
    "# Create organized output directory structure\n",
    "outputs_base_dir = \"outputs\"\n",
    "analysis_outputs_dir = f\"{outputs_base_dir}/{ANALYSIS_TYPE}\"\n",
    "os.makedirs(analysis_outputs_dir, exist_ok=True)\n",
    "\n",
    "# Setup logging\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "log_filename = f\"{analysis_outputs_dir}/{ANALYSIS_TYPE}_structure_analysis_{timestamp}.log\"\n",
    "results_filename = f\"{analysis_outputs_dir}/{ANALYSIS_TYPE}_structure_results_{timestamp}.json\"\n",
    "\n",
    "def log_print(message):\n",
    "    \"\"\"Print and log messages\"\"\"\n",
    "    print(message)\n",
    "    with open(log_filename, 'a', encoding='utf-8') as f:\n",
    "        f.write(message + '\\n')\n",
    "\n",
    "# Initialize log file\n",
    "log_print(\"SYSMON STRUCTURE CONSISTENCY ANALYSIS\")\n",
    "log_print(f\"Analysis Type: {ANALYSIS_TYPE.upper()}\")\n",
    "log_print(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "log_print(f\"Target File: {TARGET_FILE}\")\n",
    "log_print(\"=\" * 80)\n",
    "log_print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. XML Parsing and Structure Analysis Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ XML parsing and structure analysis utilities loaded\n"
     ]
    }
   ],
   "source": [
    "def sanitize_xml(xml_str):\n",
    "    \"\"\"Clean invalid characters and repair XML structure\"\"\"\n",
    "    try:\n",
    "        # Remove non-printable characters\n",
    "        cleaned = ''.join(c for c in xml_str if 31 < ord(c) < 127 or c in '\\t\\n\\r')\n",
    "        # Fix common XML issues using BeautifulSoup's parser\n",
    "        return BeautifulSoup(cleaned, \"xml\").prettify()\n",
    "    except:\n",
    "        return xml_str  # Return original if cleaning fails\n",
    "\n",
    "def generate_structure_fingerprint(event_id, fields_dict):\n",
    "    \"\"\"Generate a hash fingerprint for the structure pattern\"\"\"\n",
    "    # Create structure signature: EventID + sorted field names + field types\n",
    "    structure_elements = [str(event_id)]\n",
    "    \n",
    "    if fields_dict:\n",
    "        # Add field names and their presence (not values)\n",
    "        for field_name in sorted(fields_dict.keys()):\n",
    "            field_value = fields_dict[field_name]\n",
    "            field_type = \"present\" if field_value is not None else \"null\"\n",
    "            structure_elements.append(f\"{field_name}:{field_type}\")\n",
    "    \n",
    "    # Create hash\n",
    "    structure_string = \"|\".join(structure_elements)\n",
    "    return hashlib.md5(structure_string.encode()).hexdigest()\n",
    "\n",
    "def parse_sysmon_event_detailed(xml_str):\n",
    "    \"\"\"Parse XML with detailed structure analysis\"\"\"\n",
    "    try:\n",
    "        # Clean XML first\n",
    "        clean_xml = sanitize_xml(xml_str)\n",
    "        \n",
    "        # Parse with explicit namespace\n",
    "        namespaces = {'ns': 'http://schemas.microsoft.com/win/2004/08/events/event'}\n",
    "        root = ET.fromstring(clean_xml)\n",
    "        \n",
    "        # System section\n",
    "        system = root.find('ns:System', namespaces)\n",
    "        if not system:\n",
    "            return None, None, {}, None\n",
    "\n",
    "        event_id_elem = system.find('ns:EventID', namespaces)\n",
    "        computer_elem = system.find('ns:Computer', namespaces)\n",
    "        \n",
    "        event_id = int(event_id_elem.text) if event_id_elem is not None and event_id_elem.text else None\n",
    "        computer = computer_elem.text if computer_elem is not None and computer_elem.text else None\n",
    "\n",
    "        # EventData section - extract all fields with detailed info\n",
    "        event_data = root.find('ns:EventData', namespaces)\n",
    "        fields = {}\n",
    "        if event_data:\n",
    "            for data in event_data.findall('ns:Data', namespaces):\n",
    "                name = data.get('Name')\n",
    "                if name:\n",
    "                    fields[name] = data.text if data.text else None\n",
    "\n",
    "        # Generate structure fingerprint\n",
    "        fingerprint = generate_structure_fingerprint(event_id, fields)\n",
    "        \n",
    "        return event_id, computer, fields, fingerprint\n",
    "\n",
    "    except Exception as e:\n",
    "        return None, None, {}, None\n",
    "\n",
    "print(\"‚úÖ XML parsing and structure analysis utilities loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Loading and Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SECTION 3: DATA LOADING AND SAMPLING\n",
      "================================================================================\n",
      "\n",
      "üîÑ Loading samples from: -ds-logs-windows-sysmon_operational-default-2025-05-04-000001.jsonl\n",
      "üìä Counting total records...\n",
      "üìà Found 570,078 total records\n",
      "üéØ Sampling every 2 records for stratified coverage\n",
      "‚úÖ Collected 200,000 samples for analysis\n",
      "üìä SAMPLING SUMMARY:\n",
      "   ‚Ä¢ Total file records: 570,078\n",
      "   ‚Ä¢ Samples collected: 200,000\n",
      "   ‚Ä¢ Coverage ratio: 35.08%\n",
      "\n",
      "------------------------------------------------------------ END SECTION ------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Start section logging\n",
    "log_print(\"\\n\" + \"=\" * 80)\n",
    "log_print(\"SECTION 3: DATA LOADING AND SAMPLING\")\n",
    "log_print(\"=\" * 80)\n",
    "log_print(\"\")\n",
    "\n",
    "# Count total records\n",
    "log_print(f\"üîÑ Loading samples from: {TARGET_FILE}\")\n",
    "log_print(f\"üìä Counting total records...\")\n",
    "\n",
    "total_records = 0\n",
    "with open(TARGET_FILE, 'r') as f:\n",
    "    for line in f:\n",
    "        total_records += 1\n",
    "\n",
    "log_print(f\"üìà Found {total_records:,} total records\")\n",
    "\n",
    "# Stratified sampling approach - sample every N records for better coverage\n",
    "sample_interval = max(1, total_records // SAMPLE_SIZE)\n",
    "log_print(f\"üéØ Sampling every {sample_interval} records for stratified coverage\")\n",
    "\n",
    "# Collect samples\n",
    "collected_samples = []\n",
    "sample_count = 0\n",
    "\n",
    "with open(TARGET_FILE, 'r') as f:\n",
    "    for line_number, line in enumerate(f):\n",
    "        if line_number % sample_interval == 0 and sample_count < SAMPLE_SIZE:\n",
    "            try:\n",
    "                event = json.loads(line)\n",
    "                if 'event' in event and 'original' in event['event']:\n",
    "                    collected_samples.append((line_number, event))\n",
    "                    sample_count += 1\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "\n",
    "log_print(f\"‚úÖ Collected {len(collected_samples):,} samples for analysis\")\n",
    "log_print(f\"üìä SAMPLING SUMMARY:\")\n",
    "log_print(f\"   ‚Ä¢ Total file records: {total_records:,}\")\n",
    "log_print(f\"   ‚Ä¢ Samples collected: {len(collected_samples):,}\")\n",
    "log_print(f\"   ‚Ä¢ Coverage ratio: {(len(collected_samples)/total_records)*100:.2f}%\")\n",
    "\n",
    "# End section logging\n",
    "log_print(\"\\n\" + \"-\" * 60 + \" END SECTION \" + \"-\" * 60)\n",
    "log_print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Structure Pattern Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SECTION 4: STRUCTURE PATTERN DETECTION\n",
      "================================================================================\n",
      "\n",
      "üîç GENERATING STRUCTURE FINGERPRINTS\n",
      "==================================================\n",
      "üìã Processing samples...\n",
      "   Processed 0 / 200000 records\n",
      "   Processed 1000 / 200000 records\n",
      "   Processed 2000 / 200000 records\n",
      "   Processed 3000 / 200000 records\n",
      "   Processed 4000 / 200000 records\n",
      "   Processed 5000 / 200000 records\n",
      "   Processed 6000 / 200000 records\n",
      "   Processed 7000 / 200000 records\n",
      "   Processed 8000 / 200000 records\n",
      "   Processed 9000 / 200000 records\n",
      "   Processed 10000 / 200000 records\n",
      "   Processed 11000 / 200000 records\n",
      "   Processed 12000 / 200000 records\n",
      "   Processed 13000 / 200000 records\n",
      "   Processed 14000 / 200000 records\n",
      "   Processed 15000 / 200000 records\n",
      "   Processed 16000 / 200000 records\n",
      "   Processed 17000 / 200000 records\n",
      "   Processed 18000 / 200000 records\n",
      "   Processed 19000 / 200000 records\n",
      "   Processed 20000 / 200000 records\n",
      "   Processed 21000 / 200000 records\n",
      "   Processed 22000 / 200000 records\n",
      "   Processed 23000 / 200000 records\n",
      "   Processed 24000 / 200000 records\n",
      "   Processed 25000 / 200000 records\n",
      "   Processed 26000 / 200000 records\n",
      "   Processed 27000 / 200000 records\n",
      "   Processed 28000 / 200000 records\n",
      "   Processed 29000 / 200000 records\n",
      "   Processed 30000 / 200000 records\n",
      "   Processed 31000 / 200000 records\n",
      "   Processed 32000 / 200000 records\n",
      "   Processed 33000 / 200000 records\n",
      "   Processed 34000 / 200000 records\n",
      "   Processed 35000 / 200000 records\n",
      "   Processed 36000 / 200000 records\n",
      "   Processed 37000 / 200000 records\n",
      "   Processed 38000 / 200000 records\n",
      "   Processed 39000 / 200000 records\n",
      "   Processed 40000 / 200000 records\n",
      "   Processed 41000 / 200000 records\n",
      "   Processed 42000 / 200000 records\n",
      "   Processed 43000 / 200000 records\n",
      "   Processed 44000 / 200000 records\n",
      "   Processed 45000 / 200000 records\n",
      "   Processed 46000 / 200000 records\n",
      "   Processed 47000 / 200000 records\n",
      "   Processed 48000 / 200000 records\n",
      "   Processed 49000 / 200000 records\n",
      "   Processed 50000 / 200000 records\n",
      "   Processed 51000 / 200000 records\n",
      "   Processed 52000 / 200000 records\n",
      "   Processed 53000 / 200000 records\n",
      "   Processed 54000 / 200000 records\n",
      "   Processed 55000 / 200000 records\n",
      "   Processed 56000 / 200000 records\n",
      "   Processed 57000 / 200000 records\n",
      "   Processed 58000 / 200000 records\n",
      "   Processed 59000 / 200000 records\n",
      "   Processed 60000 / 200000 records\n",
      "   Processed 61000 / 200000 records\n",
      "   Processed 62000 / 200000 records\n",
      "   Processed 63000 / 200000 records\n",
      "   Processed 64000 / 200000 records\n",
      "   Processed 65000 / 200000 records\n",
      "   Processed 66000 / 200000 records\n",
      "   Processed 67000 / 200000 records\n",
      "   Processed 68000 / 200000 records\n",
      "   Processed 69000 / 200000 records\n",
      "   Processed 70000 / 200000 records\n",
      "   Processed 71000 / 200000 records\n",
      "   Processed 72000 / 200000 records\n",
      "   Processed 73000 / 200000 records\n",
      "   Processed 74000 / 200000 records\n",
      "   Processed 75000 / 200000 records\n",
      "   Processed 76000 / 200000 records\n",
      "   Processed 77000 / 200000 records\n",
      "   Processed 78000 / 200000 records\n",
      "   Processed 79000 / 200000 records\n",
      "   Processed 80000 / 200000 records\n",
      "   Processed 81000 / 200000 records\n",
      "   Processed 82000 / 200000 records\n",
      "   Processed 83000 / 200000 records\n",
      "   Processed 84000 / 200000 records\n",
      "   Processed 85000 / 200000 records\n",
      "   Processed 86000 / 200000 records\n",
      "   Processed 87000 / 200000 records\n",
      "   Processed 88000 / 200000 records\n",
      "   Processed 89000 / 200000 records\n",
      "   Processed 90000 / 200000 records\n",
      "   Processed 91000 / 200000 records\n",
      "   Processed 92000 / 200000 records\n",
      "   Processed 93000 / 200000 records\n",
      "   Processed 94000 / 200000 records\n",
      "   Processed 95000 / 200000 records\n",
      "   Processed 96000 / 200000 records\n",
      "   Processed 97000 / 200000 records\n",
      "   Processed 98000 / 200000 records\n",
      "   Processed 99000 / 200000 records\n",
      "   Processed 100000 / 200000 records\n",
      "   Processed 101000 / 200000 records\n",
      "   Processed 102000 / 200000 records\n",
      "   Processed 103000 / 200000 records\n",
      "   Processed 104000 / 200000 records\n",
      "   Processed 105000 / 200000 records\n",
      "   Processed 106000 / 200000 records\n",
      "   Processed 107000 / 200000 records\n",
      "   Processed 108000 / 200000 records\n",
      "   Processed 109000 / 200000 records\n",
      "   Processed 110000 / 200000 records\n",
      "   Processed 111000 / 200000 records\n",
      "   Processed 112000 / 200000 records\n",
      "   Processed 113000 / 200000 records\n",
      "   Processed 114000 / 200000 records\n",
      "   Processed 115000 / 200000 records\n",
      "   Processed 116000 / 200000 records\n",
      "   Processed 117000 / 200000 records\n",
      "   Processed 118000 / 200000 records\n",
      "   Processed 119000 / 200000 records\n",
      "   Processed 120000 / 200000 records\n",
      "   Processed 121000 / 200000 records\n",
      "   Processed 122000 / 200000 records\n",
      "   Processed 123000 / 200000 records\n",
      "   Processed 124000 / 200000 records\n",
      "   Processed 125000 / 200000 records\n",
      "   Processed 126000 / 200000 records\n",
      "   Processed 127000 / 200000 records\n",
      "   Processed 128000 / 200000 records\n",
      "   Processed 129000 / 200000 records\n",
      "   Processed 130000 / 200000 records\n",
      "   Processed 131000 / 200000 records\n",
      "   Processed 132000 / 200000 records\n",
      "   Processed 133000 / 200000 records\n",
      "   Processed 134000 / 200000 records\n",
      "   Processed 135000 / 200000 records\n",
      "   Processed 136000 / 200000 records\n",
      "   Processed 137000 / 200000 records\n",
      "   Processed 138000 / 200000 records\n",
      "   Processed 139000 / 200000 records\n",
      "   Processed 140000 / 200000 records\n",
      "   Processed 141000 / 200000 records\n",
      "   Processed 142000 / 200000 records\n",
      "   Processed 143000 / 200000 records\n",
      "   Processed 144000 / 200000 records\n",
      "   Processed 145000 / 200000 records\n",
      "   Processed 146000 / 200000 records\n",
      "   Processed 147000 / 200000 records\n",
      "   Processed 148000 / 200000 records\n",
      "   Processed 149000 / 200000 records\n",
      "   Processed 150000 / 200000 records\n",
      "   Processed 151000 / 200000 records\n",
      "   Processed 152000 / 200000 records\n",
      "   Processed 153000 / 200000 records\n",
      "   Processed 154000 / 200000 records\n",
      "   Processed 155000 / 200000 records\n",
      "   Processed 156000 / 200000 records\n",
      "   Processed 157000 / 200000 records\n",
      "   Processed 158000 / 200000 records\n",
      "   Processed 159000 / 200000 records\n",
      "   Processed 160000 / 200000 records\n",
      "   Processed 161000 / 200000 records\n",
      "   Processed 162000 / 200000 records\n",
      "   Processed 163000 / 200000 records\n",
      "   Processed 164000 / 200000 records\n",
      "   Processed 165000 / 200000 records\n",
      "   Processed 166000 / 200000 records\n",
      "   Processed 167000 / 200000 records\n",
      "   Processed 168000 / 200000 records\n",
      "   Processed 169000 / 200000 records\n",
      "   Processed 170000 / 200000 records\n",
      "   Processed 171000 / 200000 records\n",
      "   Processed 172000 / 200000 records\n",
      "   Processed 173000 / 200000 records\n",
      "   Processed 174000 / 200000 records\n",
      "   Processed 175000 / 200000 records\n",
      "   Processed 176000 / 200000 records\n",
      "   Processed 177000 / 200000 records\n",
      "   Processed 178000 / 200000 records\n",
      "   Processed 179000 / 200000 records\n",
      "   Processed 180000 / 200000 records\n",
      "   Processed 181000 / 200000 records\n",
      "   Processed 182000 / 200000 records\n",
      "   Processed 183000 / 200000 records\n",
      "   Processed 184000 / 200000 records\n",
      "   Processed 185000 / 200000 records\n",
      "   Processed 186000 / 200000 records\n",
      "   Processed 187000 / 200000 records\n",
      "   Processed 188000 / 200000 records\n",
      "   Processed 189000 / 200000 records\n",
      "   Processed 190000 / 200000 records\n",
      "   Processed 191000 / 200000 records\n",
      "   Processed 192000 / 200000 records\n",
      "   Processed 193000 / 200000 records\n",
      "   Processed 194000 / 200000 records\n",
      "   Processed 195000 / 200000 records\n",
      "   Processed 196000 / 200000 records\n",
      "   Processed 197000 / 200000 records\n",
      "   Processed 198000 / 200000 records\n",
      "   Processed 199000 / 200000 records\n",
      "‚úÖ Fingerprinting complete!\n",
      "üìä STRUCTURE ANALYSIS RESULTS:\n",
      "   ‚Ä¢ Unique structure patterns found: 20\n",
      "   ‚Ä¢ Records analyzed: 200,000\n",
      "   ‚Ä¢ Parsing success: 200,000 (100.0%)\n",
      "   ‚Ä¢ Parsing errors: 0 (0.0%)\n",
      "   ‚Ä¢ Most common pattern frequency: 89,455 records\n",
      "\n",
      "------------------------------------------------------------ END SECTION ------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Start section logging\n",
    "log_print(\"\\n\" + \"=\" * 80)\n",
    "log_print(\"SECTION 4: STRUCTURE PATTERN DETECTION\")\n",
    "log_print(\"=\" * 80)\n",
    "log_print(\"\")\n",
    "\n",
    "log_print(\"üîç GENERATING STRUCTURE FINGERPRINTS\")\n",
    "log_print(\"=\" * 50)\n",
    "\n",
    "# Structure analysis containers\n",
    "structure_patterns = {}  # fingerprint -> pattern info\n",
    "eventid_patterns = defaultdict(set)  # eventid -> set of fingerprints\n",
    "pattern_samples = {}  # fingerprint -> sample record\n",
    "parsing_stats = {'success': 0, 'errors': 0}\n",
    "\n",
    "log_print(f\"üìã Processing samples...\")\n",
    "\n",
    "# Process samples\n",
    "for idx, (line_number, event) in enumerate(collected_samples):\n",
    "    if idx % 1000 == 0:\n",
    "        log_print(f\"   Processed {idx} / {len(collected_samples)} records\")\n",
    "    \n",
    "    try:\n",
    "        xml_content = event['event']['original']\n",
    "        event_id, computer, fields, fingerprint = parse_sysmon_event_detailed(xml_content)\n",
    "        \n",
    "        if event_id is not None and fingerprint:\n",
    "            parsing_stats['success'] += 1\n",
    "            \n",
    "            # Track pattern\n",
    "            if fingerprint not in structure_patterns:\n",
    "                structure_patterns[fingerprint] = {\n",
    "                    'count': 0,\n",
    "                    'event_ids': set(),\n",
    "                    'computers': set(),\n",
    "                    'field_names': set(),\n",
    "                    'field_count': len(fields)\n",
    "                }\n",
    "                pattern_samples[fingerprint] = {\n",
    "                    'event_id': event_id,\n",
    "                    'computer': computer,\n",
    "                    'fields': fields,\n",
    "                    '@timestamp': event.get('@timestamp', None)\n",
    "                }\n",
    "            \n",
    "            # Update pattern info\n",
    "            pattern = structure_patterns[fingerprint]\n",
    "            pattern['count'] += 1\n",
    "            pattern['event_ids'].add(event_id)\n",
    "            if computer:\n",
    "                pattern['computers'].add(computer)\n",
    "            pattern['field_names'].update(fields.keys())\n",
    "            \n",
    "            # Track EventID associations\n",
    "            eventid_patterns[event_id].add(fingerprint)\n",
    "            \n",
    "        else:\n",
    "            parsing_stats['errors'] += 1\n",
    "            \n",
    "    except Exception:\n",
    "        parsing_stats['errors'] += 1\n",
    "\n",
    "log_print(f\"‚úÖ Fingerprinting complete!\")\n",
    "log_print(f\"üìä STRUCTURE ANALYSIS RESULTS:\")\n",
    "log_print(f\"   ‚Ä¢ Unique structure patterns found: {len(structure_patterns)}\")\n",
    "log_print(f\"   ‚Ä¢ Records analyzed: {len(collected_samples):,}\")\n",
    "log_print(f\"   ‚Ä¢ Parsing success: {parsing_stats['success']:,} ({(parsing_stats['success']/len(collected_samples))*100:.1f}%)\")\n",
    "log_print(f\"   ‚Ä¢ Parsing errors: {parsing_stats['errors']:,} ({(parsing_stats['errors']/len(collected_samples))*100:.1f}%)\")\n",
    "\n",
    "# Pattern frequency analysis\n",
    "pattern_frequencies = [(fp, info['count']) for fp, info in structure_patterns.items()]\n",
    "pattern_frequencies.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "if pattern_frequencies:\n",
    "    most_common_count = pattern_frequencies[0][1]\n",
    "    log_print(f\"   ‚Ä¢ Most common pattern frequency: {most_common_count:,} records\")\n",
    "\n",
    "# End section logging\n",
    "log_print(\"\\n\" + \"-\" * 60 + \" END SECTION \" + \"-\" * 60)\n",
    "log_print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. EventID-Specific Structure Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SECTION 5: EVENTID-SPECIFIC STRUCTURE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "üìã EVENTID-SPECIFIC STRUCTURE ANALYSIS\n",
      "============================================================\n",
      "\n",
      "üéØ EventID 1 - Process Creation\n",
      "--------------------------------------------------\n",
      "‚úÖ CONSISTENT STRUCTURE - Single pattern detected\n",
      "   ‚Ä¢ Records: 605\n",
      "   ‚Ä¢ Field count: 23\n",
      "   ‚Ä¢ Computers: 4\n",
      "   ‚Ä¢ Fields: ['CommandLine', 'Company', 'CurrentDirectory', 'Description', 'FileVersion', 'Hashes', 'Image', 'IntegrityLevel', 'LogonGuid', 'LogonId']...\n",
      "\n",
      "üéØ EventID 2 - File Creation Time Changed\n",
      "--------------------------------------------------\n",
      "‚úÖ CONSISTENT STRUCTURE - Single pattern detected\n",
      "   ‚Ä¢ Records: 30\n",
      "   ‚Ä¢ Field count: 9\n",
      "   ‚Ä¢ Computers: 4\n",
      "   ‚Ä¢ Fields: ['CreationUtcTime', 'Image', 'PreviousCreationUtcTime', 'ProcessGuid', 'ProcessId', 'RuleName', 'TargetFilename', 'User', 'UtcTime']\n",
      "\n",
      "üéØ EventID 3 - Network Connection\n",
      "--------------------------------------------------\n",
      "‚úÖ CONSISTENT STRUCTURE - Single pattern detected\n",
      "   ‚Ä¢ Records: 8,269\n",
      "   ‚Ä¢ Field count: 18\n",
      "   ‚Ä¢ Computers: 4\n",
      "   ‚Ä¢ Fields: ['DestinationHostname', 'DestinationIp', 'DestinationIsIpv6', 'DestinationPort', 'DestinationPortName', 'Image', 'Initiated', 'ProcessGuid', 'ProcessId', 'Protocol']...\n",
      "\n",
      "üéØ EventID 4 - Sysmon Service State Changed\n",
      "--------------------------------------------------\n",
      "‚úÖ CONSISTENT STRUCTURE - Single pattern detected\n",
      "   ‚Ä¢ Records: 2\n",
      "   ‚Ä¢ Field count: 4\n",
      "   ‚Ä¢ Computers: 2\n",
      "   ‚Ä¢ Fields: ['SchemaVersion', 'State', 'UtcTime', 'Version']\n",
      "\n",
      "üéØ EventID 5 - Process Terminated\n",
      "--------------------------------------------------\n",
      "‚úÖ CONSISTENT STRUCTURE - Single pattern detected\n",
      "   ‚Ä¢ Records: 478\n",
      "   ‚Ä¢ Field count: 6\n",
      "   ‚Ä¢ Computers: 4\n",
      "   ‚Ä¢ Fields: ['Image', 'ProcessGuid', 'ProcessId', 'RuleName', 'User', 'UtcTime']\n",
      "\n",
      "üéØ EventID 6 - Driver Loaded\n",
      "--------------------------------------------------\n",
      "‚úÖ CONSISTENT STRUCTURE - Single pattern detected\n",
      "   ‚Ä¢ Records: 181\n",
      "   ‚Ä¢ Field count: 7\n",
      "   ‚Ä¢ Computers: 4\n",
      "   ‚Ä¢ Fields: ['Hashes', 'ImageLoaded', 'RuleName', 'Signature', 'SignatureStatus', 'Signed', 'UtcTime']\n",
      "\n",
      "üéØ EventID 7 - Image/Library Loaded\n",
      "--------------------------------------------------\n",
      "‚úÖ CONSISTENT STRUCTURE - Single pattern detected\n",
      "   ‚Ä¢ Records: 31,262\n",
      "   ‚Ä¢ Field count: 16\n",
      "   ‚Ä¢ Computers: 4\n",
      "   ‚Ä¢ Fields: ['Company', 'Description', 'FileVersion', 'Hashes', 'Image', 'ImageLoaded', 'OriginalFileName', 'ProcessGuid', 'ProcessId', 'Product']...\n",
      "\n",
      "üéØ EventID 9 - Raw Access Read\n",
      "--------------------------------------------------\n",
      "‚úÖ CONSISTENT STRUCTURE - Single pattern detected\n",
      "   ‚Ä¢ Records: 460\n",
      "   ‚Ä¢ Field count: 7\n",
      "   ‚Ä¢ Computers: 4\n",
      "   ‚Ä¢ Fields: ['Device', 'Image', 'ProcessGuid', 'ProcessId', 'RuleName', 'User', 'UtcTime']\n",
      "\n",
      "üéØ EventID 10 - Process Access\n",
      "--------------------------------------------------\n",
      "‚úÖ CONSISTENT STRUCTURE - Single pattern detected\n",
      "   ‚Ä¢ Records: 27,251\n",
      "   ‚Ä¢ Field count: 13\n",
      "   ‚Ä¢ Computers: 4\n",
      "   ‚Ä¢ Fields: ['CallTrace', 'GrantedAccess', 'RuleName', 'SourceImage', 'SourceProcessGUID', 'SourceProcessId', 'SourceThreadId', 'SourceUser', 'TargetImage', 'TargetProcessGUID']...\n",
      "\n",
      "üéØ EventID 11 - File Create\n",
      "--------------------------------------------------\n",
      "‚úÖ CONSISTENT STRUCTURE - Single pattern detected\n",
      "   ‚Ä¢ Records: 2,074\n",
      "   ‚Ä¢ Field count: 8\n",
      "   ‚Ä¢ Computers: 4\n",
      "   ‚Ä¢ Fields: ['CreationUtcTime', 'Image', 'ProcessGuid', 'ProcessId', 'RuleName', 'TargetFilename', 'User', 'UtcTime']\n",
      "\n",
      "üéØ EventID 12 - Registry Event (Object create/delete)\n",
      "--------------------------------------------------\n",
      "‚úÖ CONSISTENT STRUCTURE - Single pattern detected\n",
      "   ‚Ä¢ Records: 89,455\n",
      "   ‚Ä¢ Field count: 8\n",
      "   ‚Ä¢ Computers: 4\n",
      "   ‚Ä¢ Fields: ['EventType', 'Image', 'ProcessGuid', 'ProcessId', 'RuleName', 'TargetObject', 'User', 'UtcTime']\n",
      "\n",
      "üéØ EventID 13 - Registry Event (Value Set)\n",
      "--------------------------------------------------\n",
      "‚ö†Ô∏è  INCONSISTENT STRUCTURE - 2 patterns detected\n",
      "   Pattern 1: 1 records\n",
      "     ‚Ä¢ Field count: 9\n",
      "     ‚Ä¢ Total usage: 3 records\n",
      "     ‚Ä¢ Fingerprint: fb9bbd108991d9ef...\n",
      "   Pattern 2: 1 records\n",
      "     ‚Ä¢ Field count: 9\n",
      "     ‚Ä¢ Total usage: 37,970 records\n",
      "     ‚Ä¢ Fingerprint: 05f19dcbab017f91...\n",
      "\n",
      "üéØ EventID 15 - File Create Stream Hash\n",
      "--------------------------------------------------\n",
      "‚ö†Ô∏è  INCONSISTENT STRUCTURE - 2 patterns detected\n",
      "   Pattern 1: 1 records\n",
      "     ‚Ä¢ Field count: 10\n",
      "     ‚Ä¢ Total usage: 20 records\n",
      "     ‚Ä¢ Fingerprint: d1e7d98b2bc2b805...\n",
      "   Pattern 2: 1 records\n",
      "     ‚Ä¢ Field count: 10\n",
      "     ‚Ä¢ Total usage: 2 records\n",
      "     ‚Ä¢ Fingerprint: e9bd0db1d38d6cc9...\n",
      "\n",
      "üéØ EventID 17 - Pipe Event (Pipe Created)\n",
      "--------------------------------------------------\n",
      "‚úÖ CONSISTENT STRUCTURE - Single pattern detected\n",
      "   ‚Ä¢ Records: 250\n",
      "   ‚Ä¢ Field count: 8\n",
      "   ‚Ä¢ Computers: 4\n",
      "   ‚Ä¢ Fields: ['EventType', 'Image', 'PipeName', 'ProcessGuid', 'ProcessId', 'RuleName', 'User', 'UtcTime']\n",
      "\n",
      "üéØ EventID 18 - Pipe Event (Pipe Connected)\n",
      "--------------------------------------------------\n",
      "‚úÖ CONSISTENT STRUCTURE - Single pattern detected\n",
      "   ‚Ä¢ Records: 690\n",
      "   ‚Ä¢ Field count: 8\n",
      "   ‚Ä¢ Computers: 4\n",
      "   ‚Ä¢ Fields: ['EventType', 'Image', 'PipeName', 'ProcessGuid', 'ProcessId', 'RuleName', 'User', 'UtcTime']\n",
      "\n",
      "üéØ EventID 23 - File Delete (File Delete archived)\n",
      "--------------------------------------------------\n",
      "‚úÖ CONSISTENT STRUCTURE - Single pattern detected\n",
      "   ‚Ä¢ Records: 992\n",
      "   ‚Ä¢ Field count: 10\n",
      "   ‚Ä¢ Computers: 4\n",
      "   ‚Ä¢ Fields: ['Archived', 'Hashes', 'Image', 'IsExecutable', 'ProcessGuid', 'ProcessId', 'RuleName', 'TargetFilename', 'User', 'UtcTime']\n",
      "\n",
      "üéØ EventID 24 - Clipboard Change (New content in clipboard)\n",
      "--------------------------------------------------\n",
      "‚úÖ CONSISTENT STRUCTURE - Single pattern detected\n",
      "   ‚Ä¢ Records: 3\n",
      "   ‚Ä¢ Field count: 10\n",
      "   ‚Ä¢ Computers: 3\n",
      "   ‚Ä¢ Fields: ['Archived', 'ClientInfo', 'Hashes', 'Image', 'ProcessGuid', 'ProcessId', 'RuleName', 'Session', 'User', 'UtcTime']\n",
      "\n",
      "üéØ EventID 25 - Process Tampering (Process image change)\n",
      "--------------------------------------------------\n",
      "‚úÖ CONSISTENT STRUCTURE - Single pattern detected\n",
      "   ‚Ä¢ Records: 3\n",
      "   ‚Ä¢ Field count: 7\n",
      "   ‚Ä¢ Computers: 2\n",
      "   ‚Ä¢ Fields: ['Image', 'ProcessGuid', 'ProcessId', 'RuleName', 'Type', 'User', 'UtcTime']\n",
      "\n",
      "------------------------------------------------------------ END SECTION ------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Start section logging\n",
    "log_print(\"\\n\" + \"=\" * 80)\n",
    "log_print(\"SECTION 5: EVENTID-SPECIFIC STRUCTURE ANALYSIS\")\n",
    "log_print(\"=\" * 80)\n",
    "log_print(\"\")\n",
    "\n",
    "log_print(\"üìã EVENTID-SPECIFIC STRUCTURE ANALYSIS\")\n",
    "log_print(\"=\" * 60)\n",
    "\n",
    "# Sysmon EventID descriptions\n",
    "eventid_descriptions = {\n",
    "    1: \"Process Creation\",\n",
    "    2: \"File Creation Time Changed\", \n",
    "    3: \"Network Connection\",\n",
    "    4: \"Sysmon Service State Changed\",\n",
    "    5: \"Process Terminated\",\n",
    "    6: \"Driver Loaded\",\n",
    "    7: \"Image/Library Loaded\",\n",
    "    8: \"Create Remote Thread\",\n",
    "    9: \"Raw Access Read\",\n",
    "    10: \"Process Access\",\n",
    "    11: \"File Create\",\n",
    "    12: \"Registry Event (Object create/delete)\",\n",
    "    13: \"Registry Event (Value Set)\",\n",
    "    15: \"File Create Stream Hash\",\n",
    "    17: \"Pipe Event (Pipe Created)\",\n",
    "    18: \"Pipe Event (Pipe Connected)\",\n",
    "    22: \"DNS Event (DNS query)\",\n",
    "    23: \"File Delete (File Delete archived)\",\n",
    "    24: \"Clipboard Change (New content in clipboard)\",\n",
    "    25: \"Process Tampering (Process image change)\"\n",
    "}\n",
    "\n",
    "# Analyze structure consistency per EventID\n",
    "for event_id in sorted(eventid_patterns.keys()):\n",
    "    patterns = eventid_patterns[event_id]\n",
    "    description = eventid_descriptions.get(event_id, \"Unknown EventID\")\n",
    "    \n",
    "    log_print(f\"\\nüéØ EventID {event_id} - {description}\")\n",
    "    log_print(\"-\" * 50)\n",
    "    \n",
    "    if len(patterns) == 1:\n",
    "        # Single consistent pattern\n",
    "        fingerprint = list(patterns)[0]\n",
    "        pattern_info = structure_patterns[fingerprint]\n",
    "        log_print(f\"‚úÖ CONSISTENT STRUCTURE - Single pattern detected\")\n",
    "        log_print(f\"   ‚Ä¢ Records: {pattern_info['count']:,}\")\n",
    "        log_print(f\"   ‚Ä¢ Field count: {pattern_info['field_count']}\")\n",
    "        log_print(f\"   ‚Ä¢ Computers: {len(pattern_info['computers'])}\")\n",
    "        log_print(f\"   ‚Ä¢ Fields: {sorted(list(pattern_info['field_names']))[:10]}{'...' if len(pattern_info['field_names']) > 10 else ''}\")\n",
    "    else:\n",
    "        # Multiple patterns - inconsistent structure\n",
    "        log_print(f\"‚ö†Ô∏è  INCONSISTENT STRUCTURE - {len(patterns)} patterns detected\")\n",
    "        \n",
    "        # Sort patterns by frequency for this EventID\n",
    "        eventid_pattern_freq = []\n",
    "        for fp in patterns:\n",
    "            # Count how many records of this EventID use this pattern\n",
    "            eventid_count = sum(1 for sample_fp, sample_info in pattern_samples.items() \n",
    "                              if sample_fp == fp and sample_info['event_id'] == event_id)\n",
    "            eventid_pattern_freq.append((fp, structure_patterns[fp]['count'], eventid_count))\n",
    "        \n",
    "        eventid_pattern_freq.sort(key=lambda x: x[2], reverse=True)\n",
    "        \n",
    "        for idx, (fp, total_count, eventid_count) in enumerate(eventid_pattern_freq[:3]):\n",
    "            pattern_info = structure_patterns[fp]\n",
    "            log_print(f\"   Pattern {idx+1}: {eventid_count:,} records\")\n",
    "            log_print(f\"     ‚Ä¢ Field count: {pattern_info['field_count']}\")\n",
    "            log_print(f\"     ‚Ä¢ Total usage: {total_count:,} records\")\n",
    "            log_print(f\"     ‚Ä¢ Fingerprint: {fp[:16]}...\")\n",
    "        \n",
    "        if len(eventid_pattern_freq) > 3:\n",
    "            log_print(f\"   ... and {len(eventid_pattern_freq) - 3} more patterns\")\n",
    "\n",
    "# End section logging\n",
    "log_print(\"\\n\" + \"-\" * 60 + \" END SECTION \" + \"-\" * 60)\n",
    "log_print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Detailed Pattern Analysis and Sample Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SECTION 6: DETAILED PATTERN ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "üìã DETAILED PATTERN ANALYSIS\n",
      "============================================================\n",
      "\n",
      "üîç PATTERN #1\n",
      "----------------------------------------\n",
      "üìä Frequency: 89,455 records (44.73%)\n",
      "üîë Structure Hash: 4dc5ca65df001f87...\n",
      "üéØ EventID(s): [12]\n",
      "üìè Field count: 8\n",
      "üñ•Ô∏è Computer count: 4\n",
      "üìã Fields: ['EventType', 'Image', 'ProcessGuid', 'ProcessId', 'RuleName', 'TargetObject', 'User', 'UtcTime']\n",
      "\n",
      "üìÑ Sample Record:\n",
      "   EventID: 12\n",
      "   Computer: \n",
      "   diskjockey.boombox.local\n",
      "  \n",
      "   @timestamp: 2025-05-04T11:30:05.479Z\n",
      "   Fields:\n",
      "     ‚Ä¢ RuleName            : \n",
      "   -\n",
      "  \n",
      "     ‚Ä¢ EventType           : \n",
      "   CreateKey\n",
      "  \n",
      "     ‚Ä¢ UtcTime             : \n",
      "   2025-05-04 11:30:05.479\n",
      "  \n",
      "     ‚Ä¢ ProcessGuid         : \n",
      "   {acb80d05-4f9d-6817-3800-000000001600}\n",
      "  \n",
      "     ‚Ä¢ ProcessId           : \n",
      "   2716\n",
      "  \n",
      "     ‚Ä¢ Image               : \n",
      "   C:\\Windows\\Sysmon64.exe\n",
      "  \n",
      "     ‚Ä¢ TargetObject        : \n",
      "   HKU\\.DEFAULT\\Software\\Microsoft\\SystemCertificates\\Disallowed\n",
      "  \n",
      "     ‚Ä¢ User                : \n",
      "   NT AUTHORITY\\SYSTEM\n",
      "  \n",
      "\n",
      "üîç PATTERN #2\n",
      "----------------------------------------\n",
      "üìä Frequency: 37,970 records (18.98%)\n",
      "üîë Structure Hash: 05f19dcbab017f91...\n",
      "üéØ EventID(s): [13]\n",
      "üìè Field count: 9\n",
      "üñ•Ô∏è Computer count: 4\n",
      "üìã Fields: ['Details', 'EventType', 'Image', 'ProcessGuid', 'ProcessId', 'RuleName', 'TargetObject', 'User', 'UtcTime']\n",
      "\n",
      "üìÑ Sample Record:\n",
      "   EventID: 13\n",
      "   Computer: \n",
      "   diskjockey.boombox.local\n",
      "  \n",
      "   @timestamp: 2025-05-04T11:30:05.479Z\n",
      "   Fields:\n",
      "     ‚Ä¢ RuleName            : \n",
      "   -\n",
      "  \n",
      "     ‚Ä¢ EventType           : \n",
      "   SetValue\n",
      "  \n",
      "     ‚Ä¢ UtcTime             : \n",
      "   2025-05-04 11:30:05.479\n",
      "  \n",
      "     ‚Ä¢ ProcessGuid         : \n",
      "   {acb80d05-4f8b-6817-1200-000000001600}\n",
      "  \n",
      "     ‚Ä¢ ProcessId           : \n",
      "   68\n",
      "  \n",
      "     ‚Ä¢ Image               : \n",
      "   C:\\Windows\\system32\\svchost.exe\n",
      "  \n",
      "     ‚Ä¢ TargetObject        : \n",
      "   HKLM\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\\Schedule\\TaskCache\\Tree\\Mi...\n",
      "     ‚Ä¢ Details             : \n",
      "   DWORD (0x00000003)\n",
      "  \n",
      "     ‚Ä¢ User                : \n",
      "   NT AUTHORITY\\SYSTEM\n",
      "  \n",
      "\n",
      "üîç PATTERN #3\n",
      "----------------------------------------\n",
      "üìä Frequency: 31,262 records (15.63%)\n",
      "üîë Structure Hash: 57052e9e633c73e3...\n",
      "üéØ EventID(s): [7]\n",
      "üìè Field count: 16\n",
      "üñ•Ô∏è Computer count: 4\n",
      "üìã Fields: ['Company', 'Description', 'FileVersion', 'Hashes', 'Image', 'ImageLoaded', 'OriginalFileName', 'ProcessGuid', 'ProcessId', 'Product', 'RuleName', 'Signature', 'SignatureStatus', 'Signed', 'User', 'UtcTime']\n",
      "\n",
      "üìÑ Sample Record:\n",
      "   EventID: 7\n",
      "   Computer: \n",
      "   diskjockey.boombox.local\n",
      "  \n",
      "   @timestamp: 2025-05-04T11:30:05.473Z\n",
      "   Fields:\n",
      "     ‚Ä¢ RuleName            : \n",
      "   -\n",
      "  \n",
      "     ‚Ä¢ UtcTime             : \n",
      "   2025-05-04 11:30:05.473\n",
      "  \n",
      "     ‚Ä¢ ProcessGuid         : \n",
      "   {acb80d05-4f9d-6817-3a00-000000001600}\n",
      "  \n",
      "     ‚Ä¢ ProcessId           : \n",
      "   2976\n",
      "  \n",
      "     ‚Ä¢ Image               : \n",
      "   C:\\Windows\\System32\\sppsvc.exe\n",
      "  \n",
      "     ‚Ä¢ ImageLoaded         : \n",
      "   C:\\Windows\\System32\\clbcatq.dll\n",
      "  \n",
      "     ‚Ä¢ FileVersion         : \n",
      "   2001.12.10941.16384 (WinBuild.160101.0800)\n",
      "  \n",
      "     ‚Ä¢ Description         : \n",
      "   COM+ Configuration Catalog\n",
      "  \n",
      "     ‚Ä¢ Product             : \n",
      "   Microsoft Windows Operating System\n",
      "  \n",
      "     ‚Ä¢ Company             : \n",
      "   Microsoft Corporation\n",
      "  \n",
      "     ‚Ä¢ OriginalFileName    : \n",
      "   CLBCATQ.DLL\n",
      "  \n",
      "     ‚Ä¢ Hashes              : \n",
      "   SHA256=2D7E8C878F2B200159279CAC7F1FDDCA15B2C18E03EC9510823559C4716DD1BA\n",
      "  \n",
      "     ‚Ä¢ Signed              : \n",
      "   true\n",
      "  \n",
      "     ‚Ä¢ Signature           : \n",
      "   Microsoft Windows\n",
      "  \n",
      "     ‚Ä¢ SignatureStatus     : \n",
      "   Valid\n",
      "  \n",
      "     ‚Ä¢ User                : \n",
      "   NT AUTHORITY\\NETWORK SERVICE\n",
      "  \n",
      "\n",
      "üîç PATTERN #4\n",
      "----------------------------------------\n",
      "üìä Frequency: 27,251 records (13.63%)\n",
      "üîë Structure Hash: b09e9b0fe33e3302...\n",
      "üéØ EventID(s): [10]\n",
      "üìè Field count: 13\n",
      "üñ•Ô∏è Computer count: 4\n",
      "üìã Fields: ['CallTrace', 'GrantedAccess', 'RuleName', 'SourceImage', 'SourceProcessGUID', 'SourceProcessId', 'SourceThreadId', 'SourceUser', 'TargetImage', 'TargetProcessGUID', 'TargetProcessId', 'TargetUser', 'UtcTime']\n",
      "\n",
      "üìÑ Sample Record:\n",
      "   EventID: 10\n",
      "   Computer: \n",
      "   diskjockey.boombox.local\n",
      "  \n",
      "   @timestamp: 2025-05-04T11:30:05.479Z\n",
      "   Fields:\n",
      "     ‚Ä¢ RuleName            : \n",
      "   -\n",
      "  \n",
      "     ‚Ä¢ UtcTime             : \n",
      "   2025-05-04 11:30:05.479\n",
      "  \n",
      "     ‚Ä¢ SourceProcessGUID   : \n",
      "   {acb80d05-4f89-6817-0c00-000000001600}\n",
      "  \n",
      "     ‚Ä¢ SourceProcessId     : \n",
      "   596\n",
      "  \n",
      "     ‚Ä¢ SourceThreadId      : \n",
      "   3800\n",
      "  \n",
      "     ‚Ä¢ SourceImage         : \n",
      "   C:\\Windows\\system32\\lsass.exe\n",
      "  \n",
      "     ‚Ä¢ TargetProcessGUID   : \n",
      "   {acb80d05-4f9d-6817-3a00-000000001600}\n",
      "  \n",
      "     ‚Ä¢ TargetProcessId     : \n",
      "   2976\n",
      "  \n",
      "     ‚Ä¢ TargetImage         : \n",
      "   C:\\Windows\\system32\\sppsvc.exe\n",
      "  \n",
      "     ‚Ä¢ GrantedAccess       : \n",
      "   0x1000\n",
      "  \n",
      "     ‚Ä¢ CallTrace           : \n",
      "   C:\\Windows\\SYSTEM32\\ntdll.dll+a0cb4|C:\\Windows\\system32\\lsasrv.dll+b7c8|C:\\W...\n",
      "     ‚Ä¢ SourceUser          : \n",
      "   NT AUTHORITY\\SYSTEM\n",
      "  \n",
      "     ‚Ä¢ TargetUser          : \n",
      "   NT AUTHORITY\\NETWORK SERVICE\n",
      "  \n",
      "\n",
      "üîç PATTERN #5\n",
      "----------------------------------------\n",
      "üìä Frequency: 8,269 records (4.13%)\n",
      "üîë Structure Hash: 02c108b064efacd3...\n",
      "üéØ EventID(s): [3]\n",
      "üìè Field count: 18\n",
      "üñ•Ô∏è Computer count: 4\n",
      "üìã Fields: ['DestinationHostname', 'DestinationIp', 'DestinationIsIpv6', 'DestinationPort', 'DestinationPortName', 'Image', 'Initiated', 'ProcessGuid', 'ProcessId', 'Protocol', 'RuleName', 'SourceHostname', 'SourceIp', 'SourceIsIpv6', 'SourcePort', 'SourcePortName', 'User', 'UtcTime']\n",
      "\n",
      "üìÑ Sample Record:\n",
      "   EventID: 3\n",
      "   Computer: \n",
      "   diskjockey.boombox.local\n",
      "  \n",
      "   @timestamp: 2025-05-04T11:30:01.044Z\n",
      "   Fields:\n",
      "     ‚Ä¢ RuleName            : \n",
      "   -\n",
      "  \n",
      "     ‚Ä¢ UtcTime             : \n",
      "   2025-05-04 11:30:01.044\n",
      "  \n",
      "     ‚Ä¢ ProcessGuid         : \n",
      "   {acb80d05-4f8b-6817-1700-000000001600}\n",
      "  \n",
      "     ‚Ä¢ ProcessId           : \n",
      "   1072\n",
      "  \n",
      "     ‚Ä¢ Image               : \n",
      "   C:\\Windows\\System32\\svchost.exe\n",
      "  \n",
      "     ‚Ä¢ User                : \n",
      "   NT AUTHORITY\\NETWORK SERVICE\n",
      "  \n",
      "     ‚Ä¢ Protocol            : \n",
      "   udp\n",
      "  \n",
      "     ‚Ä¢ Initiated           : \n",
      "   true\n",
      "  \n",
      "     ‚Ä¢ SourceIsIpv6        : \n",
      "   false\n",
      "  \n",
      "     ‚Ä¢ SourceIp            : \n",
      "   10.1.0.4\n",
      "  \n",
      "     ‚Ä¢ SourceHostname      : \n",
      "   diskjockey.boombox.local\n",
      "  \n",
      "     ‚Ä¢ SourcePort          : \n",
      "   60524\n",
      "  \n",
      "     ‚Ä¢ SourcePortName      : \n",
      "   -\n",
      "  \n",
      "     ‚Ä¢ DestinationIsIpv6   : \n",
      "   false\n",
      "  \n",
      "     ‚Ä¢ DestinationIp       : \n",
      "   10.1.0.4\n",
      "  \n",
      "     ‚Ä¢ DestinationHostname : \n",
      "   diskjockey.boombox.local\n",
      "  \n",
      "     ‚Ä¢ DestinationPort     : \n",
      "   53\n",
      "  \n",
      "     ‚Ä¢ DestinationPortName : \n",
      "   domain\n",
      "  \n",
      "\n",
      "üè∑Ô∏è PATTERN CLASSIFICATION:\n",
      "========================================\n",
      "‚Ä¢ COMMON patterns (‚â•5%): 4\n",
      "‚Ä¢ UNCOMMON patterns (1-5%): 2\n",
      "‚Ä¢ RARE patterns (<1%): 14\n",
      "‚Ä¢ Total patterns: 20\n",
      "\n",
      "------------------------------------------------------------ END SECTION ------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Start section logging\n",
    "log_print(\"\\n\" + \"=\" * 80)\n",
    "log_print(\"SECTION 6: DETAILED PATTERN ANALYSIS\")\n",
    "log_print(\"=\" * 80)\n",
    "log_print(\"\")\n",
    "\n",
    "log_print(\"üìã DETAILED PATTERN ANALYSIS\")\n",
    "log_print(\"=\" * 60)\n",
    "\n",
    "# Show top patterns with detailed analysis\n",
    "top_patterns = pattern_frequencies[:5]  # Top 5 most common patterns\n",
    "\n",
    "for idx, (fingerprint, count) in enumerate(top_patterns, 1):\n",
    "    pattern_info = structure_patterns[fingerprint]\n",
    "    sample = pattern_samples[fingerprint]\n",
    "    \n",
    "    percentage = (count / parsing_stats['success']) * 100\n",
    "    \n",
    "    log_print(f\"\\nüîç PATTERN #{idx}\")\n",
    "    log_print(\"-\" * 40)\n",
    "    log_print(f\"üìä Frequency: {count:,} records ({percentage:.2f}%)\")\n",
    "    log_print(f\"üîë Structure Hash: {fingerprint[:16]}...\")\n",
    "    log_print(f\"üéØ EventID(s): {sorted(list(pattern_info['event_ids']))}\")\n",
    "    log_print(f\"üìè Field count: {pattern_info['field_count']}\")\n",
    "    log_print(f\"üñ•Ô∏è Computer count: {len(pattern_info['computers'])}\")\n",
    "    \n",
    "    # Show field names\n",
    "    field_names = sorted(list(pattern_info['field_names']))\n",
    "    log_print(f\"üìã Fields: {field_names}\")\n",
    "    \n",
    "    # Show sample record\n",
    "    log_print(f\"\\nüìÑ Sample Record:\")\n",
    "    log_print(f\"   EventID: {sample['event_id']}\")\n",
    "    log_print(f\"   Computer: {sample['computer']}\")\n",
    "    log_print(f\"   @timestamp: {sample['@timestamp']}\")\n",
    "    log_print(f\"   Fields:\")\n",
    "    \n",
    "    for field_name, field_value in sample['fields'].items():\n",
    "        # Truncate long values for readability\n",
    "        display_value = str(field_value)[:80] + \"...\" if field_value and len(str(field_value)) > 80 else field_value\n",
    "        log_print(f\"     ‚Ä¢ {field_name:20s}: {display_value}\")\n",
    "\n",
    "# Pattern classification\n",
    "log_print(f\"\\nüè∑Ô∏è PATTERN CLASSIFICATION:\")\n",
    "log_print(\"=\" * 40)\n",
    "\n",
    "# Classify patterns by frequency\n",
    "total_patterns = len(structure_patterns)\n",
    "common_threshold = parsing_stats['success'] * 0.05  # 5% threshold\n",
    "rare_threshold = parsing_stats['success'] * 0.01    # 1% threshold\n",
    "\n",
    "common_patterns = sum(1 for _, count in pattern_frequencies if count >= common_threshold)\n",
    "uncommon_patterns = sum(1 for _, count in pattern_frequencies if rare_threshold <= count < common_threshold)\n",
    "rare_patterns = sum(1 for _, count in pattern_frequencies if count < rare_threshold)\n",
    "\n",
    "log_print(f\"‚Ä¢ COMMON patterns (‚â•5%): {common_patterns}\")\n",
    "log_print(f\"‚Ä¢ UNCOMMON patterns (1-5%): {uncommon_patterns}\")\n",
    "log_print(f\"‚Ä¢ RARE patterns (<1%): {rare_patterns}\")\n",
    "log_print(f\"‚Ä¢ Total patterns: {total_patterns}\")\n",
    "\n",
    "# End section logging\n",
    "log_print(\"\\n\" + \"-\" * 60 + \" END SECTION \" + \"-\" * 60)\n",
    "log_print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Field Co-occurrence Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SECTION 7: FIELD CO-OCCURRENCE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "üîó FIELD CO-OCCURRENCE ANALYSIS\n",
      "==================================================\n",
      "üìä Field presence analysis:\n",
      "   ‚Ä¢ Total unique field names: 68\n",
      "   ‚Ä¢ Unique field combinations: 17\n",
      "\n",
      "üìà Most common fields (top 15):\n",
      "   UtcTime                         200,000 (100.0%)\n",
      "   RuleName                        199,998 (100.0%)\n",
      "   ProcessId                       172,566 ( 86.3%)\n",
      "   Image                           172,566 ( 86.3%)\n",
      "   ProcessGuid                     172,566 ( 86.3%)\n",
      "   User                            172,566 ( 86.3%)\n",
      "   EventType                       128,368 ( 64.2%)\n",
      "   TargetObject                    127,428 ( 63.7%)\n",
      "   Details                          37,973 ( 19.0%)\n",
      "   Hashes                           33,043 ( 16.5%)\n",
      "   Description                      31,867 ( 15.9%)\n",
      "   FileVersion                      31,867 ( 15.9%)\n",
      "   Product                          31,867 ( 15.9%)\n",
      "   OriginalFileName                 31,867 ( 15.9%)\n",
      "   Company                          31,867 ( 15.9%)\n",
      "\n",
      "üéØ Most common field combinations (top 8):\n",
      "   Combination with  8 fields:   89,455 records ( 44.7%)\n",
      "   Combination with  9 fields:   37,973 records ( 19.0%)\n",
      "   Combination with 16 fields:   31,262 records ( 15.6%)\n",
      "   Combination with 13 fields:   27,251 records ( 13.6%)\n",
      "   Combination with 18 fields:    8,269 records (  4.1%)\n",
      "   Combination with  8 fields:    2,074 records (  1.0%)\n",
      "   Combination with 10 fields:      992 records (  0.5%)\n",
      "   Combination with  8 fields:      940 records (  0.5%)\n",
      "\n",
      "üìã EventID-specific field patterns:\n",
      "\n",
      "   EventID 1 (Process Creation):\n",
      "     ‚Ä¢ Description              :    605 (100.0%)\n",
      "     ‚Ä¢ FileVersion              :    605 (100.0%)\n",
      "     ‚Ä¢ CommandLine              :    605 (100.0%)\n",
      "     ‚Ä¢ ProcessGuid              :    605 (100.0%)\n",
      "     ‚Ä¢ Product                  :    605 (100.0%)\n",
      "     ‚Ä¢ LogonId                  :    605 (100.0%)\n",
      "     ‚Ä¢ IntegrityLevel           :    605 (100.0%)\n",
      "     ‚Ä¢ OriginalFileName         :    605 (100.0%)\n",
      "     ... and 15 more fields\n",
      "\n",
      "   EventID 2 (File Creation Time Changed):\n",
      "     ‚Ä¢ UtcTime                  :     30 (100.0%)\n",
      "     ‚Ä¢ TargetFilename           :     30 (100.0%)\n",
      "     ‚Ä¢ RuleName                 :     30 (100.0%)\n",
      "     ‚Ä¢ CreationUtcTime          :     30 (100.0%)\n",
      "     ‚Ä¢ PreviousCreationUtcTime  :     30 (100.0%)\n",
      "     ‚Ä¢ ProcessId                :     30 (100.0%)\n",
      "     ‚Ä¢ Image                    :     30 (100.0%)\n",
      "     ‚Ä¢ ProcessGuid              :     30 (100.0%)\n",
      "     ... and 1 more fields\n",
      "\n",
      "   EventID 3 (Network Connection):\n",
      "     ‚Ä¢ SourceIsIpv6             :  8,269 (100.0%)\n",
      "     ‚Ä¢ SourceHostname           :  8,269 (100.0%)\n",
      "     ‚Ä¢ UtcTime                  :  8,269 (100.0%)\n",
      "     ‚Ä¢ Protocol                 :  8,269 (100.0%)\n",
      "     ‚Ä¢ RuleName                 :  8,269 (100.0%)\n",
      "     ‚Ä¢ Initiated                :  8,269 (100.0%)\n",
      "     ‚Ä¢ DestinationIsIpv6        :  8,269 (100.0%)\n",
      "     ‚Ä¢ DestinationIp            :  8,269 (100.0%)\n",
      "     ... and 10 more fields\n",
      "\n",
      "   EventID 4 (Sysmon Service State Changed):\n",
      "     ‚Ä¢ UtcTime                  :      2 (100.0%)\n",
      "     ‚Ä¢ Version                  :      2 (100.0%)\n",
      "     ‚Ä¢ State                    :      2 (100.0%)\n",
      "     ‚Ä¢ SchemaVersion            :      2 (100.0%)\n",
      "\n",
      "   EventID 5 (Process Terminated):\n",
      "     ‚Ä¢ UtcTime                  :    478 (100.0%)\n",
      "     ‚Ä¢ RuleName                 :    478 (100.0%)\n",
      "     ‚Ä¢ ProcessId                :    478 (100.0%)\n",
      "     ‚Ä¢ Image                    :    478 (100.0%)\n",
      "     ‚Ä¢ ProcessGuid              :    478 (100.0%)\n",
      "     ‚Ä¢ User                     :    478 (100.0%)\n",
      "\n",
      "   EventID 6 (Driver Loaded):\n",
      "     ‚Ä¢ UtcTime                  :    181 (100.0%)\n",
      "     ‚Ä¢ RuleName                 :    181 (100.0%)\n",
      "     ‚Ä¢ Signed                   :    181 (100.0%)\n",
      "     ‚Ä¢ Hashes                   :    181 (100.0%)\n",
      "     ‚Ä¢ Signature                :    181 (100.0%)\n",
      "     ‚Ä¢ SignatureStatus          :    181 (100.0%)\n",
      "     ‚Ä¢ ImageLoaded              :    181 (100.0%)\n",
      "\n",
      "   EventID 7 (Image/Library Loaded):\n",
      "     ‚Ä¢ UtcTime                  : 31,262 (100.0%)\n",
      "     ‚Ä¢ Description              : 31,262 (100.0%)\n",
      "     ‚Ä¢ FileVersion              : 31,262 (100.0%)\n",
      "     ‚Ä¢ RuleName                 : 31,262 (100.0%)\n",
      "     ‚Ä¢ Signed                   : 31,262 (100.0%)\n",
      "     ‚Ä¢ SignatureStatus          : 31,262 (100.0%)\n",
      "     ‚Ä¢ ProcessId                : 31,262 (100.0%)\n",
      "     ‚Ä¢ Image                    : 31,262 (100.0%)\n",
      "     ... and 8 more fields\n",
      "\n",
      "   EventID 9 (Raw Access Read):\n",
      "     ‚Ä¢ UtcTime                  :    460 (100.0%)\n",
      "     ‚Ä¢ RuleName                 :    460 (100.0%)\n",
      "     ‚Ä¢ ProcessId                :    460 (100.0%)\n",
      "     ‚Ä¢ Image                    :    460 (100.0%)\n",
      "     ‚Ä¢ Device                   :    460 (100.0%)\n",
      "     ‚Ä¢ ProcessGuid              :    460 (100.0%)\n",
      "     ‚Ä¢ User                     :    460 (100.0%)\n",
      "\n",
      "   EventID 10 (Process Access):\n",
      "     ‚Ä¢ UtcTime                  : 27,251 (100.0%)\n",
      "     ‚Ä¢ TargetProcessGUID        : 27,251 (100.0%)\n",
      "     ‚Ä¢ RuleName                 : 27,251 (100.0%)\n",
      "     ‚Ä¢ SourceProcessId          : 27,251 (100.0%)\n",
      "     ‚Ä¢ SourceProcessGUID        : 27,251 (100.0%)\n",
      "     ‚Ä¢ SourceImage              : 27,251 (100.0%)\n",
      "     ‚Ä¢ TargetUser               : 27,251 (100.0%)\n",
      "     ‚Ä¢ TargetProcessId          : 27,251 (100.0%)\n",
      "     ... and 5 more fields\n",
      "\n",
      "   EventID 11 (File Create):\n",
      "     ‚Ä¢ UtcTime                  :  2,074 (100.0%)\n",
      "     ‚Ä¢ TargetFilename           :  2,074 (100.0%)\n",
      "     ‚Ä¢ RuleName                 :  2,074 (100.0%)\n",
      "     ‚Ä¢ CreationUtcTime          :  2,074 (100.0%)\n",
      "     ‚Ä¢ ProcessId                :  2,074 (100.0%)\n",
      "     ‚Ä¢ Image                    :  2,074 (100.0%)\n",
      "     ‚Ä¢ ProcessGuid              :  2,074 (100.0%)\n",
      "     ‚Ä¢ User                     :  2,074 (100.0%)\n",
      "\n",
      "   EventID 12 (Registry Event (Object create/delete)):\n",
      "     ‚Ä¢ UtcTime                  : 89,455 (100.0%)\n",
      "     ‚Ä¢ RuleName                 : 89,455 (100.0%)\n",
      "     ‚Ä¢ User                     : 89,455 (100.0%)\n",
      "     ‚Ä¢ ProcessId                : 89,455 (100.0%)\n",
      "     ‚Ä¢ Image                    : 89,455 (100.0%)\n",
      "     ‚Ä¢ EventType                : 89,455 (100.0%)\n",
      "     ‚Ä¢ ProcessGuid              : 89,455 (100.0%)\n",
      "     ‚Ä¢ TargetObject             : 89,455 (100.0%)\n",
      "\n",
      "   EventID 13 (Registry Event (Value Set)):\n",
      "     ‚Ä¢ UtcTime                  : 37,973 (100.0%)\n",
      "     ‚Ä¢ Details                  : 37,973 (100.0%)\n",
      "     ‚Ä¢ RuleName                 : 37,973 (100.0%)\n",
      "     ‚Ä¢ User                     : 37,973 (100.0%)\n",
      "     ‚Ä¢ ProcessId                : 37,973 (100.0%)\n",
      "     ‚Ä¢ Image                    : 37,973 (100.0%)\n",
      "     ‚Ä¢ EventType                : 37,973 (100.0%)\n",
      "     ‚Ä¢ ProcessGuid              : 37,973 (100.0%)\n",
      "     ... and 1 more fields\n",
      "\n",
      "   EventID 15 (File Create Stream Hash):\n",
      "     ‚Ä¢ UtcTime                  :     22 (100.0%)\n",
      "     ‚Ä¢ Contents                 :     22 (100.0%)\n",
      "     ‚Ä¢ TargetFilename           :     22 (100.0%)\n",
      "     ‚Ä¢ RuleName                 :     22 (100.0%)\n",
      "     ‚Ä¢ CreationUtcTime          :     22 (100.0%)\n",
      "     ‚Ä¢ ProcessId                :     22 (100.0%)\n",
      "     ‚Ä¢ Image                    :     22 (100.0%)\n",
      "     ‚Ä¢ Hash                     :     22 (100.0%)\n",
      "     ... and 2 more fields\n",
      "\n",
      "   EventID 17 (Pipe Event (Pipe Created)):\n",
      "     ‚Ä¢ UtcTime                  :    250 (100.0%)\n",
      "     ‚Ä¢ PipeName                 :    250 (100.0%)\n",
      "     ‚Ä¢ RuleName                 :    250 (100.0%)\n",
      "     ‚Ä¢ ProcessId                :    250 (100.0%)\n",
      "     ‚Ä¢ Image                    :    250 (100.0%)\n",
      "     ‚Ä¢ EventType                :    250 (100.0%)\n",
      "     ‚Ä¢ ProcessGuid              :    250 (100.0%)\n",
      "     ‚Ä¢ User                     :    250 (100.0%)\n",
      "\n",
      "   EventID 18 (Pipe Event (Pipe Connected)):\n",
      "     ‚Ä¢ UtcTime                  :    690 (100.0%)\n",
      "     ‚Ä¢ PipeName                 :    690 (100.0%)\n",
      "     ‚Ä¢ RuleName                 :    690 (100.0%)\n",
      "     ‚Ä¢ ProcessId                :    690 (100.0%)\n",
      "     ‚Ä¢ Image                    :    690 (100.0%)\n",
      "     ‚Ä¢ EventType                :    690 (100.0%)\n",
      "     ‚Ä¢ ProcessGuid              :    690 (100.0%)\n",
      "     ‚Ä¢ User                     :    690 (100.0%)\n",
      "\n",
      "   EventID 23 (File Delete (File Delete archived)):\n",
      "     ‚Ä¢ UtcTime                  :    992 (100.0%)\n",
      "     ‚Ä¢ TargetFilename           :    992 (100.0%)\n",
      "     ‚Ä¢ RuleName                 :    992 (100.0%)\n",
      "     ‚Ä¢ Archived                 :    992 (100.0%)\n",
      "     ‚Ä¢ ProcessId                :    992 (100.0%)\n",
      "     ‚Ä¢ Image                    :    992 (100.0%)\n",
      "     ‚Ä¢ Hashes                   :    992 (100.0%)\n",
      "     ‚Ä¢ ProcessGuid              :    992 (100.0%)\n",
      "     ... and 2 more fields\n",
      "\n",
      "   EventID 24 (Clipboard Change (New content in clipboard)):\n",
      "     ‚Ä¢ UtcTime                  :      3 (100.0%)\n",
      "     ‚Ä¢ RuleName                 :      3 (100.0%)\n",
      "     ‚Ä¢ ClientInfo               :      3 (100.0%)\n",
      "     ‚Ä¢ Archived                 :      3 (100.0%)\n",
      "     ‚Ä¢ ProcessId                :      3 (100.0%)\n",
      "     ‚Ä¢ Image                    :      3 (100.0%)\n",
      "     ‚Ä¢ Hashes                   :      3 (100.0%)\n",
      "     ‚Ä¢ ProcessGuid              :      3 (100.0%)\n",
      "     ... and 2 more fields\n",
      "\n",
      "   EventID 25 (Process Tampering (Process image change)):\n",
      "     ‚Ä¢ UtcTime                  :      3 (100.0%)\n",
      "     ‚Ä¢ RuleName                 :      3 (100.0%)\n",
      "     ‚Ä¢ ProcessId                :      3 (100.0%)\n",
      "     ‚Ä¢ Image                    :      3 (100.0%)\n",
      "     ‚Ä¢ Type                     :      3 (100.0%)\n",
      "     ‚Ä¢ ProcessGuid              :      3 (100.0%)\n",
      "     ‚Ä¢ User                     :      3 (100.0%)\n",
      "\n",
      "------------------------------------------------------------ END SECTION ------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Start section logging\n",
    "log_print(\"\\n\" + \"=\" * 80)\n",
    "log_print(\"SECTION 7: FIELD CO-OCCURRENCE ANALYSIS\")\n",
    "log_print(\"=\" * 80)\n",
    "log_print(\"\")\n",
    "\n",
    "log_print(\"üîó FIELD CO-OCCURRENCE ANALYSIS\")\n",
    "log_print(\"=\" * 50)\n",
    "\n",
    "# Collect field statistics\n",
    "field_counts = Counter()\n",
    "field_combinations = Counter()\n",
    "eventid_field_matrix = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "# Process all samples for field statistics\n",
    "for fingerprint, pattern_info in structure_patterns.items():\n",
    "    field_set = pattern_info['field_names']\n",
    "    record_count = pattern_info['count']\n",
    "    event_ids = pattern_info['event_ids']\n",
    "    \n",
    "    # Count individual fields\n",
    "    for field in field_set:\n",
    "        field_counts[field] += record_count\n",
    "    \n",
    "    # Count field combinations (as frozenset for consistency)\n",
    "    field_combination = frozenset(field_set)\n",
    "    field_combinations[field_combination] += record_count\n",
    "    \n",
    "    # Build EventID-field matrix\n",
    "    for event_id in event_ids:\n",
    "        for field in field_set:\n",
    "            eventid_field_matrix[event_id][field] += record_count\n",
    "\n",
    "log_print(f\"üìä Field presence analysis:\")\n",
    "log_print(f\"   ‚Ä¢ Total unique field names: {len(field_counts)}\")\n",
    "log_print(f\"   ‚Ä¢ Unique field combinations: {len(field_combinations)}\")\n",
    "\n",
    "# Most common fields\n",
    "log_print(f\"\\nüìà Most common fields (top 15):\")\n",
    "for field, count in field_counts.most_common(15):\n",
    "    percentage = (count / parsing_stats['success']) * 100\n",
    "    log_print(f\"   {field:30s} {count:8,} ({percentage:5.1f}%)\")\n",
    "\n",
    "# Field combinations\n",
    "log_print(f\"\\nüéØ Most common field combinations (top 8):\")\n",
    "for field_combo, count in field_combinations.most_common(8):\n",
    "    percentage = (count / parsing_stats['success']) * 100\n",
    "    combo_size = len(field_combo)\n",
    "    log_print(f\"   Combination with {combo_size:2d} fields: {count:8,} records ({percentage:5.1f}%)\")\n",
    "\n",
    "# EventID-specific field analysis\n",
    "log_print(f\"\\nüìã EventID-specific field patterns:\")\n",
    "for event_id in sorted(eventid_field_matrix.keys()):\n",
    "    fields_for_eventid = eventid_field_matrix[event_id]\n",
    "    total_records_eventid = sum(fields_for_eventid.values()) // len(fields_for_eventid) if fields_for_eventid else 0\n",
    "    \n",
    "    if total_records_eventid > 0:\n",
    "        description = eventid_descriptions.get(event_id, \"Unknown\")\n",
    "        log_print(f\"\\n   EventID {event_id} ({description}):\")\n",
    "        \n",
    "        # Show top fields for this EventID\n",
    "        eventid_field_list = [(field, count) for field, count in fields_for_eventid.items()]\n",
    "        eventid_field_list.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        for field, count in eventid_field_list[:8]:  # Top 8 fields\n",
    "            percentage = (count / total_records_eventid) * 100 if total_records_eventid > 0 else 0\n",
    "            log_print(f\"     ‚Ä¢ {field:25s}: {count:6,} ({percentage:5.1f}%)\")\n",
    "        \n",
    "        if len(eventid_field_list) > 8:\n",
    "            log_print(f\"     ... and {len(eventid_field_list) - 8} more fields\")\n",
    "\n",
    "# End section logging\n",
    "log_print(\"\\n\" + \"-\" * 60 + \" END SECTION \" + \"-\" * 60)\n",
    "log_print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Structure Consistency Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SECTION 8: STRUCTURE CONSISTENCY REPORT\n",
      "================================================================================\n",
      "\n",
      "üìä STRUCTURE CONSISTENCY REPORT\n",
      "============================================================\n",
      "üîç CONSISTENCY METRICS:\n",
      "   ‚Ä¢ Total unique structure patterns: 20\n",
      "   ‚Ä¢ Total EventIDs analyzed: 18\n",
      "   ‚Ä¢ Consistent EventIDs (single pattern): 16\n",
      "   ‚Ä¢ Inconsistent EventIDs (multiple patterns): 2\n",
      "   ‚Ä¢ Structure diversity ratio: 0.010%\n",
      "\n",
      "üìà COVERAGE ANALYSIS:\n",
      "   ‚Ä¢ Top 3 patterns cover: 79.3% of data\n",
      "   ‚Ä¢ Top 5 patterns cover: 97.1% of data\n",
      "   ‚Ä¢ Top 10 patterns cover: 99.5% of data\n",
      "\n",
      "üü¢ OVERALL ASSESSMENT: HIGHLY CONSISTENT\n",
      "\n",
      "üí° PROCESSING RECOMMENDATIONS:\n",
      "   üü¢ High consistency - recommend EventID-specific processing\n",
      "   üü¢ Most EventIDs have single, stable structure patterns\n",
      "   üü¢ Standard field mapping approach will work well\n",
      "\n",
      "üõ†Ô∏è FIELD HANDLING STRATEGIES:\n",
      "   ‚Ä¢ Always present fields (2): Standard extraction\n",
      "   ‚Ä¢ Conditional fields (6): Null handling required\n",
      "   ‚Ä¢ Rare fields (60): Consider exclusion or special handling\n",
      "\n",
      "üéØ EXECUTIVE SUMMARY:\n",
      "   Dataset shows highly consistent with 20 unique patterns.\n",
      "   16/18 EventIDs have consistent structure.\n",
      "   Recommended approach: EventID-specific processing.\n",
      "\n",
      "------------------------------------------------------------ END SECTION ------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Start section logging\n",
    "log_print(\"\\n\" + \"=\" * 80)\n",
    "log_print(\"SECTION 8: STRUCTURE CONSISTENCY REPORT\")\n",
    "log_print(\"=\" * 80)\n",
    "log_print(\"\")\n",
    "\n",
    "log_print(\"üìä STRUCTURE CONSISTENCY REPORT\")\n",
    "log_print(\"=\" * 60)\n",
    "\n",
    "# Calculate consistency metrics\n",
    "total_patterns = len(structure_patterns)\n",
    "total_eventids = len(eventid_patterns)\n",
    "total_records = parsing_stats['success']\n",
    "\n",
    "# Coverage analysis\n",
    "if pattern_frequencies:\n",
    "    top_3_coverage = sum(count for _, count in pattern_frequencies[:3])\n",
    "    top_5_coverage = sum(count for _, count in pattern_frequencies[:5])\n",
    "    top_10_coverage = sum(count for _, count in pattern_frequencies[:min(10, len(pattern_frequencies))])\n",
    "else:\n",
    "    top_3_coverage = top_5_coverage = top_10_coverage = 0\n",
    "\n",
    "# EventID consistency\n",
    "consistent_eventids = sum(1 for eid, patterns in eventid_patterns.items() if len(patterns) == 1)\n",
    "inconsistent_eventids = total_eventids - consistent_eventids\n",
    "\n",
    "log_print(f\"üîç CONSISTENCY METRICS:\")\n",
    "log_print(f\"   ‚Ä¢ Total unique structure patterns: {total_patterns}\")\n",
    "log_print(f\"   ‚Ä¢ Total EventIDs analyzed: {total_eventids}\")\n",
    "log_print(f\"   ‚Ä¢ Consistent EventIDs (single pattern): {consistent_eventids}\")\n",
    "log_print(f\"   ‚Ä¢ Inconsistent EventIDs (multiple patterns): {inconsistent_eventids}\")\n",
    "log_print(f\"   ‚Ä¢ Structure diversity ratio: {(total_patterns/total_records)*100:.3f}%\")\n",
    "\n",
    "log_print(f\"\\nüìà COVERAGE ANALYSIS:\")\n",
    "if total_records > 0:\n",
    "    log_print(f\"   ‚Ä¢ Top 3 patterns cover: {(top_3_coverage/total_records)*100:.1f}% of data\")\n",
    "    log_print(f\"   ‚Ä¢ Top 5 patterns cover: {(top_5_coverage/total_records)*100:.1f}% of data\")\n",
    "    log_print(f\"   ‚Ä¢ Top 10 patterns cover: {(top_10_coverage/total_records)*100:.1f}% of data\")\n",
    "\n",
    "# Determine overall assessment\n",
    "consistency_ratio = consistent_eventids / total_eventids if total_eventids > 0 else 0\n",
    "diversity_ratio = total_patterns / total_records if total_records > 0 else 1\n",
    "\n",
    "if consistency_ratio >= 0.8 and diversity_ratio <= 0.01:\n",
    "    assessment = \"HIGHLY CONSISTENT\"\n",
    "    status_emoji = \"üü¢\"\n",
    "elif consistency_ratio >= 0.6 and diversity_ratio <= 0.05:\n",
    "    assessment = \"MODERATELY CONSISTENT\"\n",
    "    status_emoji = \"üü°\"\n",
    "else:\n",
    "    assessment = \"INCONSISTENT - REQUIRES ATTENTION\"\n",
    "    status_emoji = \"üî¥\"\n",
    "\n",
    "log_print(f\"\\n{status_emoji} OVERALL ASSESSMENT: {assessment}\")\n",
    "\n",
    "# Processing recommendations\n",
    "log_print(f\"\\nüí° PROCESSING RECOMMENDATIONS:\")\n",
    "\n",
    "if consistency_ratio >= 0.8:\n",
    "    log_print(f\"   üü¢ High consistency - recommend EventID-specific processing\")\n",
    "    log_print(f\"   üü¢ Most EventIDs have single, stable structure patterns\")\n",
    "    log_print(f\"   üü¢ Standard field mapping approach will work well\")\n",
    "elif consistency_ratio >= 0.6:\n",
    "    log_print(f\"   üü° Moderate consistency - recommend hybrid processing\")\n",
    "    log_print(f\"   üü° Primary pattern for each EventID with fallback handling\")\n",
    "    log_print(f\"   üü° Field validation and error handling required\")\n",
    "else:\n",
    "    log_print(f\"   üî¥ Low consistency - recommend robust error handling\")\n",
    "    log_print(f\"   üî¥ Multiple processing pipelines may be needed\")\n",
    "    log_print(f\"   üî¥ Extensive field validation and schema flexibility required\")\n",
    "\n",
    "# Field handling strategies\n",
    "always_present_fields = [field for field, count in field_counts.items() if count >= total_records * 0.95]\n",
    "conditional_fields = [field for field, count in field_counts.items() if 0.5 <= (count/total_records) < 0.95]\n",
    "rare_fields = [field for field, count in field_counts.items() if count < total_records * 0.5]\n",
    "\n",
    "log_print(f\"\\nüõ†Ô∏è FIELD HANDLING STRATEGIES:\")\n",
    "log_print(f\"   ‚Ä¢ Always present fields ({len(always_present_fields)}): Standard extraction\")\n",
    "log_print(f\"   ‚Ä¢ Conditional fields ({len(conditional_fields)}): Null handling required\")\n",
    "log_print(f\"   ‚Ä¢ Rare fields ({len(rare_fields)}): Consider exclusion or special handling\")\n",
    "\n",
    "# Executive summary\n",
    "log_print(f\"\\nüéØ EXECUTIVE SUMMARY:\")\n",
    "log_print(f\"   Dataset shows {assessment.lower()} with {total_patterns} unique patterns.\")\n",
    "log_print(f\"   {consistent_eventids}/{total_eventids} EventIDs have consistent structure.\")\n",
    "log_print(f\"   Recommended approach: {'EventID-specific' if consistency_ratio >= 0.8 else 'Hybrid with validation' if consistency_ratio >= 0.6 else 'Robust multi-pattern'} processing.\")\n",
    "\n",
    "# End section logging\n",
    "log_print(\"\\n\" + \"-\" * 60 + \" END SECTION \" + \"-\" * 60)\n",
    "log_print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Analysis Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saving detailed analysis results...\n",
      "‚úÖ Results saved to: outputs/2b-sysmon/2b-sysmon_structure_results_20250629_113637.json\n",
      "üìÅ Output directory: outputs/2b-sysmon\n",
      "üéâ Sysmon structure consistency analysis complete!\n",
      "\n",
      "üìã Analysis complete! Results saved to: outputs/2b-sysmon/2b-sysmon_structure_analysis_20250629_113637.log\n",
      "üìä Detailed results saved to: outputs/2b-sysmon/2b-sysmon_structure_results_20250629_113637.json\n",
      "üìÅ Output directory: outputs/2b-sysmon\n",
      "üéâ Sysmon structure consistency analysis complete!\n"
     ]
    }
   ],
   "source": [
    "# Prepare results for JSON export\n",
    "results = {\n",
    "    'analysis_metadata': {\n",
    "        'analysis_type': ANALYSIS_TYPE,\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'target_file': TARGET_FILE,\n",
    "        'sample_size': len(collected_samples),\n",
    "        'total_records': total_records\n",
    "    },\n",
    "    'parsing_statistics': parsing_stats,\n",
    "    'structure_patterns': {\n",
    "        'total_patterns': total_patterns,\n",
    "        'pattern_frequencies': pattern_frequencies[:10],  # Top 10\n",
    "        'pattern_classification': {\n",
    "            'common': common_patterns,\n",
    "            'uncommon': uncommon_patterns,\n",
    "            'rare': rare_patterns\n",
    "        }\n",
    "    },\n",
    "    'eventid_analysis': {\n",
    "        'total_eventids': total_eventids,\n",
    "        'consistent_eventids': consistent_eventids,\n",
    "        'inconsistent_eventids': inconsistent_eventids,\n",
    "        'eventid_pattern_counts': {str(eid): len(patterns) for eid, patterns in eventid_patterns.items()}\n",
    "    },\n",
    "    'field_analysis': {\n",
    "        'total_unique_fields': len(field_counts),\n",
    "        'always_present_fields': always_present_fields,\n",
    "        'conditional_fields': conditional_fields[:20],  # Limit size\n",
    "        'rare_fields': rare_fields[:20],  # Limit size\n",
    "        'field_combinations': len(field_combinations)\n",
    "    },\n",
    "    'consistency_metrics': {\n",
    "        'consistency_ratio': consistency_ratio,\n",
    "        'diversity_ratio': diversity_ratio,\n",
    "        'assessment': assessment,\n",
    "        'coverage': {\n",
    "            'top_3': (top_3_coverage/total_records)*100 if total_records > 0 else 0,\n",
    "            'top_5': (top_5_coverage/total_records)*100 if total_records > 0 else 0,\n",
    "            'top_10': (top_10_coverage/total_records)*100 if total_records > 0 else 0\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save results\n",
    "log_print(f\"üíæ Saving detailed analysis results...\")\n",
    "with open(results_filename, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "log_print(f\"‚úÖ Results saved to: {results_filename}\")\n",
    "log_print(f\"üìÅ Output directory: {analysis_outputs_dir}\")\n",
    "log_print(f\"üéâ Sysmon structure consistency analysis complete!\")\n",
    "\n",
    "print(f\"\\nüìã Analysis complete! Results saved to: {log_filename}\")\n",
    "print(f\"üìä Detailed results saved to: {results_filename}\")\n",
    "print(f\"üìÅ Output directory: {analysis_outputs_dir}\")\n",
    "print(f\"üéâ Sysmon structure consistency analysis complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataset-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
