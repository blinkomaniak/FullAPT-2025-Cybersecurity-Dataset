{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sysmon JSONL to CSV Converter\n",
    "\n",
    "## üìñ Overview\n",
    "\n",
    "This notebook transforms Windows Sysmon events from JSONL format (extracted from Elasticsearch) into structured CSV datasets for machine learning analysis. It processes raw Windows Event Log XML embedded within JSONL files and converts them into tabular format with proper field mapping per event type.\n",
    "\n",
    "### üéØ Purpose\n",
    "\n",
    "- **Parse** Windows Event Log XML from JSONL files  \n",
    "- **Extract** structured data using event-specific field schemas\n",
    "- **Transform** multi-format security events into unified CSV structure\n",
    "- **Clean** and validate data for machine learning pipeline readiness\n",
    "\n",
    "### üìä Input/Output\n",
    "\n",
    "- **Input**: JSONL file with Windows Sysmon events (from notebook #1)\n",
    "- **Output**: Structured CSV file with event-specific columns and proper field mapping\n",
    "\n",
    "### üîß Key Features\n",
    "\n",
    "- **Schema-based parsing** for 18+ different Sysmon event types\n",
    "- **Robust XML handling** with sanitization and error recovery\n",
    "- **Detailed logging** for debugging and quality assessment\n",
    "- **Field mapping validation** with missing field tracking\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Required Libraries\n",
    "\n",
    "Import essential libraries for XML parsing, data manipulation, and file processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Sysmon Event Schema Mapping\n",
    "\n",
    "Define field mappings for each Sysmon EventID based on official documentation and data analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fields obtained from https://www.ultimatewindowssecurity.com/securitylog/encyclopedia/ and manual inspection of the data\n",
    "# EventID 8 uses lowercase 'guid' in XML, but we'll map to uppercase 'GUID' columns in DataFrame for consistency\n",
    "fields_per_eventid = {\n",
    "1: ['UtcTime', 'ProcessGuid', 'ProcessId', 'Image', 'CommandLine', 'CurrentDirectory', 'User', 'Hashes', 'ParentProcessGuid', 'ParentProcessId', 'ParentImage', 'ParentCommandLine'],\n",
    "2: ['UtcTime', 'ProcessGuid', 'ProcessId', 'Image', 'TargetFilename', 'CreationUtcTime', 'PreviousCreationUtcTime', 'User'],\n",
    "3: ['UtcTime', 'ProcessGuid', 'ProcessId', 'Image', 'User', 'Protocol', 'SourceIsIpv6', 'SourceIp', 'SourceHostname', 'SourcePort', 'SourcePortName', 'DestinationIsIpv6', 'DestinationIp', 'DestinationHostname', 'DestinationPort', 'DestinationPortName'],\n",
    "5: ['UtcTime', 'ProcessGuid', 'ProcessId', 'Image', 'User'],\n",
    "6: ['UtcTime', 'ImageLoaded', 'Hashes', 'User'],\n",
    "7: ['UtcTime', 'ProcessGuid', 'ProcessId', 'Image', 'ImageLoaded', 'OriginalFileName', 'Hashes', 'User'],\n",
    "8: ['UtcTime', 'SourceProcessGuid', 'SourceProcessId', 'SourceImage', 'TargetProcessGuid', 'TargetProcessId', 'TargetImage', 'NewThreadId', 'SourceUser', 'TargetUser'],\n",
    "9: ['UtcTime', 'ProcessGuid', 'ProcessId', 'Image', 'Device', 'User'],\n",
    "10: ['UtcTime', 'SourceProcessGUID', 'SourceProcessId', 'SourceImage', 'TargetProcessGUID', 'TargetProcessId', 'TargetImage', 'SourceThreadId', 'SourceUser', 'TargetUser'],\n",
    "11: ['UtcTime', 'ProcessGuid', 'ProcessId', 'Image', 'TargetFilename', 'CreationUtcTime', 'User'],\n",
    "12: ['EventType', 'UtcTime', 'ProcessGuid', 'ProcessId', 'Image', 'TargetObject', 'User'],\n",
    "13: ['EventType', 'UtcTime', 'ProcessGuid', 'ProcessId', 'Image', 'TargetObject', 'User'],\n",
    "14: ['EventType', 'UtcTime', 'ProcessGuid', 'ProcessId', 'Image', 'TargetObject', 'User'],\n",
    "15: ['UtcTime', 'ProcessGuid', 'ProcessId', 'Image', 'TargetFilename', 'CreationUtcTime', 'Hash', 'User'],\n",
    "16: ['UtcTime', 'Configuration', 'ConfigurationFileHash', 'User'],\n",
    "17: ['EventType', 'UtcTime', 'ProcessGuid', 'ProcessId', 'PipeName', 'Image', 'User'],\n",
    "18: ['EventType', 'UtcTime', 'ProcessGuid', 'ProcessId', 'PipeName', 'Image', 'User'],\n",
    "22: ['UtcTime', 'ProcessGuid', 'ProcessId', 'Image', 'QueryName', 'QueryStatus', 'QueryResults', 'User'],\n",
    "23: ['UtcTime', 'ProcessGuid', 'ProcessId', 'User', 'Image', 'TargetFilename', 'Hashes'],\n",
    "24: ['UtcTime', 'ProcessGuid', 'ProcessId', 'User', 'Image', 'Hashes'],\n",
    "25: ['UtcTime', 'ProcessGuid', 'ProcessId', 'User', 'Image']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßπ XML Sanitization Function\n",
    "\n",
    "Clean and repair malformed XML strings before parsing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_xml(xml_str):\n",
    "    \"\"\"Clean invalid characters and repair XML structure\"\"\"\n",
    "    # Remove non-printable characters\n",
    "    cleaned = ''.join(c for c in xml_str if 31 < ord(c) < 127 or c in '\\t\\n\\r')\n",
    "    # Fix common XML issues using BeautifulSoup's parser\n",
    "    return BeautifulSoup(cleaned, \"xml\").prettify()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç XML Parser Function\n",
    "\n",
    "Parse Windows Event Log XML to extract EventID, Computer, and event-specific fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sysmon_event(xml_str):\n",
    "    \"\"\"Parse XML with enhanced error handling\"\"\"\n",
    "    try:\n",
    "        # Clean XML first\n",
    "        clean_xml = sanitize_xml(xml_str)\n",
    "        \n",
    "        # Parse with explicit namespace\n",
    "        namespaces = {'ns': 'http://schemas.microsoft.com/win/2004/08/events/event'}\n",
    "        root = ET.fromstring(clean_xml)\n",
    "        \n",
    "        # System section - with null checks\n",
    "        system = root.find('ns:System', namespaces)\n",
    "        if not system:\n",
    "            return None, None, None\n",
    "\n",
    "        event_id_elem = system.find('ns:EventID', namespaces)\n",
    "        computer_elem = system.find('ns:Computer', namespaces)\n",
    "        \n",
    "        # event_id = event_id_elem.text.strip() if (event_id_elem and event_id_elem.text) else None\n",
    "        event_id = int(event_id_elem.text) if event_id_elem is not None else None\n",
    "        \n",
    "        # computer = computer_elem.text.strip() if (computer_elem and computer_elem.text) else None\n",
    "        computer = computer_elem.text if computer_elem is not None else None\n",
    "\n",
    "        # EventData section\n",
    "        event_data = root.find('ns:EventData', namespaces)\n",
    "        fields = {}\n",
    "        if event_data:\n",
    "            for data in event_data.findall('ns:Data', namespaces):\n",
    "                name = data.get('Name')\n",
    "                # fields[name] = data.text.strip() if data.text else None\n",
    "                fields[name] = data.text if data.text else None\n",
    "\n",
    "        return event_id, computer, fields\n",
    "\n",
    "    except Exception as e:\n",
    "        # Log problematic XML samples for debugging\n",
    "        with open('bad_xml_samples.txt', 'a') as bad_xml:\n",
    "            bad_xml.write(f\"Error: {str(e)}\\n\")\n",
    "            bad_xml.write(f\"XML: {xml_str[:500]}...\\n\")\n",
    "            bad_xml.write(\"-\" * 50 + \"\\n\")\n",
    "        print(f\"XML parsing failed: {str(e)}\")\n",
    "        return None, None, None  # Crucial: return tuple of 3 Nones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Data Type Conversion Helper Functions\n",
    "\n",
    "Define utility functions for proper data type conversion and cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_int_conversion(value):\n",
    "    \"\"\"Safely convert value to integer, handling whitespace, NaN and invalid values\"\"\"\n",
    "    if value is None or pd.isna(value):\n",
    "        return None\n",
    "    try:\n",
    "        # Strip whitespace first, then convert\n",
    "        cleaned_value = str(value).strip()\n",
    "        if not cleaned_value:\n",
    "            return None\n",
    "        return int(float(cleaned_value))\n",
    "    except (ValueError, TypeError):\n",
    "        return None\n",
    "\n",
    "def clean_guid(value):\n",
    "    \"\"\"Remove whitespace and brackets from GUID values and ensure string type\"\"\"\n",
    "    if value is None or pd.isna(value):\n",
    "        return None\n",
    "    try:\n",
    "        # First strip whitespace, then remove curly brackets\n",
    "        cleaned = str(value).strip()\n",
    "        if not cleaned:\n",
    "            return None\n",
    "        # Remove curly brackets\n",
    "        cleaned = cleaned.strip('{}')\n",
    "        return cleaned if cleaned else None\n",
    "    except (ValueError, TypeError):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Main Processing Function\n",
    "\n",
    "Process JSONL file and convert Sysmon events to structured DataFrame with comprehensive error tracking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_events(jsonl_path):\n",
    "    \"\"\"Process events with error tracking and data type optimization\"\"\"\n",
    "    records = []\n",
    "    error_count = 0\n",
    "    missing_fields_tracker = {}  # Track missing fields per EventID\n",
    "    eventid_counts = {}  # Track total events per EventID\n",
    "    \n",
    "    # Define columns that should be integers\n",
    "    integer_columns = {\n",
    "        'ProcessId', 'SourcePort', 'DestinationPort', 'SourceProcessId', \n",
    "        'ParentProcessId', 'SourceThreadId', 'TargetProcessId'\n",
    "    }\n",
    "    \n",
    "    # Define columns that are GUIDs (need bracket stripping)\n",
    "    guid_columns = {\n",
    "        'ProcessGuid', 'SourceProcessGUID', 'TargetProcessGUID', 'ParentProcessGuid'\n",
    "    }\n",
    "    \n",
    "    with open(jsonl_path, 'r') as f:\n",
    "        for line_number, line in enumerate(f, 1):\n",
    "            try:\n",
    "                event = json.loads(line)\n",
    "                xml_str = event['event']['original']\n",
    "                \n",
    "                event_id, computer, fields = parse_sysmon_event(xml_str)\n",
    "                \n",
    "                # Skip if essential fields missing with detailed logging\n",
    "                if not event_id or not computer:\n",
    "                    with open('parsing_errors.log', 'a') as log:\n",
    "                        log.write(f\"Line {line_number}: Failed - EventID={event_id}, Computer={computer}\\n\")\n",
    "                        log.write(f\"XML sample: {xml_str[:200]}...\\n\\n\")\n",
    "                    error_count += 1\n",
    "                    continue\n",
    "                \n",
    "                # Count total events per EventID\n",
    "                eventid_counts[event_id] = eventid_counts.get(event_id, 0) + 1\n",
    "                \n",
    "                # Build record - REMOVED @timestamp field to avoid redundancy with UtcTime\n",
    "                record = {\n",
    "                    'EventID': event_id,\n",
    "                    'Computer': computer\n",
    "                }\n",
    "                \n",
    "                # Add fields from mapping with mismatch tracking and data type optimization\n",
    "                expected_fields = fields_per_eventid.get(event_id, [])\n",
    "                for field in expected_fields:\n",
    "                    # Special handling for EventID 8: map lowercase guid fields to uppercase GUID columns\n",
    "                    if event_id == 8:\n",
    "                        if field == 'SourceProcessGuid':\n",
    "                            # Store in uppercase GUID column for consistency with EventID 10\n",
    "                            column_name = 'SourceProcessGUID'\n",
    "                            if field not in fields:\n",
    "                                if event_id not in missing_fields_tracker:\n",
    "                                    missing_fields_tracker[event_id] = {}\n",
    "                                if field not in missing_fields_tracker[event_id]:\n",
    "                                    missing_fields_tracker[event_id][field] = 0\n",
    "                                missing_fields_tracker[event_id][field] += 1\n",
    "                            \n",
    "                            # Get value and apply GUID cleaning\n",
    "                            raw_value = fields.get(field, None)\n",
    "                            record[column_name] = clean_guid(raw_value)\n",
    "                            continue\n",
    "                        elif field == 'TargetProcessGuid':\n",
    "                            # Store in uppercase GUID column for consistency with EventID 10\n",
    "                            column_name = 'TargetProcessGUID'\n",
    "                            if field not in fields:\n",
    "                                if event_id not in missing_fields_tracker:\n",
    "                                    missing_fields_tracker[event_id] = {}\n",
    "                                if field not in missing_fields_tracker[event_id]:\n",
    "                                    missing_fields_tracker[event_id][field] = 0\n",
    "                                missing_fields_tracker[event_id][field] += 1\n",
    "                            \n",
    "                            # Get value and apply GUID cleaning\n",
    "                            raw_value = fields.get(field, None)\n",
    "                            record[column_name] = clean_guid(raw_value)\n",
    "                            continue\n",
    "                    \n",
    "                    # Normal field processing for all other cases\n",
    "                    if field not in fields:  # Field expected but not found in XML\n",
    "                        if event_id not in missing_fields_tracker:\n",
    "                            missing_fields_tracker[event_id] = {}\n",
    "                        if field not in missing_fields_tracker[event_id]:\n",
    "                            missing_fields_tracker[event_id][field] = 0\n",
    "                        missing_fields_tracker[event_id][field] += 1\n",
    "                    \n",
    "                    # Get the raw value\n",
    "                    raw_value = fields.get(field, None)\n",
    "                    \n",
    "                    # Apply data type conversions based on field name\n",
    "                    if field in integer_columns:\n",
    "                        # Convert to integer using module-level function\n",
    "                        record[field] = safe_int_conversion(raw_value)\n",
    "                    elif field in guid_columns:\n",
    "                        # Clean GUID using module-level function\n",
    "                        record[field] = clean_guid(raw_value)\n",
    "                    else:\n",
    "                        # Keep as-is for other fields but strip whitespace if it's a string\n",
    "                        if raw_value is not None:\n",
    "                            cleaned_value = str(raw_value).strip()\n",
    "                            record[field] = cleaned_value if cleaned_value else None\n",
    "                        else:\n",
    "                            record[field] = raw_value\n",
    "                \n",
    "                records.append(record)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing line {line_number}: {str(e)}\")\n",
    "                error_count += 1\n",
    "    \n",
    "    print(f\"\\nProcessing complete. Errors: {error_count}, Success: {len(records)}\")\n",
    "    \n",
    "    # Print detailed missing fields summary with statistics\n",
    "    if missing_fields_tracker:\n",
    "        print(\"\\nDetailed missing fields analysis:\")\n",
    "        print(\"-\" * 60)\n",
    "        for event_id, missing_fields in missing_fields_tracker.items():\n",
    "            total_events = eventid_counts[event_id]\n",
    "            print(f\"EventID {event_id}: {total_events} total events\")\n",
    "            for field, missing_count in missing_fields.items():\n",
    "                percentage = (missing_count / total_events) * 100\n",
    "                print(f\"  ‚Ä¢ Field '{field}': {missing_count}/{total_events} missing ({percentage:.1f}%)\")\n",
    "            print()\n",
    "    else:\n",
    "        print(\"\\nNo missing fields detected - all schema mappings match XML structure.\")\n",
    "    \n",
    "    # Print EventID distribution summary\n",
    "    print(\"EventID distribution:\")\n",
    "    print(\"-\" * 40)\n",
    "    for event_id in sorted(eventid_counts.keys()):\n",
    "        print(f\"EventID {event_id}: {eventid_counts[event_id]:,} events\")\n",
    "    \n",
    "    return pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÅ File Configuration\n",
    "\n",
    "Set input and output file paths for processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"-ds-logs-windows-sysmon_operational-default-2025-05-04-000001.jsonl\"\n",
    "output_file = \"sysmon-2025-05-04-000001.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing complete. Errors: 0, Success: 570078\n",
      "\n",
      "Detailed missing fields analysis:\n",
      "------------------------------------------------------------\n",
      "EventID 6: 481 total events\n",
      "  ‚Ä¢ Field 'User': 481/481 missing (100.0%)\n",
      "\n",
      "EventID distribution:\n",
      "----------------------------------------\n",
      "EventID 1: 1,461 events\n",
      "EventID 2: 57 events\n",
      "EventID 3: 16,918 events\n",
      "EventID 4: 6 events\n",
      "EventID 5: 965 events\n",
      "EventID 6: 481 events\n",
      "EventID 7: 63,892 events\n",
      "EventID 9: 1,158 events\n",
      "EventID 10: 57,814 events\n",
      "EventID 11: 4,271 events\n",
      "EventID 12: 289,812 events\n",
      "EventID 13: 129,227 events\n",
      "EventID 15: 46 events\n",
      "EventID 17: 488 events\n",
      "EventID 18: 1,491 events\n",
      "EventID 23: 1,979 events\n",
      "EventID 24: 5 events\n",
      "EventID 25: 7 events\n"
     ]
    }
   ],
   "source": [
    "df = process_events(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßΩ Data Cleaning Function\n",
    "\n",
    "Clean DataFrame by removing whitespace and normalizing empty values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataframe(df):\n",
    "    \"\"\"Clean whitespace and optimize data types\"\"\"\n",
    "    # Trim whitespace for string columns\n",
    "    str_cols = df.select_dtypes(['object']).columns\n",
    "    df[str_cols] = df[str_cols].apply(lambda x: x.str.strip())\n",
    "    \n",
    "    # Replace empty strings with None\n",
    "    df.replace({'': None}, inplace=True)\n",
    "    \n",
    "    # Optimize data types for better performance and storage\n",
    "    \n",
    "    # Integer columns - convert to nullable integer type\n",
    "    integer_columns = ['ProcessId', 'SourcePort', 'DestinationPort', 'SourceProcessId', \n",
    "                      'ParentProcessId', 'SourceThreadId', 'TargetProcessId']\n",
    "    \n",
    "    for col in integer_columns:\n",
    "        if col in df.columns:\n",
    "            # Convert to nullable integer type (Int64 handles NaN properly)\n",
    "            df[col] = df[col].astype('Int64')\n",
    "            print(f\"Converted {col} to Int64 type\")\n",
    "    \n",
    "    # GUID columns - ensure they're string type (not object)\n",
    "    guid_columns = ['ProcessGuid', 'SourceProcessGUID', 'TargetProcessGUID', 'ParentProcessGuid']\n",
    "    \n",
    "    for col in guid_columns:\n",
    "        if col in df.columns:\n",
    "            # Convert to string type (handles NaN as <NA>)\n",
    "            df[col] = df[col].astype('string')\n",
    "            print(f\"Converted {col} to string type\")\n",
    "    \n",
    "    # Convert Computer and other categorical-like columns to category type for memory efficiency\n",
    "    categorical_columns = ['Computer', 'Protocol', 'EventType']\n",
    "    \n",
    "    for col in categorical_columns:\n",
    "        if col in df.columns and df[col].nunique() < df.shape[0] * 0.5:  # Only if less than 50% unique values\n",
    "            df[col] = df[col].astype('category')\n",
    "            print(f\"Converted {col} to category type\")\n",
    "    \n",
    "    print(f\"\\nData type optimization complete!\")\n",
    "    print(f\"Memory usage after optimization: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßº Apply Data Cleaning\n",
    "\n",
    "Clean the processed DataFrame to prepare for CSV export:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted ProcessId to Int64 type\n",
      "Converted SourcePort to Int64 type\n",
      "Converted DestinationPort to Int64 type\n",
      "Converted SourceProcessId to Int64 type\n",
      "Converted ParentProcessId to Int64 type\n",
      "Converted SourceThreadId to Int64 type\n",
      "Converted TargetProcessId to Int64 type\n",
      "Converted ProcessGuid to string type\n",
      "Converted SourceProcessGUID to string type\n",
      "Converted TargetProcessGUID to string type\n",
      "Converted ParentProcessGuid to string type\n",
      "Converted Computer to category type\n",
      "Converted Protocol to category type\n",
      "Converted EventType to category type\n",
      "\n",
      "Data type optimization complete!\n",
      "Memory usage after optimization: 847.15 MB\n"
     ]
    }
   ],
   "source": [
    "df = clean_dataframe(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Export to CSV\n",
    "\n",
    "Save the cleaned DataFrame as CSV file for machine learning pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Optional Data Exploration\n",
    "\n",
    "The following commented cells provide various data exploration and validation options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 570078 entries, 0 to 570077\n",
      "Data columns (total 44 columns):\n",
      " #   Column                   Non-Null Count   Dtype   \n",
      "---  ------                   --------------   -----   \n",
      " 0   EventID                  570078 non-null  int64   \n",
      " 1   Computer                 570078 non-null  category\n",
      " 2   UtcTime                  570072 non-null  object  \n",
      " 3   ProcessGuid              511777 non-null  string  \n",
      " 4   ProcessId                511777 non-null  Int64   \n",
      " 5   Image                    511777 non-null  object  \n",
      " 6   User                     511770 non-null  object  \n",
      " 7   Protocol                 16918 non-null   category\n",
      " 8   SourceIsIpv6             16918 non-null   object  \n",
      " 9   SourceIp                 16918 non-null   object  \n",
      " 10  SourceHostname           16918 non-null   object  \n",
      " 11  SourcePort               16918 non-null   Int64   \n",
      " 12  SourcePortName           16918 non-null   object  \n",
      " 13  DestinationIsIpv6        16918 non-null   object  \n",
      " 14  DestinationIp            16918 non-null   object  \n",
      " 15  DestinationHostname      16918 non-null   object  \n",
      " 16  DestinationPort          16918 non-null   Int64   \n",
      " 17  DestinationPortName      16918 non-null   object  \n",
      " 18  ImageLoaded              64373 non-null   object  \n",
      " 19  OriginalFileName         63892 non-null   object  \n",
      " 20  Hashes                   67818 non-null   object  \n",
      " 21  EventType                421018 non-null  category\n",
      " 22  TargetObject             419039 non-null  object  \n",
      " 23  SourceProcessGUID        57814 non-null   string  \n",
      " 24  SourceProcessId          57814 non-null   Int64   \n",
      " 25  SourceImage              57814 non-null   object  \n",
      " 26  TargetProcessGUID        57814 non-null   string  \n",
      " 27  TargetProcessId          57814 non-null   Int64   \n",
      " 28  TargetImage              57814 non-null   object  \n",
      " 29  SourceThreadId           57814 non-null   Int64   \n",
      " 30  SourceUser               57814 non-null   object  \n",
      " 31  TargetUser               57814 non-null   object  \n",
      " 32  PipeName                 1979 non-null    object  \n",
      " 33  CommandLine              1461 non-null    object  \n",
      " 34  CurrentDirectory         1461 non-null    object  \n",
      " 35  ParentProcessGuid        1461 non-null    string  \n",
      " 36  ParentProcessId          1461 non-null    Int64   \n",
      " 37  ParentImage              1461 non-null    object  \n",
      " 38  ParentCommandLine        1461 non-null    object  \n",
      " 39  TargetFilename           6353 non-null    object  \n",
      " 40  CreationUtcTime          4374 non-null    object  \n",
      " 41  Device                   1158 non-null    object  \n",
      " 42  PreviousCreationUtcTime  57 non-null      object  \n",
      " 43  Hash                     46 non-null      object  \n",
      "dtypes: Int64(7), category(3), int64(1), object(29), string(4)\n",
      "memory usage: 183.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df = pd.read_csv(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check rows where the Utctime is null\n",
    "# new_df[new_df['UtcTime'].isnull()].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df[new_df['UtcTime'].isnull()]['EventID'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df['EventID'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df[new_df['EventID'] == 1].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df[new_df['EventID'] == 3].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df[new_df['EventID'] == 5].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df[new_df['EventID'] == 6].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df[new_df['EventID'] == 6]['ImageLoaded'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df[new_df['EventID'] == 7].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df[new_df['EventID'] == 8].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df[new_df['EventID'] == 9].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df[new_df['EventID'] == 10].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df[new_df['EventID'] == 10]['TargetProcessGUID'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df[new_df['EventID'] == 11].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df[new_df['EventID'] == 12].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df[new_df['EventID'] == 13].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df[new_df['EventID'] == 14].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df[new_df['EventID'] == 15].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df[new_df['EventID'] == 16].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df[new_df['EventID'] == 17].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df[new_df['EventID'] == 18].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df[new_df['EventID'] == 22].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df[new_df['EventID'] == 23].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df[new_df['EventID'] == 24].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df[new_df['EventID'] == 25].head(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataset-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
