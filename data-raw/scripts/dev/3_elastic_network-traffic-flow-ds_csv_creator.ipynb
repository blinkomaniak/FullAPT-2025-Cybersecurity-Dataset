{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Traffic Flow JSONL to CSV Converter\n",
    "\n",
    "This notebook converts Elasticsearch network traffic flow data from JSONL format to CSV format for machine learning analysis. The conversion extracts specific fields from nested JSON structures and creates a structured dataset suitable for cybersecurity research.\n",
    "\n",
    "**Input**: `-ds-logs-network_traffic-flow-default-2025-05-04-000001.jsonl` (Packetbeat network flow logs)  \n",
    "**Output**: `network_traffic_flow-2025-05-04-000001.csv` (Structured CSV dataset)\n",
    "\n",
    "**Key Features**:\n",
    "- Handles nested JSON structures with safe value extraction\n",
    "- Manages both scalar and array fields appropriately\n",
    "- Preserves all network flow metadata for analysis\n",
    "- Based on field analysis from notebooks 3a and 3b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Import essential libraries for JSON processing, data manipulation, and CSV generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for data processing\n",
    "import json       # For parsing JSONL records\n",
    "import pandas as pd  # For DataFrame creation and CSV export  \n",
    "import numpy as np   # For handling missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Helper Function for Safe Value Extraction\n",
    "\n",
    "Define a utility function to safely navigate nested JSON structures and extract field values without errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nested_value(doc, path):\n",
    "    \"\"\"\n",
    "    Safely retrieve nested values from document structure using dot notation.\n",
    "    \n",
    "    Args:\n",
    "        doc (dict): The source JSON document\n",
    "        path (str): Dot-separated path to the desired field (e.g., 'host.os.platform')\n",
    "        \n",
    "    Returns:\n",
    "        The value at the specified path, or None if path doesn't exist\n",
    "        \n",
    "    Examples:\n",
    "        get_nested_value({'host': {'os': {'platform': 'windows'}}}, 'host.os.platform') → 'windows'\n",
    "        get_nested_value({'event': {'type': ['connection']}}, 'event.type[0]') → 'connection'\n",
    "    \"\"\"\n",
    "    keys = path.split('.')\n",
    "    current = doc\n",
    "    \n",
    "    for key in keys:\n",
    "        if isinstance(current, dict):\n",
    "            # Navigate through dictionary structure\n",
    "            current = current.get(key)\n",
    "        elif isinstance(current, list) and key.isdigit():\n",
    "            # Handle array indexing (e.g., 'event.type[0]' → key='type[0]')\n",
    "            try:\n",
    "                current = current[int(key)] if int(key) < len(current) else None\n",
    "            except (ValueError, IndexError):\n",
    "                return None\n",
    "        else:\n",
    "            # Path doesn't exist in current structure\n",
    "            return None\n",
    "            \n",
    "        # Stop if we hit a dead end\n",
    "        if current is None:\n",
    "            return None\n",
    "            \n",
    "    return current"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Field Mapping Configuration\n",
    "\n",
    "Define the mapping between JSON field paths and CSV column names. This mapping is based on analysis from notebooks 3a and 3b, which identified the most relevant fields for network flow analysis.\n",
    "\n",
    "**Field Categories**:\n",
    "- **Temporal**: Timestamp information\n",
    "- **Network**: Source/destination IPs, ports, bytes, packets\n",
    "- **Process**: Process information (present in ~64% of records)\n",
    "- **Host**: Host identification and OS information\n",
    "- **Flow**: Network flow metadata and identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Total fields to extract: 34\n",
      "🔍 Field categories: timestamp, destination, event, host, network, traffic, process, source\n"
     ]
    }
   ],
   "source": [
    "# Define field mapping: (JSON_path, CSV_column_name)\n",
    "# Based on field analysis from notebooks 3a (exploratory) and 3b (structure consistency)\n",
    "fields = [\n",
    "    # === TEMPORAL FIELDS ===\n",
    "    ('@timestamp', 'timestamp'),                    # Always present (100%)\n",
    "    \n",
    "    # === DESTINATION FIELDS ===\n",
    "    ('destination.bytes', 'destination_bytes'),     # Always present (100%)\n",
    "    ('destination.ip', 'destination_ip'),           # Always present (100%)\n",
    "    ('destination.mac', 'destination_mac'),         # Always present (100%)\n",
    "    ('destination.packets', 'destination_packets'), # Always present (100%)\n",
    "    ('destination.port', 'destination_port'),       # Always present (100%)\n",
    "    ('destination.process', 'destination_process'), # Rare field (~2.8% presence) - kept for analysis\n",
    "    \n",
    "    # === EVENT FIELDS ===\n",
    "    ('event.action', 'event_action'),               # Always present (100%)\n",
    "    ('event.duration', 'event_duration'),           # Always present (100%)\n",
    "    ('event.type[0]', 'event_type'),               # Always present (100%) - extract first element\n",
    "    \n",
    "    # === HOST FIELDS ===\n",
    "    ('host.hostname', 'host_hostname'),             # Always present (100%)\n",
    "    ('host.ip', 'host_ip'),                        # Always present (100%) - keep all IPs as list\n",
    "    ('host.mac[0]', 'host_mac'),                   # Always present (100%) - extract first MAC\n",
    "    ('host.os.platform', 'host_os_platform'),      # Always present (100%)\n",
    "    \n",
    "    # === NETWORK FIELDS ===\n",
    "    ('network.bytes', 'network_bytes'),             # Always present (100%)\n",
    "    ('network.packets', 'network_packets'),         # Always present (100%)\n",
    "    ('network.transport', 'network_transport'),     # Always present (100%) - tcp/udp\n",
    "    ('network.type', 'network_type'),               # Always present (100%) - ipv4/ipv6\n",
    "    \n",
    "    # === NETWORK TRAFFIC FIELDS ===\n",
    "    ('network_traffic.flow.id', 'network_traffic_flow_id'), # Always present (100%)\n",
    "    \n",
    "    # === PROCESS FIELDS ===\n",
    "    ('process.args', 'process_args'),               # Conditional (~64% presence) - keep as list\n",
    "    ('process.executable', 'process_executable'),   # Conditional (~64% presence)\n",
    "    ('process.name', 'process_name'),               # Conditional (~64% presence)\n",
    "    ('process.parent.pid', 'process_parent_pid'),   # Conditional (~64% presence)\n",
    "    ('process.pid', 'process_pid'),                 # Conditional (~64% presence)\n",
    "    \n",
    "    # === SOURCE FIELDS ===\n",
    "    ('source.bytes', 'source_bytes'),               # Always present (100%)\n",
    "    ('source.ip', 'source_ip'),                     # Always present (100%)\n",
    "    ('source.mac', 'source_mac'),                   # Always present (100%)\n",
    "    ('source.packets', 'source_packets'),           # Always present (100%) - fixed typo from 'packet'\n",
    "    ('source.port', 'source_port'),                 # Always present (100%)\n",
    "    \n",
    "    # === SOURCE PROCESS FIELDS ===\n",
    "    ('source.process.args', 'source_process_args'), # Conditional (~61% presence) - keep as list\n",
    "    ('source.process.executable', 'source_process_executable'), # Conditional (~61% presence)\n",
    "    ('source.process.name', 'source_process_name'), # Conditional (~61% presence)\n",
    "    ('source.process.pid', 'source_process_pid'),   # Conditional (~61% presence)\n",
    "    ('source.process.ppid', 'source_process_ppid')  # Conditional (~61% presence)\n",
    "]\n",
    "\n",
    "print(f\"📊 Total fields to extract: {len(fields)}\")\n",
    "print(f\"🔍 Field categories: timestamp, destination, event, host, network, traffic, process, source\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. File Configuration\n",
    "\n",
    "Set input and output file paths for the conversion process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Input file: -ds-logs-network_traffic-flow-default-2025-05-04-000001.jsonl\n",
      "📤 Output file: network_traffic_flow-2025-05-04-000001.csv\n",
      "✅ Input file found - Size: 2309.2 MB\n"
     ]
    }
   ],
   "source": [
    "# Input and output file configuration\n",
    "traffic_flow_filename = '-ds-logs-network_traffic-flow-default-2025-05-04-000001.jsonl'  # Source JSONL file\n",
    "output_filename = 'network_traffic_flow-2025-05-04-000001.csv'                            # Target CSV file\n",
    "\n",
    "print(f\"📥 Input file: {traffic_flow_filename}\")\n",
    "print(f\"📤 Output file: {output_filename}\")\n",
    "\n",
    "# Verify input file exists\n",
    "import os\n",
    "if os.path.exists(traffic_flow_filename):\n",
    "    file_size_mb = os.path.getsize(traffic_flow_filename) / (1024 * 1024)\n",
    "    print(f\"✅ Input file found - Size: {file_size_mb:.1f} MB\")\n",
    "else:\n",
    "    print(f\"❌ Input file not found: {traffic_flow_filename}\")\n",
    "    print(f\"📁 Current directory files: {[f for f in os.listdir('.') if f.endswith('.jsonl')]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Processing and Conversion\n",
    "\n",
    "Process the JSONL file line by line, extract specified fields, and convert to structured records.\n",
    "\n",
    "**Processing Logic**:\n",
    "- **Array Fields**: Keep as Python lists (`host.ip`, `process.args`, `source.process.args`, `destination.process`)\n",
    "- **Scalar Fields**: Extract single values, use NaN for missing values\n",
    "- **Error Handling**: Skip malformed JSON lines and continue processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Starting JSONL to CSV conversion...\n",
      "📊 Processing 34 fields per record\n",
      "   Processed 100,000 lines...\n",
      "   Processed 200,000 lines...\n",
      "   Processed 300,000 lines...\n",
      "   Processed 400,000 lines...\n",
      "   Processed 500,000 lines...\n",
      "   Processed 600,000 lines...\n",
      "   Processed 700,000 lines...\n",
      "   Processed 800,000 lines...\n",
      "   Processed 900,000 lines...\n",
      "   Processed 1,000,000 lines...\n",
      "✅ Processing complete!\n",
      "📈 Total lines processed: 1,090,212\n",
      "📊 Valid records extracted: 1,090,212\n",
      "⚠️  JSON errors skipped: 0\n",
      "📋 Success rate: 100.00%\n",
      "🔧 Port fields converted to integers: ['destination.port', 'source.port']\n"
     ]
    }
   ],
   "source": [
    "# Process documents and extract fields\n",
    "print(\"🔄 Starting JSONL to CSV conversion...\")\n",
    "print(f\"📊 Processing {len(fields)} fields per record\")\n",
    "\n",
    "records = []           # List to store processed records\n",
    "line_count = 0         # Track total lines processed\n",
    "error_count = 0        # Track JSON parsing errors\n",
    "\n",
    "# Define which fields should be treated as arrays (kept as lists)\n",
    "array_fields = ['process.args', 'source.process.args', 'host.ip', 'destination.process']\n",
    "\n",
    "# Define port fields that need integer conversion (fix Elasticsearch float issue)\n",
    "port_fields = ['destination.port', 'source.port']\n",
    "\n",
    "with open(traffic_flow_filename, 'r') as f:\n",
    "    for line_number, line in enumerate(f, 1):\n",
    "        line_count += 1\n",
    "        \n",
    "        # Progress indicator for large files\n",
    "        if line_count % 100000 == 0:\n",
    "            print(f\"   Processed {line_count:,} lines...\")\n",
    "        \n",
    "        try:\n",
    "            # Parse JSON line\n",
    "            doc = json.loads(line)\n",
    "            record = {}\n",
    "            \n",
    "            # Extract each mapped field\n",
    "            for path, column in fields:\n",
    "                # Get value using safe nested extraction\n",
    "                value = get_nested_value(doc, path)\n",
    "                \n",
    "                # Handle array fields consistently - keep as lists\n",
    "                if path in array_fields:\n",
    "                    if isinstance(value, list):\n",
    "                        record[column] = value              # Keep existing list\n",
    "                    else:\n",
    "                        record[column] = [value] if value is not None else []  # Wrap single value or empty list\n",
    "                \n",
    "                # Handle port fields - convert floats to integers (fix Elasticsearch issue)\n",
    "                elif path in port_fields and value is not None:\n",
    "                    try:\n",
    "                        # Convert float ports to integers (e.g., 443.0 → 443)\n",
    "                        record[column] = int(float(value))  # Handle both int and float inputs\n",
    "                    except (ValueError, TypeError):\n",
    "                        # If conversion fails, use NaN\n",
    "                        record[column] = np.nan\n",
    "                        \n",
    "                else:\n",
    "                    # Handle other scalar fields - use NaN for missing values\n",
    "                    record[column] = value if value is not None else np.nan\n",
    "                    \n",
    "            records.append(record)\n",
    "            \n",
    "        except json.JSONDecodeError:\n",
    "            # Skip malformed JSON lines\n",
    "            error_count += 1\n",
    "            continue\n",
    "\n",
    "print(f\"✅ Processing complete!\")\n",
    "print(f\"📈 Total lines processed: {line_count:,}\")\n",
    "print(f\"📊 Valid records extracted: {len(records):,}\")\n",
    "print(f\"⚠️  JSON errors skipped: {error_count:,}\")\n",
    "print(f\"📋 Success rate: {(len(records)/line_count)*100:.2f}%\")\n",
    "print(f\"🔧 Port fields converted to integers: {port_fields}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. DataFrame Creation and CSV Export\n",
    "\n",
    "Convert the processed records to a pandas DataFrame and export to CSV format for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Creating DataFrame and exporting to CSV...\n",
      "📊 DataFrame created successfully!\n",
      "📏 Shape: 1,090,212 rows × 34 columns\n",
      "💾 Memory usage: ~1659.6 MB\n",
      "\n",
      "📋 DATASET SUMMARY:\n",
      "   • Total records: 1,090,212\n",
      "   • Total columns: 34\n",
      "   • Column names: ['timestamp', 'destination_bytes', 'destination_ip', 'destination_mac', 'destination_packets']... (showing first 5)\n",
      "\n",
      "✅ CSV export successful!\n",
      "📤 Output file: network_traffic_flow-2025-05-04-000001.csv\n",
      "📁 File size: 785.8 MB\n",
      "🎯 Ready for machine learning analysis!\n",
      "\n",
      "📄 Sample of first 3 rows:\n",
      "                  timestamp  destination_bytes destination_ip    destination_mac  destination_packets  destination_port destination_process  event_action  event_duration  event_type host_hostname     host_ip  host_mac host_os_platform  network_bytes  network_packets network_transport network_type                                  network_traffic_flow_id process_args process_executable process_name  process_parent_pid  process_pid  source_bytes source_ip         source_mac  source_packets  source_...\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame and export to CSV\n",
    "print(\"🔄 Creating DataFrame and exporting to CSV...\")\n",
    "\n",
    "# Convert records list to pandas DataFrame\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "print(f\"📊 DataFrame created successfully!\")\n",
    "print(f\"📏 Shape: {df.shape[0]:,} rows × {df.shape[1]} columns\")\n",
    "print(f\"💾 Memory usage: ~{df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "\n",
    "# Display basic info about the dataset\n",
    "print(f\"\\n📋 DATASET SUMMARY:\")\n",
    "print(f\"   • Total records: {len(df):,}\")\n",
    "print(f\"   • Total columns: {len(df.columns)}\")\n",
    "print(f\"   • Column names: {list(df.columns)[:5]}... (showing first 5)\")\n",
    "\n",
    "# Save to CSV with appropriate handling for list columns\n",
    "df.to_csv(output_filename, index=False)\n",
    "\n",
    "print(f\"\\n✅ CSV export successful!\")\n",
    "print(f\"📤 Output file: {output_filename}\")\n",
    "\n",
    "# Verify output file was created\n",
    "if os.path.exists(output_filename):\n",
    "    output_size_mb = os.path.getsize(output_filename) / (1024 * 1024)\n",
    "    print(f\"📁 File size: {output_size_mb:.1f} MB\")\n",
    "    print(f\"🎯 Ready for machine learning analysis!\")\n",
    "else:\n",
    "    print(f\"❌ Error: Output file was not created\")\n",
    "\n",
    "# Display sample of first few rows (limit display for readability)\n",
    "print(f\"\\n📄 Sample of first 3 rows:\")\n",
    "print(df.head(3).to_string()[:500] + \"...\" if len(str(df.head(3))) > 500 else df.head(3).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔬 EXPLORATORY DATA ANALYSIS\n",
    "\n",
    "Now that we have converted the JSONL data to a structured DataFrame, let's perform comprehensive exploratory analysis to understand the network traffic patterns, security characteristics, and data quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Dataset Overview & Statistics\n",
    "\n",
    "Get comprehensive understanding of the dataset structure, data types, and basic statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "📊 COMPREHENSIVE DATASET OVERVIEW\n",
      "================================================================================\n",
      "\n",
      "🔢 BASIC STATISTICS:\n",
      "   • Total Records: 1,090,212\n",
      "   • Total Columns: 34\n",
      "   • Memory Usage: 1659.6 MB\n",
      "   • Index Range: 0 to 1090211\n",
      "\n",
      "📋 DATA TYPES DISTRIBUTION:\n",
      "   • object         : 19 columns\n",
      "   • float64        : 10 columns\n",
      "   • int64          :  5 columns\n",
      "\n",
      "🔍 COLUMN CATEGORIES:\n",
      "   • Array/List columns: 4 → ['destination_process', 'host_ip', 'process_args']...\n",
      "   • Numeric columns: 15 → ['destination_bytes', 'destination_packets', 'destination_port']...\n",
      "   • String columns: 15 → ['timestamp', 'destination_ip', 'destination_mac']...\n",
      "\n",
      "📊 MISSING DATA ANALYSIS:\n",
      "   Fields with missing values:\n",
      "   • event_type                    : 1,090,212 (100.0%)\n",
      "   • host_mac                      : 1,090,212 (100.0%)\n",
      "   • source_process_executable     : 441,517 (40.5%)\n",
      "   • source_process_name           : 441,517 (40.5%)\n",
      "   • source_process_pid            : 441,517 (40.5%)\n",
      "   • source_process_ppid           : 441,517 (40.5%)\n",
      "   • process_pid                   : 408,472 (37.5%)\n",
      "   • process_parent_pid            : 408,472 (37.5%)\n",
      "   • process_executable            : 408,472 (37.5%)\n",
      "   • process_name                  : 408,472 (37.5%)\n",
      "\n",
      "🎯 SAMPLE DATA PREVIEW:\n",
      "   First 3 records (key fields only):\n",
      "               timestamp source_ip destination_ip network_transport  destination_port\n",
      "2025-05-04T11:30:08.613Z       NaN            NaN               NaN               NaN\n",
      "2025-05-04T11:30:08.613Z  10.1.0.4    224.0.0.252               udp            5355.0\n",
      "2025-05-04T11:30:08.613Z  10.1.0.4    224.0.0.252               udp            5355.0\n"
     ]
    }
   ],
   "source": [
    "# Dataset Overview\n",
    "print(\"=\" * 80)\n",
    "print(\"📊 COMPREHENSIVE DATASET OVERVIEW\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n🔢 BASIC STATISTICS:\")\n",
    "print(f\"   • Total Records: {len(df):,}\")\n",
    "print(f\"   • Total Columns: {len(df.columns)}\")\n",
    "print(f\"   • Memory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "print(f\"   • Index Range: {df.index.min()} to {df.index.max()}\")\n",
    "\n",
    "print(f\"\\n📋 DATA TYPES DISTRIBUTION:\")\n",
    "dtype_counts = df.dtypes.value_counts()\n",
    "for dtype, count in dtype_counts.items():\n",
    "    print(f\"   • {str(dtype):15s}: {count:2d} columns\")\n",
    "\n",
    "print(f\"\\n🔍 COLUMN CATEGORIES:\")\n",
    "array_columns = [col for col in df.columns if df[col].apply(lambda x: isinstance(x, list)).any()]\n",
    "numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "string_columns = df.select_dtypes(include=['object']).columns.tolist()\n",
    "string_columns = [col for col in string_columns if col not in array_columns]\n",
    "\n",
    "print(f\"   • Array/List columns: {len(array_columns)} → {array_columns[:3]}{'...' if len(array_columns) > 3 else ''}\")\n",
    "print(f\"   • Numeric columns: {len(numeric_columns)} → {numeric_columns[:3]}{'...' if len(numeric_columns) > 3 else ''}\")\n",
    "print(f\"   • String columns: {len(string_columns)} → {string_columns[:3]}{'...' if len(string_columns) > 3 else ''}\")\n",
    "\n",
    "print(f\"\\n📊 MISSING DATA ANALYSIS:\")\n",
    "missing_counts = df.isnull().sum()\n",
    "missing_data = missing_counts[missing_counts > 0].sort_values(ascending=False)\n",
    "if len(missing_data) > 0:\n",
    "    print(\"   Fields with missing values:\")\n",
    "    for field, count in missing_data.head(10).items():\n",
    "        percentage = (count / len(df)) * 100\n",
    "        print(f\"   • {field:30s}: {count:,} ({percentage:.1f}%)\")\n",
    "else:\n",
    "    print(\"   🎉 No missing values detected!\")\n",
    "\n",
    "print(f\"\\n🎯 SAMPLE DATA PREVIEW:\")\n",
    "print(\"   First 3 records (key fields only):\")\n",
    "preview_cols = ['timestamp', 'source_ip', 'destination_ip', 'network_transport', 'destination_port']\n",
    "print(df[preview_cols].head(3).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Network Protocol & Transport Analysis\n",
    "\n",
    "Analyze network protocol distributions, transport layer characteristics, and communication patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "🌐 NETWORK PROTOCOL & TRANSPORT ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "🚦 TRANSPORT PROTOCOL DISTRIBUTION:\n",
      "   • TCP     : 1,012,577 (92.9%)\n",
      "   • UDP     : 68,666 (6.3%)\n",
      "   • IPV6-ICMP: 1,709 (0.2%)\n",
      "   • ICMP    : 217 (0.0%)\n",
      "\n",
      "📡 NETWORK TYPE DISTRIBUTION:\n",
      "   • IPV4    : 1,076,481 (98.7%)\n",
      "   • IPV6    : 7,076 (0.6%)\n",
      "\n",
      "🎯 PORT ANALYSIS - MOST COMMON DESTINATION PORTS:\n",
      "   • Port 9200.000000 (Elasticsearch): 560,025 (51.37%)\n",
      "   • Port 34736.000000 (Unknown     ): 264,224 (24.24%)\n",
      "   • Port 389.000000 (LDAP        ): 68,684 (6.30%)\n",
      "   • Port 53.000000 (DNS         ): 47,354 (4.34%)\n",
      "   • Port 3268.000000 (Unknown     ): 23,666 (2.17%)\n",
      "   • Port 443.000000 (HTTPS       ): 17,175 (1.58%)\n",
      "   • Port 88.000000 (Unknown     ): 16,904 (1.55%)\n",
      "   • Port 1433.000000 (SQL Server  ): 7,029 (0.64%)\n",
      "   • Port 8220.000000 (Unknown     ): 6,780 (0.62%)\n",
      "   • Port 80.000000 (HTTP        ): 6,186 (0.57%)\n",
      "   • Port 445.000000 (SMB         ): 4,704 (0.43%)\n",
      "   • Port 135.000000 (RPC         ): 4,396 (0.40%)\n",
      "   • Port 49667.000000 (Unknown     ): 4,103 (0.38%)\n",
      "   • Port 46396.000000 (Unknown     ): 3,890 (0.36%)\n",
      "   • Port 8888.000000 (Unknown     ): 3,611 (0.33%)\n",
      "\n",
      "📊 SOURCE PORT CHARACTERISTICS:\n",
      "   • Min source port: 53\n",
      "   • Max source port: 65534\n",
      "   • Avg source port: 31206\n",
      "   • Unique source ports: 12,377\n",
      "\n",
      "🔌 SOURCE PORT CATEGORIES:\n",
      "   • Well-known ports (≤1023): 282,922 (26.0%)\n",
      "   • Registered ports (1024-49151): 306,513 (28.1%)\n",
      "   • Ephemeral ports (≥49152): 491,808 (45.1%)\n"
     ]
    }
   ],
   "source": [
    "# Network Protocol Analysis\n",
    "print(\"=\" * 80)\n",
    "print(\"🌐 NETWORK PROTOCOL & TRANSPORT ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n🚦 TRANSPORT PROTOCOL DISTRIBUTION:\")\n",
    "transport_dist = df['network_transport'].value_counts()\n",
    "for protocol, count in transport_dist.items():\n",
    "    percentage = (count / len(df)) * 100\n",
    "    print(f\"   • {protocol.upper():8s}: {count:,} ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\n📡 NETWORK TYPE DISTRIBUTION:\")\n",
    "network_type_dist = df['network_type'].value_counts()\n",
    "for net_type, count in network_type_dist.items():\n",
    "    percentage = (count / len(df)) * 100\n",
    "    print(f\"   • {net_type.upper():8s}: {count:,} ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\n🎯 PORT ANALYSIS - MOST COMMON DESTINATION PORTS:\")\n",
    "top_dest_ports = df['destination_port'].value_counts().head(15)\n",
    "for port, count in top_dest_ports.items():\n",
    "    percentage = (count / len(df)) * 100\n",
    "    # Common port mappings for context\n",
    "    port_services = {\n",
    "        53: 'DNS', 80: 'HTTP', 443: 'HTTPS', 445: 'SMB', 135: 'RPC', \n",
    "        139: 'NetBIOS', 389: 'LDAP', 636: 'LDAPS', 3389: 'RDP',\n",
    "        9200: 'Elasticsearch', 5355: 'LLMNR', 1433: 'SQL Server'\n",
    "    }\n",
    "    service = port_services.get(port, 'Unknown')\n",
    "    print(f\"   • Port {port:0f} ({service:12s}): {count:,} ({percentage:.2f}%)\")\n",
    "\n",
    "print(f\"\\n📊 SOURCE PORT CHARACTERISTICS:\")\n",
    "source_port_stats = df['source_port'].describe()\n",
    "print(f\"   • Min source port: {source_port_stats['min']:.0f}\")\n",
    "print(f\"   • Max source port: {source_port_stats['max']:.0f}\")\n",
    "print(f\"   • Avg source port: {source_port_stats['mean']:.0f}\")\n",
    "print(f\"   • Unique source ports: {df['source_port'].nunique():,}\")\n",
    "\n",
    "# Analyze ephemeral vs well-known ports\n",
    "ephemeral_sources = df[df['source_port'] >= 49152].shape[0]  # RFC 6335 ephemeral range\n",
    "wellknown_sources = df[df['source_port'] <= 1023].shape[0]\n",
    "registered_sources = df[(df['source_port'] > 1023) & (df['source_port'] < 49152)].shape[0]\n",
    "\n",
    "print(f\"\\n🔌 SOURCE PORT CATEGORIES:\")\n",
    "print(f\"   • Well-known ports (≤1023): {wellknown_sources:,} ({(wellknown_sources/len(df))*100:.1f}%)\")\n",
    "print(f\"   • Registered ports (1024-49151): {registered_sources:,} ({(registered_sources/len(df))*100:.1f}%)\")\n",
    "print(f\"   • Ephemeral ports (≥49152): {ephemeral_sources:,} ({(ephemeral_sources/len(df))*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Traffic Volume Analysis\n",
    "\n",
    "Examine data transfer patterns, bandwidth usage, and communication intensity across the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "📈 TRAFFIC VOLUME & BANDWIDTH ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "💾 BYTES TRANSFER STATISTICS:\n",
      "   Network Bytes (Total Flow):\n",
      "   • Min: 42 bytes\n",
      "   • Max: 24,262,608 bytes (23.1 MB)\n",
      "   • Mean: 228,364 bytes\n",
      "   • Median: 14,052 bytes\n",
      "   • Total network traffic: 248,964,662,840 bytes (231.87 GB)\n",
      "\n",
      "📤 SOURCE BYTES STATISTICS:\n",
      "   • Mean: 195,389 bytes | Max: 23,890,532 bytes\n",
      "   • Total uploaded: 213,015,725,309 bytes (198.39 GB)\n",
      "\n",
      "📥 DESTINATION BYTES STATISTICS:\n",
      "   • Mean: 33,775 bytes | Max: 14,069,480 bytes\n",
      "   • Total downloaded: 35,948,937,531 bytes (33.48 GB)\n",
      "\n",
      "📦 PACKET STATISTICS:\n",
      "   Network Packets (Total Flow):\n",
      "   • Min: 1 packets\n",
      "   • Max: 20,545 packets\n",
      "   • Mean: 191.0 packets per flow\n",
      "   • Total packets: 208,247,285\n",
      "\n",
      "🔢 FLOW EFFICIENCY METRICS:\n",
      "   • Average bytes per packet: 789.5 bytes\n",
      "   • Flows with >1000 bytes: 974,023 (89.3%)\n",
      "   • Large flows (>1MB): 67,591 (6.2%)\n",
      "\n",
      "🏆 TOP TRAFFIC GENERATORS (by total bytes):\n",
      "    1. 10.1.0.5       : 130.54 GB in 260,870 flows\n",
      "    2. 10.1.0.6       : 58.06 GB in 580,142 flows\n",
      "    3. 10.1.0.7       : 8.96 GB in 182,822 flows\n",
      "    4. 10.1.0.4       : 0.80 GB in 36,450 flows\n",
      "    5. 10.2.0.20      : 0.02 GB in 13,606 flows\n",
      "    6. 192.168.0.4    : 0.00 GB in 534 flows\n",
      "    7. fe80::753b:3801:530a:d5a4: 0.00 GB in 3,101 flows\n",
      "    8. fe80::ac69:cdb8:409a:9db8: 0.00 GB in 2,966 flows\n",
      "    9. fe80::1f2:37ae:1b78:885a: 0.00 GB in 607 flows\n",
      "   10. 52.182.143.215 : 0.00 GB in 55 flows\n",
      "\n",
      "🎯 TOP TRAFFIC RECEIVERS (by total bytes):\n",
      "    1. 192.168.0.4    : 21.04 GB in 273,358.0 flows\n",
      "    2. 10.2.0.20      : 7.77 GB in 565,112.0 flows\n",
      "    3. 10.1.0.4       : 3.72 GB in 160,887.0 flows\n",
      "    4. 10.1.0.6       : 0.45 GB in 5,602.0 flows\n",
      "    5. 10.1.0.7       : 0.21 GB in 10,312.0 flows\n",
      "    6. 34.149.100.209 : 0.06 GB in 151.0 flows\n",
      "    7. 10.1.0.5       : 0.05 GB in 8,694.0 flows\n",
      "    8. 34.107.152.202 : 0.02 GB in 129.0 flows\n",
      "    9. 23.208.31.183  : 0.02 GB in 391.0 flows\n",
      "   10. 52.137.106.217 : 0.01 GB in 258.0 flows\n"
     ]
    }
   ],
   "source": [
    "# Traffic Volume Analysis  \n",
    "print(\"=\" * 80)\n",
    "print(\"📈 TRAFFIC VOLUME & BANDWIDTH ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n💾 BYTES TRANSFER STATISTICS:\")\n",
    "# Network total bytes\n",
    "total_bytes_stats = df['network_bytes'].describe()\n",
    "print(f\"   Network Bytes (Total Flow):\")\n",
    "print(f\"   • Min: {total_bytes_stats['min']:,.0f} bytes\")\n",
    "print(f\"   • Max: {total_bytes_stats['max']:,.0f} bytes ({total_bytes_stats['max']/1024/1024:.1f} MB)\")\n",
    "print(f\"   • Mean: {total_bytes_stats['mean']:,.0f} bytes\")\n",
    "print(f\"   • Median: {total_bytes_stats['50%']:,.0f} bytes\")\n",
    "print(f\"   • Total network traffic: {df['network_bytes'].sum():,.0f} bytes ({df['network_bytes'].sum()/1024/1024/1024:.2f} GB)\")\n",
    "\n",
    "# Source vs Destination bytes\n",
    "src_bytes_stats = df['source_bytes'].describe()\n",
    "dst_bytes_stats = df['destination_bytes'].describe()\n",
    "print(f\"\\n📤 SOURCE BYTES STATISTICS:\")\n",
    "print(f\"   • Mean: {src_bytes_stats['mean']:,.0f} bytes | Max: {src_bytes_stats['max']:,.0f} bytes\")\n",
    "print(f\"   • Total uploaded: {df['source_bytes'].sum():,.0f} bytes ({df['source_bytes'].sum()/1024/1024/1024:.2f} GB)\")\n",
    "\n",
    "print(f\"\\n📥 DESTINATION BYTES STATISTICS:\")\n",
    "print(f\"   • Mean: {dst_bytes_stats['mean']:,.0f} bytes | Max: {dst_bytes_stats['max']:,.0f} bytes\")\n",
    "print(f\"   • Total downloaded: {df['destination_bytes'].sum():,.0f} bytes ({df['destination_bytes'].sum()/1024/1024/1024:.2f} GB)\")\n",
    "\n",
    "print(f\"\\n📦 PACKET STATISTICS:\")\n",
    "network_packets_stats = df['network_packets'].describe()\n",
    "print(f\"   Network Packets (Total Flow):\")\n",
    "print(f\"   • Min: {network_packets_stats['min']:,.0f} packets\")\n",
    "print(f\"   • Max: {network_packets_stats['max']:,.0f} packets\")\n",
    "print(f\"   • Mean: {network_packets_stats['mean']:,.1f} packets per flow\")\n",
    "print(f\"   • Total packets: {df['network_packets'].sum():,.0f}\")\n",
    "\n",
    "print(f\"\\n🔢 FLOW EFFICIENCY METRICS:\")\n",
    "# Calculate average bytes per packet\n",
    "df_temp = df[df['network_packets'] > 0].copy()  # Avoid division by zero\n",
    "df_temp['bytes_per_packet'] = df_temp['network_bytes'] / df_temp['network_packets']\n",
    "avg_bytes_per_packet = df_temp['bytes_per_packet'].mean()\n",
    "print(f\"   • Average bytes per packet: {avg_bytes_per_packet:.1f} bytes\")\n",
    "print(f\"   • Flows with >1000 bytes: {(df['network_bytes'] > 1000).sum():,} ({(df['network_bytes'] > 1000).sum()/len(df)*100:.1f}%)\")\n",
    "print(f\"   • Large flows (>1MB): {(df['network_bytes'] > 1024*1024).sum():,} ({(df['network_bytes'] > 1024*1024).sum()/len(df)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n🏆 TOP TRAFFIC GENERATORS (by total bytes):\")\n",
    "# Group by source IP and sum bytes\n",
    "top_sources = df.groupby('source_ip')['source_bytes'].agg(['sum', 'count']).sort_values('sum', ascending=False).head(10)\n",
    "for idx, (ip, stats) in enumerate(top_sources.iterrows(), 1):\n",
    "    total_gb = stats['sum'] / 1024 / 1024 / 1024\n",
    "    print(f\"   {idx:2d}. {ip:15s}: {total_gb:.2f} GB in {stats['count']:,} flows\")\n",
    "\n",
    "print(f\"\\n🎯 TOP TRAFFIC RECEIVERS (by total bytes):\")\n",
    "# Group by destination IP and sum bytes\n",
    "top_destinations = df.groupby('destination_ip')['destination_bytes'].agg(['sum', 'count']).sort_values('sum', ascending=False).head(10)\n",
    "for idx, (ip, stats) in enumerate(top_destinations.iterrows(), 1):\n",
    "    total_gb = stats['sum'] / 1024 / 1024 / 1024\n",
    "    print(f\"   {idx:2d}. {ip:15s}: {total_gb:.2f} GB in {stats['count']:,} flows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Host & Network Infrastructure Analysis\n",
    "\n",
    "Analyze host characteristics, network topology, operating systems, and infrastructure patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "🏠 HOST & NETWORK INFRASTRUCTURE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "💻 HOST INFORMATION:\n",
      "   • Unique hostnames: 4\n",
      "   • Unique source IPs: 38\n",
      "   • Unique destination IPs: 644\n",
      "   • Total unique IPs (src+dst): 648\n",
      "\n",
      "🖥️ OPERATING SYSTEM DISTRIBUTION:\n",
      "   • Windows     : 1,090,212 (100.0%)\n",
      "\n",
      "🌐 NETWORK TOPOLOGY ANALYSIS:\n",
      "   Source IP Categories:\n",
      "   • Internal    : 1,074,603 (98.6%)\n",
      "   • External    : 8,954 (0.8%)\n",
      "   • Unknown     : 6,655 (0.6%)\n",
      "   Destination IP Categories:\n",
      "   • Internal    : 1,035,366 (95.0%)\n",
      "   • External    : 44,015 (4.0%)\n",
      "   • Unknown     : 6,655 (0.6%)\n",
      "   • Multicast   : 4,176 (0.4%)\n",
      "\n",
      "🏆 MOST ACTIVE HOSTS:\n",
      "   1. theblock        (23.54.61.183) - 152.37 GB, 136956.6K packets [windows]\n",
      "   2. waterfalls      (23.208.31.151) - 62.49 GB, 52759.5K packets [windows]\n",
      "   3. endofroad       (10.1.0.7    ) - 15.15 GB, 16616.9K packets [windows]\n",
      "   4. diskjockey      (10.1.0.4    ) - 1.85 GB, 1892.9K packets [windows]\n",
      "\n",
      "📡 NETWORK COMMUNICATION PATTERNS:\n",
      "   • Internal ↔ Internal: 1,033,488 (94.8%)\n",
      "   • Internal → External: 36,939 (3.4%)\n",
      "   • Internal → Multicast: 4,176 (0.4%)\n"
     ]
    }
   ],
   "source": [
    "# Host & Network Infrastructure Analysis\n",
    "print(\"=\" * 80)\n",
    "print(\"🏠 HOST & NETWORK INFRASTRUCTURE ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n💻 HOST INFORMATION:\")\n",
    "unique_hosts = df['host_hostname'].nunique()\n",
    "unique_host_ips = df['source_ip'].nunique()\n",
    "print(f\"   • Unique hostnames: {unique_hosts:,}\")\n",
    "print(f\"   • Unique source IPs: {unique_host_ips:,}\")\n",
    "print(f\"   • Unique destination IPs: {df['destination_ip'].nunique():,}\")\n",
    "print(f\"   • Total unique IPs (src+dst): {pd.concat([df['source_ip'], df['destination_ip']]).nunique():,}\")\n",
    "\n",
    "print(f\"\\n🖥️ OPERATING SYSTEM DISTRIBUTION:\")\n",
    "os_dist = df['host_os_platform'].value_counts()\n",
    "for os_name, count in os_dist.items():\n",
    "    percentage = (count / len(df)) * 100\n",
    "    print(f\"   • {os_name.capitalize():12s}: {count:,} ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\n🌐 NETWORK TOPOLOGY ANALYSIS:\")\n",
    "# Analyze internal vs external communication\n",
    "def categorize_ip(ip):\n",
    "    # Handle NaN values\n",
    "    if pd.isna(ip) or not isinstance(ip, str):\n",
    "        return 'Unknown'\n",
    "    if ip.startswith('10.') or ip.startswith('192.168.') or ip.startswith('172.'):\n",
    "        return 'Internal'\n",
    "    elif ip.startswith('224.') or ip.startswith('239.'):\n",
    "        return 'Multicast'\n",
    "    else:\n",
    "        return 'External'\n",
    "\n",
    "df['source_ip_type'] = df['source_ip'].apply(categorize_ip)\n",
    "df['destination_ip_type'] = df['destination_ip'].apply(categorize_ip)\n",
    "\n",
    "src_ip_types = df['source_ip_type'].value_counts()\n",
    "dst_ip_types = df['destination_ip_type'].value_counts()\n",
    "\n",
    "print(f\"   Source IP Categories:\")\n",
    "for ip_type, count in src_ip_types.items():\n",
    "    percentage = (count / len(df)) * 100\n",
    "    print(f\"   • {ip_type:12s}: {count:,} ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"   Destination IP Categories:\")\n",
    "for ip_type, count in dst_ip_types.items():\n",
    "    percentage = (count / len(df)) * 100\n",
    "    print(f\"   • {ip_type:12s}: {count:,} ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\n🏆 MOST ACTIVE HOSTS:\")\n",
    "# Filter out NaN values for host analysis\n",
    "valid_hosts = df[df['host_hostname'].notna() & df['source_ip'].notna()].copy()\n",
    "if len(valid_hosts) > 0:\n",
    "    host_activity = valid_hosts.groupby('host_hostname').agg({\n",
    "        'network_bytes': 'sum',\n",
    "        'network_packets': 'sum',\n",
    "        'source_ip': lambda x: x.iloc[0],  # Get IP for this host\n",
    "        'host_os_platform': lambda x: x.iloc[0]  # Get OS for this host\n",
    "    }).sort_values('network_bytes', ascending=False).head(8)\n",
    "\n",
    "    for idx, (hostname, stats) in enumerate(host_activity.iterrows(), 1):\n",
    "        gb = stats['network_bytes'] / 1024 / 1024 / 1024\n",
    "        packets_k = stats['network_packets'] / 1000\n",
    "        print(f\"   {idx}. {hostname:15s} ({stats['source_ip']:12s}) - {gb:.2f} GB, {packets_k:.1f}K packets [{stats['host_os_platform']}]\")\n",
    "else:\n",
    "    print(\"   • No valid host data available for analysis\")\n",
    "\n",
    "print(f\"\\n📡 NETWORK COMMUNICATION PATTERNS:\")\n",
    "# Internal to internal\n",
    "internal_to_internal = df[(df['source_ip_type'] == 'Internal') & (df['destination_ip_type'] == 'Internal')].shape[0]\n",
    "# Internal to external  \n",
    "internal_to_external = df[(df['source_ip_type'] == 'Internal') & (df['destination_ip_type'] == 'External')].shape[0]\n",
    "# Internal to multicast\n",
    "internal_to_multicast = df[(df['source_ip_type'] == 'Internal') & (df['destination_ip_type'] == 'Multicast')].shape[0]\n",
    "\n",
    "print(f\"   • Internal ↔ Internal: {internal_to_internal:,} ({(internal_to_internal/len(df))*100:.1f}%)\")\n",
    "print(f\"   • Internal → External: {internal_to_external:,} ({(internal_to_external/len(df))*100:.1f}%)\")\n",
    "print(f\"   • Internal → Multicast: {internal_to_multicast:,} ({(internal_to_multicast/len(df))*100:.1f}%)\")\n",
    "\n",
    "# Clean up temporary columns\n",
    "df.drop(['source_ip_type', 'destination_ip_type'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Process Information Analysis\n",
    "\n",
    "Analyze process-level network activity, executable patterns, and process relationships when data is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "⚙️ PROCESS INFORMATION ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "📊 PROCESS DATA AVAILABILITY:\n",
      "   • process_name             : 681,740 records (62.5%)\n",
      "   • process_executable       : 681,740 records (62.5%)\n",
      "   • process_pid              : 681,740 records (62.5%)\n",
      "   • process_parent_pid       : 681,740 records (62.5%)\n",
      "\n",
      "📊 SOURCE PROCESS DATA AVAILABILITY:\n",
      "   • source_process_name           : 648,695 records (59.5%)\n",
      "   • source_process_executable     : 648,695 records (59.5%)\n",
      "   • source_process_pid            : 648,695 records (59.5%)\n",
      "   • source_process_ppid           : 648,695 records (59.5%)\n",
      "\n",
      "🔍 PROCESS NAME ANALYSIS (681,740 records with process data):\n",
      "    1. .                        : 273,055 (40.1%)\n",
      "    2. agentbeat.exe            : 264,500 (38.8%)\n",
      "    3. w3wp.exe                 : 37,047 (5.4%)\n",
      "    4. lsass.exe                : 20,198 (3.0%)\n",
      "    5. dns.exe                  : 19,537 (2.9%)\n",
      "    6. svchost.exe              : 13,518 (2.0%)\n",
      "    7. elastic-agent.exe        : 11,500 (1.7%)\n",
      "    8. Microsoft.Exchange.RpcClientAccess.Service.exe: 4,868 (0.7%)\n",
      "    9. MSExchangeHMWorker.exe   : 4,380 (0.6%)\n",
      "   10. sqlservr.exe             : 3,556 (0.5%)\n",
      "   11. Microsoft.Exchange.ServiceHost.exe: 3,466 (0.5%)\n",
      "   12. SystemFailureReporter.exe: 3,459 (0.5%)\n",
      "   13. MSExchangeMailboxAssistants.exe: 3,345 (0.5%)\n",
      "   14. SQLCMD.EXE               : 2,914 (0.4%)\n",
      "   15. Microsoft.Exchange.EdgeSyncSvc.exe: 2,423 (0.4%)\n",
      "\n",
      "💿 EXECUTABLE PATH ANALYSIS:\n",
      "    1.                                              : 273,055 (40.1%)\n",
      "    2. C:\\Program Files\\Elastic\\Agent\\data\\elastic-agent-8.18.0-1c9cf2\\components: 264,500 (38.8%)\n",
      "    3. C:\\Windows\\System32                          : 53,993 (7.9%)\n",
      "    4. C:\\Windows\\System32\\inetsrv                  : 37,047 (5.4%)\n",
      "    5. C:\\Program Files\\Microsoft\\Exchange Server\\V15\\Bin: 27,488 (4.0%)\n",
      "    6. C:\\Program Files\\Elastic\\Agent\\data\\elastic-agent-8.18.0-1c9cf2: 11,500 (1.7%)\n",
      "    7. C:\\Program Files\\Microsoft SQL Server\\MSSQL16.MSSQLSERVER\\MSSQL\\Binn: 3,689 (0.5%)\n",
      "    8. C:\\Users\\Public                              : 3,459 (0.5%)\n",
      "    9. C:\\Program Files\\Microsoft SQL Server\\Client SDK\\ODBC\\170\\Tools\\Binn: 2,914 (0.4%)\n",
      "   10. C:\\Users\\Public\\Downloads                    : 956 (0.1%)\n",
      "\n",
      "🔍 SOURCE PROCESS NAME ANALYSIS (648,695 records with source process data):\n",
      "    1. .                        : 271,487 (41.9%)\n",
      "    2. agentbeat.exe            : 264,500 (40.8%)\n",
      "    3. w3wp.exe                 : 36,775 (5.7%)\n",
      "    4. dns.exe                  : 14,545 (2.2%)\n",
      "    5. svchost.exe              : 12,163 (1.9%)\n",
      "    6. elastic-agent.exe        : 6,589 (1.0%)\n",
      "    7. Microsoft.Exchange.RpcClientAccess.Service.exe: 4,800 (0.7%)\n",
      "    8. lsass.exe                : 4,496 (0.7%)\n",
      "    9. MSExchangeHMWorker.exe   : 4,380 (0.7%)\n",
      "   10. SystemFailureReporter.exe: 3,459 (0.5%)\n",
      "   11. Microsoft.Exchange.ServiceHost.exe: 3,398 (0.5%)\n",
      "   12. MSExchangeMailboxAssistants.exe: 3,277 (0.5%)\n",
      "   13. SQLCMD.EXE               : 2,914 (0.4%)\n",
      "   14. Microsoft.Exchange.EdgeSyncSvc.exe: 2,355 (0.4%)\n",
      "   15. MSExchangeHMHost.exe     : 1,305 (0.2%)\n",
      "\n",
      "📋 PROCESS ARGUMENTS ANALYSIS:\n",
      "   • Records with process args: 406,704 (37.3%)\n",
      "   • Records with source process args: 375,325 (34.4%)\n",
      "\n",
      "📈 PROCESS ARGUMENTS STATISTICS:\n",
      "   • Average arguments per process: 19.8\n",
      "   • Max arguments: 28\n",
      "   • Processes with >5 args: 314,054\n",
      "\n",
      "🌐 PROCESS NETWORK ACTIVITY:\n",
      "   Top network-active processes:\n",
      "    1. agentbeat.exe       : 120537.1 MB total, 264500 flows,  1 ports\n",
      "    2. SystemFailureReporter.exe: 18501.8 MB total, 3459 flows,  2 ports\n",
      "    3. .                   :  8808.1 MB total, 273055 flows,  6 ports\n",
      "    4. w3wp.exe            :  3580.0 MB total, 37047 flows,  8 ports\n",
      "    5. MSExchangeHMWorker.exe:  1363.1 MB total, 4380 flows,  2 ports\n",
      "    6. lsass.exe           :   940.5 MB total, 20198 flows, 114 ports\n",
      "    7. plink.exe           :   853.6 MB total,  956 flows,  2 ports\n",
      "    8. elastic-agent.exe   :   389.9 MB total, 11500 flows, 10 ports\n",
      "    9. MSExchangeMailboxAssistants.exe:   300.8 MB total, 3345 flows,  3 ports\n",
      "   10. svchost.exe         :   220.1 MB total, 13518 flows, 18 ports\n"
     ]
    }
   ],
   "source": [
    "# Process Information Analysis\n",
    "print(\"=\" * 80)\n",
    "print(\"⚙️ PROCESS INFORMATION ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check process data availability\n",
    "process_fields = ['process_name', 'process_executable', 'process_pid', 'process_parent_pid']\n",
    "source_process_fields = ['source_process_name', 'source_process_executable', 'source_process_pid', 'source_process_ppid']\n",
    "\n",
    "print(f\"\\n📊 PROCESS DATA AVAILABILITY:\")\n",
    "for field in process_fields:\n",
    "    non_null_count = df[field].notna().sum()\n",
    "    percentage = (non_null_count / len(df)) * 100\n",
    "    print(f\"   • {field:25s}: {non_null_count:,} records ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\n📊 SOURCE PROCESS DATA AVAILABILITY:\")\n",
    "for field in source_process_fields:\n",
    "    non_null_count = df[field].notna().sum()\n",
    "    percentage = (non_null_count / len(df)) * 100\n",
    "    print(f\"   • {field:30s}: {non_null_count:,} records ({percentage:.1f}%)\")\n",
    "\n",
    "# Analyze process data when available\n",
    "process_data = df[df['process_name'].notna()].copy()\n",
    "if len(process_data) > 0:\n",
    "    print(f\"\\n🔍 PROCESS NAME ANALYSIS ({len(process_data):,} records with process data):\")\n",
    "    top_processes = process_data['process_name'].value_counts().head(15)\n",
    "    for idx, (process, count) in enumerate(top_processes.items(), 1):\n",
    "        percentage = (count / len(process_data)) * 100\n",
    "        print(f\"   {idx:2d}. {process:25s}: {count:,} ({percentage:.1f}%)\")\n",
    "\n",
    "    print(f\"\\n💿 EXECUTABLE PATH ANALYSIS:\")\n",
    "    # Extract directory paths from executables\n",
    "    exe_data = process_data[process_data['process_executable'].notna()].copy()\n",
    "    if len(exe_data) > 0:\n",
    "        exe_data['exe_dir'] = exe_data['process_executable'].apply(lambda x: '\\\\'.join(x.split('\\\\')[:-1]) if isinstance(x, str) and '\\\\' in x else x)\n",
    "        top_dirs = exe_data['exe_dir'].value_counts().head(10)\n",
    "        for idx, (directory, count) in enumerate(top_dirs.items(), 1):\n",
    "            percentage = (count / len(exe_data)) * 100\n",
    "            print(f\"   {idx:2d}. {directory:45s}: {count:,} ({percentage:.1f}%)\")\n",
    "\n",
    "# Analyze source process data when available  \n",
    "src_process_data = df[df['source_process_name'].notna()].copy()\n",
    "if len(src_process_data) > 0:\n",
    "    print(f\"\\n🔍 SOURCE PROCESS NAME ANALYSIS ({len(src_process_data):,} records with source process data):\")\n",
    "    top_src_processes = src_process_data['source_process_name'].value_counts().head(15)\n",
    "    for idx, (process, count) in enumerate(top_src_processes.items(), 1):\n",
    "        percentage = (count / len(src_process_data)) * 100\n",
    "        print(f\"   {idx:2d}. {process:25s}: {count:,} ({percentage:.1f}%)\")\n",
    "\n",
    "# Analyze process arguments when available\n",
    "print(f\"\\n📋 PROCESS ARGUMENTS ANALYSIS:\")\n",
    "process_args_available = df['process_args'].apply(lambda x: isinstance(x, list) and len(x) > 0).sum()\n",
    "source_process_args_available = df['source_process_args'].apply(lambda x: isinstance(x, list) and len(x) > 0).sum()\n",
    "\n",
    "print(f\"   • Records with process args: {process_args_available:,} ({(process_args_available/len(df))*100:.1f}%)\")\n",
    "print(f\"   • Records with source process args: {source_process_args_available:,} ({(source_process_args_available/len(df))*100:.1f}%)\")\n",
    "\n",
    "if process_args_available > 0:\n",
    "    # Analyze argument patterns\n",
    "    args_data = df[df['process_args'].apply(lambda x: isinstance(x, list) and len(x) > 0)].copy()\n",
    "    args_data['arg_count'] = args_data['process_args'].apply(len)\n",
    "    \n",
    "    print(f\"\\n📈 PROCESS ARGUMENTS STATISTICS:\")\n",
    "    print(f\"   • Average arguments per process: {args_data['arg_count'].mean():.1f}\")\n",
    "    print(f\"   • Max arguments: {args_data['arg_count'].max()}\")\n",
    "    print(f\"   • Processes with >5 args: {(args_data['arg_count'] > 5).sum():,}\")\n",
    "\n",
    "# Process network activity correlation\n",
    "if len(process_data) > 0:\n",
    "    print(f\"\\n🌐 PROCESS NETWORK ACTIVITY:\")\n",
    "    process_traffic = process_data.groupby('process_name').agg({\n",
    "        'network_bytes': ['sum', 'mean', 'count'],\n",
    "        'destination_port': lambda x: x.nunique()\n",
    "    }).round(2)\n",
    "    \n",
    "    process_traffic.columns = ['total_bytes', 'avg_bytes', 'flow_count', 'unique_ports']\n",
    "    process_traffic = process_traffic.sort_values('total_bytes', ascending=False).head(10)\n",
    "    \n",
    "    print(f\"   Top network-active processes:\")\n",
    "    for idx, (process, stats) in enumerate(process_traffic.iterrows(), 1):\n",
    "        mb = stats['total_bytes'] / 1024 / 1024\n",
    "        print(f\"   {idx:2d}. {process:20s}: {mb:7.1f} MB total, {stats['flow_count']:4.0f} flows, {stats['unique_ports']:2.0f} ports\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Temporal Analysis\n",
    "\n",
    "Examine traffic patterns over time, flow durations, and temporal characteristics of network communication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "⏰ TEMPORAL ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "📅 TIME RANGE ANALYSIS:\n",
      "   • Start time: 2025-05-04 11:30:08.613000+00:00\n",
      "   • End time: 2025-05-04 12:40:00.999000+00:00\n",
      "   • Total duration: 0 days 01:09:52.386000\n",
      "   • Total hours: 1.2 hours\n",
      "\n",
      "⏱️ FLOW DURATION ANALYSIS:\n",
      "   • Min duration: 0.000 seconds\n",
      "   • Max duration: 3414.9 seconds (56.9 minutes)\n",
      "   • Mean duration: 47.658 seconds\n",
      "   • Median duration: 1.371 seconds\n",
      "\n",
      "📊 FLOW DURATION CATEGORIES:\n",
      "   • Very short (<100ms): 417,770 (38.3%)\n",
      "   • Short (100ms-1s): 118,352 (10.9%)\n",
      "   • Medium (1s-1min): 430,465 (39.5%)\n",
      "   • Long (>1min): 123,625 (11.3%)\n",
      "\n",
      "📈 TRAFFIC PATTERNS BY HOUR:\n",
      "   Hour | Flows   | Traffic (GB)\n",
      "   -----|---------|-------------\n",
      "    0h  |       0 |     0.00\n",
      "    1h  |       0 |     0.00\n",
      "    2h  |       0 |     0.00\n",
      "    3h  |       0 |     0.00\n",
      "    4h  |       0 |     0.00\n",
      "    5h  |       0 |     0.00\n",
      "    6h  |       0 |     0.00\n",
      "    7h  |       0 |     0.00\n",
      "    8h  |       0 |     0.00\n",
      "    9h  |       0 |     0.00\n",
      "   10h  |       0 |     0.00\n",
      "   11h  | 362,902 |    96.27\n",
      "   12h  | 727,310 |   135.60\n",
      "   13h  |       0 |     0.00\n",
      "   14h  |       0 |     0.00\n",
      "   15h  |       0 |     0.00\n",
      "   16h  |       0 |     0.00\n",
      "   17h  |       0 |     0.00\n",
      "   18h  |       0 |     0.00\n",
      "   19h  |       0 |     0.00\n",
      "   20h  |       0 |     0.00\n",
      "   21h  |       0 |     0.00\n",
      "   22h  |       0 |     0.00\n",
      "   23h  |       0 |     0.00\n",
      "\n",
      "🏆 PEAK ACTIVITY:\n",
      "   • Peak flows: 12:00 (727,310 flows)\n",
      "   • Peak traffic: 12:00 (135.60 GB)\n",
      "\n",
      "📊 TRAFFIC PATTERNS BY DAY:\n",
      "   Date       | Flows    | Traffic (GB)\n",
      "   -----------|-----------|--------------\n",
      "   2025-05-04 | 1,090,212 |    231.87\n"
     ]
    }
   ],
   "source": [
    "# Temporal Analysis\n",
    "print(\"=\" * 80)\n",
    "print(\"⏰ TEMPORAL ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Convert timestamp to datetime for analysis\n",
    "df['timestamp_dt'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "print(f\"\\n📅 TIME RANGE ANALYSIS:\")\n",
    "time_range = df['timestamp_dt'].agg(['min', 'max'])\n",
    "duration = time_range['max'] - time_range['min']\n",
    "print(f\"   • Start time: {time_range['min']}\")\n",
    "print(f\"   • End time: {time_range['max']}\")\n",
    "print(f\"   • Total duration: {duration}\")\n",
    "print(f\"   • Total hours: {duration.total_seconds() / 3600:.1f} hours\")\n",
    "\n",
    "print(f\"\\n⏱️ FLOW DURATION ANALYSIS:\")\n",
    "# Convert event duration from nanoseconds to more readable units\n",
    "df['duration_seconds'] = df['event_duration'] / 1_000_000_000  # Convert nanoseconds to seconds\n",
    "duration_stats = df['duration_seconds'].describe()\n",
    "\n",
    "print(f\"   • Min duration: {duration_stats['min']:.3f} seconds\")\n",
    "print(f\"   • Max duration: {duration_stats['max']:.1f} seconds ({duration_stats['max']/60:.1f} minutes)\")\n",
    "print(f\"   • Mean duration: {duration_stats['mean']:.3f} seconds\")\n",
    "print(f\"   • Median duration: {duration_stats['50%']:.3f} seconds\")\n",
    "\n",
    "# Categorize flows by duration\n",
    "very_short = (df['duration_seconds'] < 0.1).sum()  # < 100ms\n",
    "short = ((df['duration_seconds'] >= 0.1) & (df['duration_seconds'] < 1)).sum()  # 100ms - 1s\n",
    "medium = ((df['duration_seconds'] >= 1) & (df['duration_seconds'] < 60)).sum()  # 1s - 1min\n",
    "long_flows = (df['duration_seconds'] >= 60).sum()  # > 1min\n",
    "\n",
    "print(f\"\\n📊 FLOW DURATION CATEGORIES:\")\n",
    "print(f\"   • Very short (<100ms): {very_short:,} ({(very_short/len(df))*100:.1f}%)\")\n",
    "print(f\"   • Short (100ms-1s): {short:,} ({(short/len(df))*100:.1f}%)\")\n",
    "print(f\"   • Medium (1s-1min): {medium:,} ({(medium/len(df))*100:.1f}%)\")\n",
    "print(f\"   • Long (>1min): {long_flows:,} ({(long_flows/len(df))*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n📈 TRAFFIC PATTERNS BY HOUR:\")\n",
    "df['hour'] = df['timestamp_dt'].dt.hour\n",
    "hourly_traffic = df.groupby('hour').agg({\n",
    "    'network_bytes': 'sum',\n",
    "    'timestamp': 'count'  # Flow count\n",
    "}).round(2)\n",
    "\n",
    "hourly_traffic['bytes_gb'] = hourly_traffic['network_bytes'] / 1024 / 1024 / 1024\n",
    "print(\"   Hour | Flows   | Traffic (GB)\")\n",
    "print(\"   -----|---------|-------------\")\n",
    "for hour in range(24):\n",
    "    if hour in hourly_traffic.index:\n",
    "        flows = hourly_traffic.loc[hour, 'timestamp']\n",
    "        gb = hourly_traffic.loc[hour, 'bytes_gb']\n",
    "        print(f\"   {hour:2d}h  | {flows:7,.0f} | {gb:8.2f}\")\n",
    "    else:\n",
    "        print(f\"   {hour:2d}h  |       0 |     0.00\")\n",
    "\n",
    "# Find peak hours\n",
    "peak_flows_hour = hourly_traffic['timestamp'].idxmax()\n",
    "peak_traffic_hour = hourly_traffic['bytes_gb'].idxmax()\n",
    "print(f\"\\n🏆 PEAK ACTIVITY:\")\n",
    "print(f\"   • Peak flows: {peak_flows_hour:02d}:00 ({hourly_traffic.loc[peak_flows_hour, 'timestamp']:,.0f} flows)\")\n",
    "print(f\"   • Peak traffic: {peak_traffic_hour:02d}:00 ({hourly_traffic.loc[peak_traffic_hour, 'bytes_gb']:.2f} GB)\")\n",
    "\n",
    "print(f\"\\n📊 TRAFFIC PATTERNS BY DAY:\")\n",
    "df['date'] = df['timestamp_dt'].dt.date\n",
    "daily_traffic = df.groupby('date').agg({\n",
    "    'network_bytes': 'sum',\n",
    "    'timestamp': 'count'\n",
    "}).round(2)\n",
    "\n",
    "daily_traffic['bytes_gb'] = daily_traffic['network_bytes'] / 1024 / 1024 / 1024\n",
    "print(\"   Date       | Flows    | Traffic (GB)\")\n",
    "print(\"   -----------|-----------|--------------\")\n",
    "for date, stats in daily_traffic.iterrows():\n",
    "    flows = stats['timestamp']\n",
    "    gb = stats['bytes_gb']\n",
    "    print(f\"   {date} | {flows:8,.0f} | {gb:9.2f}\")\n",
    "\n",
    "# Analyze weekend vs weekday patterns (if applicable)\n",
    "df['weekday'] = df['timestamp_dt'].dt.day_name()\n",
    "weekday_traffic = df.groupby('weekday').agg({\n",
    "    'network_bytes': 'sum',\n",
    "    'timestamp': 'count'\n",
    "}).round(2)\n",
    "\n",
    "if len(weekday_traffic) > 1:\n",
    "    print(f\"\\n📅 WEEKDAY PATTERNS:\")\n",
    "    weekday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    for day in weekday_order:\n",
    "        if day in weekday_traffic.index:\n",
    "            flows = weekday_traffic.loc[day, 'timestamp']\n",
    "            gb = weekday_traffic.loc[day, 'network_bytes'] / 1024 / 1024 / 1024\n",
    "            print(f\"   • {day:9s}: {flows:,} flows, {gb:.2f} GB\")\n",
    "\n",
    "# Clean up temporary columns\n",
    "df.drop(['timestamp_dt', 'duration_seconds', 'hour', 'date', 'weekday'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Security-Focused Analysis\n",
    "\n",
    "Identify potential security patterns, anomalies, and suspicious network behaviors in the traffic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "🔒 SECURITY-FOCUSED ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "🚨 UNUSUAL PORT ANALYSIS:\n",
      "   • Connections to high ports (>50000): 9,732 (0.89%)\n",
      "   • Connections to suspicious ports: 0\n",
      "\n",
      "📊 TRAFFIC VOLUME ANOMALIES:\n",
      "   • Large flows (>99th percentile, >3,645,820 bytes): 10,871 (1.00%)\n",
      "   • High outbound traffic (>95th percentile): 54,471 (5.00%)\n",
      "\n",
      "🔍 COMMUNICATION PATTERN ANALYSIS:\n",
      "   • Potential port scanners (>50 dest ports): 2\n",
      "   Top potential scanners:\n",
      "   1. 10.2.0.20: 152 unique ports, 13,606 total flows\n",
      "   2. 10.1.0.4: 119 unique ports, 36,450 total flows\n",
      "\n",
      "📡 POTENTIAL BEACONING ANALYSIS:\n",
      "   • Frequent communication pairs (>100 flows): 141\n",
      "   Top communication pairs:\n",
      "   1. 10.1.0.6 → 192.168.0.4: 269,232 flows, avg 15994 bytes\n",
      "   2. 10.1.0.5 → 10.2.0.20: 219,454 flows, avg 648136 bytes\n",
      "   3. 10.1.0.7 → 10.2.0.20: 168,045 flows, avg 65739 bytes\n",
      "   4. 10.1.0.6 → 10.2.0.20: 163,501 flows, avg 341393 bytes\n",
      "   5. 10.1.0.6 → 10.1.0.4: 129,472 flows, avg 59452 bytes\n",
      "   6. 10.1.0.5 → 10.1.0.4: 20,535 flows, avg 1876 bytes\n",
      "   7. 10.1.0.4 → 10.2.0.20: 15,805 flows, avg 62715 bytes\n",
      "   8. 10.1.0.7 → 10.1.0.4: 10,750 flows, avg 1932 bytes\n",
      "\n",
      "🔗 NETWORK FLOW CHARACTERISTICS:\n",
      "   • Unique flow IDs: 20,421\n",
      "   • Records per flow (avg): 53.4\n",
      "   • Multi-record flows: 20,208 (99.0%)\n",
      "\n",
      "🎯 EXTERNAL COMMUNICATION ANALYSIS:\n",
      "   • Flows to external IPs: 45,038 (4.1%)\n",
      "   Top external destinations:\n",
      "    1. 23.221.70.13   : 3,081 flows, 3.8 MB received\n",
      "    2. ff02::1:2      : 2,084 flows, 0.0 MB received\n",
      "    3. ff02::1:3      : 1,621 flows, 0.0 MB received\n",
      "    4. 23.54.61.183   : 1,588 flows, 12.1 MB received\n",
      "    5. ff02::fb       : 1,386 flows, 0.0 MB received\n",
      "    6. 204.61.216.50  : 1,071 flows, 0.4 MB received\n",
      "    7. 239.255.255.250: 1,023 flows, 0.0 MB received\n",
      "    8. 52.231.186.36  : 804 flows, 3.6 MB received\n",
      "    9. 199.180.180.63 : 661 flows, 0.3 MB received\n",
      "   10. 52.247.240.158 : 615 flows, 0.9 MB received\n",
      "\n",
      "🛡️ PROTOCOL SECURITY ANALYSIS:\n",
      "   • Encrypted traffic (common secure ports): 17,693 (1.6%)\n",
      "   • Unencrypted traffic (common insecure ports): 53,540 (4.9%)\n",
      "\n",
      "📋 SECURITY SUMMARY:\n",
      "   • Total network flows analyzed: 1,090,212\n",
      "   • Unique source IPs: 38\n",
      "   • Unique destination IPs: 644\n",
      "   • Most active protocol: TCP\n",
      "   • Average flow duration: 47.66 seconds\n",
      "   • Data quality: 84.7% fields populated\n",
      "\n",
      "🎉 EXPLORATORY ANALYSIS COMPLETE!\n",
      "   The dataset has been thoroughly analyzed and is ready for machine learning workflows.\n",
      "   Key insights have been extracted for cybersecurity analysis and anomaly detection.\n"
     ]
    }
   ],
   "source": [
    "# Security-Focused Analysis\n",
    "print(\"=\" * 80)\n",
    "print(\"🔒 SECURITY-FOCUSED ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n🚨 UNUSUAL PORT ANALYSIS:\")\n",
    "# Define suspicious/uncommon ports\n",
    "suspicious_ports = [1337, 31337, 4444, 6666, 8080, 1234, 12345, 54321, 9999]\n",
    "high_ports = df[df['destination_port'] > 50000]\n",
    "unusual_ports = df[df['destination_port'].isin(suspicious_ports)]\n",
    "\n",
    "print(f\"   • Connections to high ports (>50000): {len(high_ports):,} ({(len(high_ports)/len(df))*100:.2f}%)\")\n",
    "print(f\"   • Connections to suspicious ports: {len(unusual_ports):,}\")\n",
    "\n",
    "if len(unusual_ports) > 0:\n",
    "    print(f\"   Suspicious port activity:\")\n",
    "    for port in suspicious_ports:\n",
    "        count = (df['destination_port'] == port).sum()\n",
    "        if count > 0:\n",
    "            print(f\"   • Port {port}: {count:,} connections\")\n",
    "\n",
    "print(f\"\\n📊 TRAFFIC VOLUME ANOMALIES:\")\n",
    "# Identify flows with unusually high byte transfers\n",
    "bytes_q99 = df['network_bytes'].quantile(0.99)\n",
    "large_flows = df[df['network_bytes'] > bytes_q99]\n",
    "print(f\"   • Large flows (>99th percentile, >{bytes_q99:,.0f} bytes): {len(large_flows):,} ({(len(large_flows)/len(df))*100:.2f}%)\")\n",
    "\n",
    "# Identify potential data exfiltration (high outbound traffic)\n",
    "high_outbound = df[df['source_bytes'] > df['source_bytes'].quantile(0.95)]\n",
    "print(f\"   • High outbound traffic (>95th percentile): {len(high_outbound):,} ({(len(high_outbound)/len(df))*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\n🔍 COMMUNICATION PATTERN ANALYSIS:\")\n",
    "# Analyze port scanning behavior (same source to multiple destination ports) - filter valid IPs\n",
    "valid_source_ips = df[df['source_ip'].notna() & df['destination_port'].notna()].copy()\n",
    "if len(valid_source_ips) > 0:\n",
    "    source_port_diversity = valid_source_ips.groupby('source_ip')['destination_port'].nunique().sort_values(ascending=False)\n",
    "    potential_scanners = source_port_diversity[source_port_diversity > 50]  # More than 50 different dest ports\n",
    "\n",
    "    print(f\"   • Potential port scanners (>50 dest ports): {len(potential_scanners):,}\")\n",
    "    if len(potential_scanners) > 0:\n",
    "        print(f\"   Top potential scanners:\")\n",
    "        for idx, (ip, port_count) in enumerate(potential_scanners.head(5).items(), 1):\n",
    "            flow_count = (df['source_ip'] == ip).sum()\n",
    "            print(f\"   {idx}. {ip}: {port_count} unique ports, {flow_count:,} total flows\")\n",
    "else:\n",
    "    print(f\"   • No valid source IP data for port scanning analysis\")\n",
    "\n",
    "# Analyze beaconing behavior (regular communication patterns)\n",
    "print(f\"\\n📡 POTENTIAL BEACONING ANALYSIS:\")\n",
    "valid_comm_data = df[df['source_ip'].notna() & df['destination_ip'].notna()].copy()\n",
    "if len(valid_comm_data) > 0:\n",
    "    communication_pairs = valid_comm_data.groupby(['source_ip', 'destination_ip']).size().sort_values(ascending=False)\n",
    "    frequent_pairs = communication_pairs[communication_pairs > 100]  # More than 100 flows between same IPs\n",
    "\n",
    "    print(f\"   • Frequent communication pairs (>100 flows): {len(frequent_pairs):,}\")\n",
    "    if len(frequent_pairs) > 0:\n",
    "        print(f\"   Top communication pairs:\")\n",
    "        for idx, ((src, dst), count) in enumerate(frequent_pairs.head(8).items(), 1):\n",
    "            avg_bytes = df[(df['source_ip'] == src) & (df['destination_ip'] == dst)]['network_bytes'].mean()\n",
    "            print(f\"   {idx}. {src} → {dst}: {count:,} flows, avg {avg_bytes:.0f} bytes\")\n",
    "else:\n",
    "    print(f\"   • No valid communication data for beaconing analysis\")\n",
    "\n",
    "print(f\"\\n🔗 NETWORK FLOW CHARACTERISTICS:\")\n",
    "# Analyze flow finality (completed vs ongoing connections)\n",
    "if 'network_traffic_flow_id' in df.columns:\n",
    "    valid_flows = df[df['network_traffic_flow_id'].notna()]\n",
    "    if len(valid_flows) > 0:\n",
    "        unique_flows = valid_flows['network_traffic_flow_id'].nunique()\n",
    "        total_records = len(valid_flows)\n",
    "        print(f\"   • Unique flow IDs: {unique_flows:,}\")\n",
    "        print(f\"   • Records per flow (avg): {total_records/unique_flows:.1f}\")\n",
    "        \n",
    "        # Find flows with multiple records (ongoing connections)\n",
    "        flow_counts = valid_flows['network_traffic_flow_id'].value_counts()\n",
    "        multi_record_flows = flow_counts[flow_counts > 1]\n",
    "        print(f\"   • Multi-record flows: {len(multi_record_flows):,} ({(len(multi_record_flows)/unique_flows)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n🎯 EXTERNAL COMMUNICATION ANALYSIS:\")\n",
    "# Categorize IPs and analyze external communication\n",
    "def is_external_ip(ip):\n",
    "    # Handle NaN values\n",
    "    if pd.isna(ip) or not isinstance(ip, str):\n",
    "        return False\n",
    "    return not (ip.startswith('10.') or ip.startswith('192.168.') or ip.startswith('172.') or ip.startswith('224.'))\n",
    "\n",
    "external_dest_flows = df[df['destination_ip'].apply(is_external_ip)]\n",
    "if len(external_dest_flows) > 0:\n",
    "    print(f\"   • Flows to external IPs: {len(external_dest_flows):,} ({(len(external_dest_flows)/len(df))*100:.1f}%)\")\n",
    "    \n",
    "    # Top external destinations\n",
    "    top_external = external_dest_flows['destination_ip'].value_counts().head(10)\n",
    "    print(f\"   Top external destinations:\")\n",
    "    for idx, (ip, count) in enumerate(top_external.items(), 1):\n",
    "        total_bytes = external_dest_flows[external_dest_flows['destination_ip'] == ip]['destination_bytes'].sum()\n",
    "        mb = total_bytes / 1024 / 1024\n",
    "        print(f\"   {idx:2d}. {ip:15s}: {count:,} flows, {mb:.1f} MB received\")\n",
    "else:\n",
    "    print(f\"   • No external communication detected\")\n",
    "\n",
    "print(f\"\\n🛡️ PROTOCOL SECURITY ANALYSIS:\")\n",
    "# Analyze encrypted vs unencrypted traffic based on common ports\n",
    "encrypted_ports = [443, 993, 995, 636, 22, 990]  # HTTPS, IMAPS, POP3S, LDAPS, SSH, FTPS\n",
    "unencrypted_ports = [80, 21, 23, 25, 53, 110, 143]  # HTTP, FTP, Telnet, SMTP, DNS, POP3, IMAP\n",
    "\n",
    "# Filter valid port data\n",
    "valid_port_data = df[df['destination_port'].notna()]\n",
    "if len(valid_port_data) > 0:\n",
    "    encrypted_traffic = valid_port_data[valid_port_data['destination_port'].isin(encrypted_ports)]\n",
    "    unencrypted_traffic = valid_port_data[valid_port_data['destination_port'].isin(unencrypted_ports)]\n",
    "\n",
    "    print(f\"   • Encrypted traffic (common secure ports): {len(encrypted_traffic):,} ({(len(encrypted_traffic)/len(df))*100:.1f}%)\")\n",
    "    print(f\"   • Unencrypted traffic (common insecure ports): {len(unencrypted_traffic):,} ({(len(unencrypted_traffic)/len(df))*100:.1f}%)\")\n",
    "else:\n",
    "    print(f\"   • No valid port data for security analysis\")\n",
    "\n",
    "print(f\"\\n📋 SECURITY SUMMARY:\")\n",
    "print(f\"   • Total network flows analyzed: {len(df):,}\")\n",
    "print(f\"   • Unique source IPs: {df['source_ip'].nunique():,}\")\n",
    "print(f\"   • Unique destination IPs: {df['destination_ip'].nunique():,}\")\n",
    "if df['network_transport'].notna().any():\n",
    "    print(f\"   • Most active protocol: {df['network_transport'].mode()[0].upper()}\")\n",
    "print(f\"   • Average flow duration: {df['event_duration'].mean()/1e9:.2f} seconds\")\n",
    "print(f\"   • Data quality: {((df.notna().sum().sum() / (len(df) * len(df.columns))) * 100):.1f}% fields populated\")\n",
    "\n",
    "print(f\"\\n🎉 EXPLORATORY ANALYSIS COMPLETE!\")\n",
    "print(f\"   The dataset has been thoroughly analyzed and is ready for machine learning workflows.\")\n",
    "print(f\"   Key insights have been extracted for cybersecurity analysis and anomaly detection.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Analysis Summary\n",
    "\n",
    "This comprehensive exploratory data analysis has revealed key insights about the network traffic dataset:\n",
    "\n",
    "### 📊 **Key Findings**\n",
    "- **Dataset Scale**: 1,090,212 network flow records with 34 features\n",
    "- **Time Coverage**: Complete temporal analysis of network activity patterns\n",
    "- **Protocol Distribution**: Analysis of TCP/UDP traffic characteristics\n",
    "- **Security Insights**: Port scanning detection, anomaly identification, encryption analysis\n",
    "\n",
    "### 🔍 **Analysis Coverage**\n",
    "1. **Data Quality Assessment**: Missing data patterns, field completeness\n",
    "2. **Network Protocols**: Transport layer analysis, port distributions\n",
    "3. **Traffic Patterns**: Volume analysis, top talkers, communication flows\n",
    "4. **Infrastructure**: Host analysis, network topology, OS distributions\n",
    "5. **Process Activity**: Executable analysis, process network behavior\n",
    "6. **Temporal Patterns**: Hourly/daily traffic trends, flow durations\n",
    "7. **Security Analysis**: Anomaly detection, suspicious patterns, threat indicators\n",
    "\n",
    "### 🚀 **Ready for Machine Learning**\n",
    "The dataset is now fully characterized and prepared for:\n",
    "- **Anomaly Detection Models**: SAE, LSTM-SAE, Isolation Forest\n",
    "- **Classification Tasks**: Attack vs benign traffic classification\n",
    "- **Feature Engineering**: Based on identified patterns and distributions\n",
    "- **Security Analytics**: Threat detection and network behavior analysis\n",
    "\n",
    "### 📋 **Next Steps**\n",
    "1. Use insights for feature selection in ML models\n",
    "2. Apply findings to enhance anomaly detection algorithms\n",
    "3. Leverage temporal patterns for sequential modeling approaches\n",
    "4. Implement security-focused analysis for threat detection"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataset-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
