{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initial Setup: Logging System and Output Organization\n",
    "\n",
    "**What this section does**: Sets up a comprehensive logging system to capture all analysis outputs in organized files for easy inspection and review.\n",
    "\n",
    "**Simple explanation**: \n",
    "- Creates output directories for different analysis types (network-flows vs sysmon)\n",
    "- Generates timestamped log files so you can track when analysis was run\n",
    "- Sets up automatic output capture so everything gets saved to files\n",
    "\n",
    "**Paraphrasing tips**:\n",
    "- \"What type of data am I analyzing?\" → Sets `ANALYSIS_TYPE` variable\n",
    "- \"Where will my results be saved?\" → Creates organized folder structure in `outputs/`\n",
    "- \"How can I review what happened?\" → All print statements get saved to timestamped log files\n",
    "\n",
    "**Key insight**: This creates a professional logging infrastructure that makes it easy to review analysis results later and compare different runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Analysis type: 3b-network-flows\n",
      "📁 Output directory: outputs/3b-network-flows\n",
      "📊 Log file: outputs/3b-network-flows/3b-network-flows_structure_analysis_20250629_065016.log\n",
      "💾 Results file: outputs/3b-network-flows/3b-network-flows_structure_results_20250629_065016.json\n",
      "✅ Logging system initialized for network-flows structure analysis!\n",
      "📂 All outputs will be organized in: outputs/3b-network-flows/\n",
      "🔍 Use 'with capture_output(\"Section Name\", section_number):' to log outputs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "from datetime import datetime\n",
    "from contextlib import contextmanager\n",
    "\n",
    "# Define analysis type and create organized output structure\n",
    "ANALYSIS_TYPE = \"3b-network-flows\"  # This will be \"sysmon\" for sysmon analysis\n",
    "outputs_base_dir = \"outputs\"\n",
    "analysis_outputs_dir = f\"{outputs_base_dir}/{ANALYSIS_TYPE}\"\n",
    "\n",
    "# Create output directories\n",
    "if not os.path.exists(outputs_base_dir):\n",
    "    os.makedirs(outputs_base_dir)\n",
    "    print(f\"✅ Created base outputs directory: {outputs_base_dir}\")\n",
    "\n",
    "if not os.path.exists(analysis_outputs_dir):\n",
    "    os.makedirs(analysis_outputs_dir)\n",
    "    print(f\"✅ Created analysis outputs directory: {analysis_outputs_dir}\")\n",
    "\n",
    "# Generate timestamped filenames with descriptive names\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "log_filename = f\"{analysis_outputs_dir}/{ANALYSIS_TYPE}_structure_analysis_{timestamp}.log\"\n",
    "results_filename = f\"{analysis_outputs_dir}/{ANALYSIS_TYPE}_structure_results_{timestamp}.json\"\n",
    "\n",
    "print(f\"📝 Analysis type: {ANALYSIS_TYPE}\")\n",
    "print(f\"📁 Output directory: {analysis_outputs_dir}\")\n",
    "print(f\"📊 Log file: {log_filename}\")\n",
    "print(f\"💾 Results file: {results_filename}\")\n",
    "\n",
    "# Initialize log file with header\n",
    "with open(log_filename, 'w', encoding='utf-8') as log_file:\n",
    "    log_file.write(f\"NETWORK FLOWS STRUCTURE CONSISTENCY ANALYSIS\\n\")\n",
    "    log_file.write(f\"Analysis Type: {ANALYSIS_TYPE.upper()}\\n\")\n",
    "    log_file.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    log_file.write(f\"Target File: -ds-logs-network_traffic-flow-default-2025-05-04-000001.jsonl\\n\")\n",
    "    log_file.write(f\"{'='*80}\\n\\n\")\n",
    "\n",
    "class LogCapture:\n",
    "    def __init__(self, log_filename):\n",
    "        self.log_filename = log_filename\n",
    "        self.original_stdout = sys.stdout\n",
    "    \n",
    "    def write(self, text):\n",
    "        self.original_stdout.write(text)\n",
    "        self.original_stdout.flush()\n",
    "        \n",
    "        with open(self.log_filename, 'a', encoding='utf-8') as log_file:\n",
    "            log_file.write(text)\n",
    "            log_file.flush()\n",
    "    \n",
    "    def flush(self):\n",
    "        self.original_stdout.flush()\n",
    "\n",
    "log_capture = LogCapture(log_filename)\n",
    "\n",
    "def log_section(section_name, section_number=None):\n",
    "    header = f\"\\n{'='*80}\\n\"\n",
    "    if section_number:\n",
    "        header += f\"SECTION {section_number}: {section_name.upper()}\\n\"\n",
    "    else:\n",
    "        header += f\"{section_name.upper()}\\n\"\n",
    "    header += f\"{'='*80}\\n\\n\"\n",
    "    \n",
    "    with open(log_filename, 'a', encoding='utf-8') as log_file:\n",
    "        log_file.write(header)\n",
    "    \n",
    "    print(header.strip())\n",
    "\n",
    "@contextmanager\n",
    "def capture_output(section_name, section_number=None):\n",
    "    log_section(section_name, section_number)\n",
    "    \n",
    "    original_stdout = sys.stdout\n",
    "    sys.stdout = log_capture\n",
    "    \n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        sys.stdout = original_stdout\n",
    "        \n",
    "        with open(log_filename, 'a', encoding='utf-8') as log_file:\n",
    "            log_file.write(f\"\\n{'-'*60} END SECTION {'-'*60}\\n\\n\")\n",
    "\n",
    "print(\"✅ Logging system initialized for network-flows structure analysis!\")\n",
    "print(f\"📂 All outputs will be organized in: {analysis_outputs_dir}/\")\n",
    "print(f\"🔍 Use 'with capture_output(\\\"Section Name\\\", section_number):' to log outputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Schema Fingerprinting Functions: The Core Analysis Engine\n",
    "\n",
    "**What this section does**: Defines the fundamental functions that detect and classify different data structure patterns in JSON records.\n",
    "\n",
    "**Simple explanation**: \n",
    "- `generate_schema_fingerprint()`: Creates a unique \"fingerprint\" for each record's structure using MD5 hashing\n",
    "- `classify_structure_variations()`: Groups similar structures and labels them by frequency (PRIMARY, SECONDARY, VARIANT, etc.)\n",
    "- `analyze_field_presence_patterns()`: Tracks which fields appear together and how often\n",
    "\n",
    "**Paraphrasing tips**:\n",
    "- \"What patterns exist in my data?\" → The fingerprinting function identifies unique structural patterns\n",
    "- \"How important is each pattern?\" → Classification function ranks patterns by frequency and importance\n",
    "- \"Which fields always appear together?\" → Field analysis reveals co-occurrence relationships\n",
    "\n",
    "**Key insight**: These functions work together to transform chaotic JSON data into organized patterns that can be systematically processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Libraries imported successfully\n",
      "✅ All analysis functions defined\n",
      "📊 Analysis started at: 2025-06-29 06:50:16\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "import hashlib\n",
    "from collections import Counter, defaultdict\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "def generate_schema_fingerprint(record, max_depth=10):\n",
    "    \"\"\"\n",
    "    Generate a structural fingerprint for a JSON record using MD5 hashing.\n",
    "    Returns both hash and detailed structure for analysis.\n",
    "    \"\"\"\n",
    "    def extract_structure(obj, current_depth=0):\n",
    "        if current_depth >= max_depth:\n",
    "            return \"max_depth_reached\"\n",
    "        \n",
    "        if isinstance(obj, dict):\n",
    "            # Sort keys for consistent hashing\n",
    "            structure = {}\n",
    "            for key in sorted(obj.keys()):\n",
    "                structure[key] = extract_structure(obj[key], current_depth + 1)\n",
    "            return structure\n",
    "        elif isinstance(obj, list):\n",
    "            if not obj:\n",
    "                return \"empty_list\"\n",
    "            # For lists, analyze first element structure and indicate it's a list\n",
    "            return {\"list_of\": extract_structure(obj[0], current_depth + 1)}\n",
    "        else:\n",
    "            # Return type information for primitive values\n",
    "            return type(obj).__name__\n",
    "    \n",
    "    # Extract the complete structure\n",
    "    structure_detail = extract_structure(record)\n",
    "    \n",
    "    # Create a stable string representation for hashing\n",
    "    structure_str = json.dumps(structure_detail, sort_keys=True)\n",
    "    \n",
    "    # Generate MD5 hash for the structure\n",
    "    structure_hash = hashlib.md5(structure_str.encode('utf-8')).hexdigest()\n",
    "    \n",
    "    return structure_hash, structure_detail\n",
    "\n",
    "def classify_structure_variations(structure_counts, total_samples):\n",
    "    \"\"\"\n",
    "    Classify structure patterns based on frequency and characteristics.\n",
    "    \"\"\"\n",
    "    classifications = {}\n",
    "    \n",
    "    for structure_hash, count in structure_counts.items():\n",
    "        percentage = (count / total_samples) * 100\n",
    "        \n",
    "        if percentage >= 50:\n",
    "            classification = \"PRIMARY_SCHEMA\"\n",
    "        elif percentage >= 20:\n",
    "            classification = \"SECONDARY_SCHEMA\"\n",
    "        elif percentage >= 5:\n",
    "            classification = \"VARIANT\"\n",
    "        elif percentage >= 1:\n",
    "            classification = \"RARE_VARIANT\"\n",
    "        else:\n",
    "            classification = \"OUTLIER\"\n",
    "        \n",
    "        classifications[structure_hash] = {\n",
    "            'classification': classification,\n",
    "            'count': count,\n",
    "            'percentage': percentage\n",
    "        }\n",
    "    \n",
    "    return classifications\n",
    "\n",
    "def analyze_field_presence_patterns(records):\n",
    "    \"\"\"\n",
    "    Analyze which fields appear together and count field frequency.\n",
    "    \"\"\"\n",
    "    def extract_all_field_paths(obj, prefix=\"\"):\n",
    "        \"\"\"Recursively extract all field paths from a nested object.\"\"\"\n",
    "        paths = []\n",
    "        \n",
    "        if isinstance(obj, dict):\n",
    "            for key, value in obj.items():\n",
    "                current_path = f\"{prefix}.{key}\" if prefix else key\n",
    "                paths.append(current_path)\n",
    "                \n",
    "                if isinstance(value, (dict, list)):\n",
    "                    paths.extend(extract_all_field_paths(value, current_path))\n",
    "        elif isinstance(obj, list) and obj:\n",
    "            # For lists, analyze the first element\n",
    "            paths.extend(extract_all_field_paths(obj[0], prefix))\n",
    "        \n",
    "        return paths\n",
    "    \n",
    "    # Track field combinations and individual field counts\n",
    "    field_combinations = Counter()\n",
    "    field_counts = Counter()\n",
    "    \n",
    "    for record in records:\n",
    "        # Get all field paths in this record\n",
    "        field_paths = extract_all_field_paths(record)\n",
    "        \n",
    "        # Count individual fields\n",
    "        for path in field_paths:\n",
    "            field_counts[path] += 1\n",
    "        \n",
    "        # Count this specific combination of fields\n",
    "        field_combo = tuple(sorted(field_paths))\n",
    "        field_combinations[field_combo] += 1\n",
    "    \n",
    "    return field_combinations, field_counts\n",
    "\n",
    "print(\"✅ Libraries imported successfully\")\n",
    "print(\"✅ All analysis functions defined\")\n",
    "print(f\"📊 Analysis started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Sampling: Smart Data Collection Strategy\n",
    "\n",
    "**What this section does**: Loads a representative sample of records from the massive JSONL file using stratified sampling for better coverage across the entire dataset.\n",
    "\n",
    "**Simple explanation**: \n",
    "- Counts total records in the file first (over 1 million records!)\n",
    "- Uses stratified sampling to get records from throughout the file, not just the beginning\n",
    "- Collects 5,000 samples that represent the entire dataset's diversity\n",
    "\n",
    "**Paraphrasing tips**:\n",
    "- \"How big is my dataset?\" → Counts total records to understand scope\n",
    "- \"How do I avoid bias?\" → Stratified sampling ensures representative coverage\n",
    "- \"What's a good sample size?\" → 5,000 records provides statistical confidence\n",
    "\n",
    "**Key insight**: Smart sampling is crucial for large datasets - taking records from throughout the file prevents bias from temporal changes or data collection patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples_to_analyze = 200_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SECTION 3: DATA LOADING AND SAMPLING\n",
      "================================================================================\n",
      "🔄 Loading samples from: -ds-logs-network_traffic-flow-default-2025-05-04-000001.jsonl\n",
      "📊 Counting total records...\n",
      "📈 Found 1,090,212 total records\n",
      "🎯 Sampling every 5 records for stratified coverage\n",
      "✅ Collected 200000 samples for analysis\n",
      "📊 SAMPLING SUMMARY:\n",
      "   • Total file records: 1,090,212\n",
      "   • Samples collected: 200,000\n",
      "   • Coverage ratio: 18.35%\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "jsonl_file_path = '-ds-logs-network_traffic-flow-default-2025-05-04-000001.jsonl'\n",
    "SAMPLE_SIZE = num_samples_to_analyze  # Analyze 5000 records for comprehensive coverage\n",
    "random.seed(42)  # For reproducible results\n",
    "\n",
    "def stratified_sample_jsonl(file_path, sample_size):\n",
    "    \"\"\"\n",
    "    Perform stratified sampling across the file to get better representation.\n",
    "    \"\"\"\n",
    "    # First pass: count total records\n",
    "    print(\"📊 Counting total records...\")\n",
    "    total_records = 0\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                total_records += 1\n",
    "    \n",
    "    print(f\"📈 Found {total_records:,} total records\")\n",
    "    \n",
    "    # Calculate sampling interval for even distribution\n",
    "    interval = max(1, total_records // sample_size)\n",
    "    print(f\"🎯 Sampling every {interval} records for stratified coverage\")\n",
    "    \n",
    "    # Second pass: collect stratified samples\n",
    "    samples = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line_num, line in enumerate(f, 1):\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            \n",
    "            # Take every nth record plus some random samples\n",
    "            if (line_num % interval == 0) or (random.random() < 0.0001):\n",
    "                try:\n",
    "                    record = json.loads(line)\n",
    "                    samples.append(record)\n",
    "                    \n",
    "                    if len(samples) >= sample_size:\n",
    "                        break\n",
    "                        \n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "    \n",
    "    print(f\"✅ Collected {len(samples)} samples for analysis\")\n",
    "    return samples, total_records\n",
    "\n",
    "# Load samples with logging\n",
    "with capture_output(\"Data Loading and Sampling\", 3):\n",
    "    print(f\"🔄 Loading samples from: {jsonl_file_path}\")\n",
    "    sample_records, total_file_records = stratified_sample_jsonl(jsonl_file_path, SAMPLE_SIZE)\n",
    "    \n",
    "    print(f\"📊 SAMPLING SUMMARY:\")\n",
    "    print(f\"   • Total file records: {total_file_records:,}\")\n",
    "    print(f\"   • Samples collected: {len(sample_records):,}\")\n",
    "    print(f\"   • Coverage ratio: {(len(sample_records)/total_file_records)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Structure Pattern Detection: Discovering Hidden Data Patterns\n",
    "\n",
    "**What this section does**: Processes all sample records to generate structural fingerprints and discovers how many unique patterns exist in the dataset.\n",
    "\n",
    "**Simple explanation**: \n",
    "- Runs each record through the fingerprinting function to identify its structure\n",
    "- Counts how many times each unique pattern appears\n",
    "- Classifies patterns by frequency (OUTLIER, RARE_VARIANT, SECONDARY_SCHEMA, etc.)\n",
    "\n",
    "**Paraphrasing tips**:\n",
    "- \"What patterns exist and how frequent are they?\" → Discovers unique structural patterns and their occurrence rates\n",
    "- \"Is my data consistent or chaotic?\" → Pattern count reveals dataset complexity\n",
    "- \"Which patterns matter most?\" → Classification shows which patterns to focus on\n",
    "\n",
    "**Key insight**: This analysis reveals whether your dataset has consistent structure (few patterns) or high variability (many patterns), which determines your processing strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SECTION 4: STRUCTURE PATTERN DETECTION\n",
      "================================================================================\n",
      "🔍 GENERATING STRUCTURE FINGERPRINTS\n",
      "==================================================\n",
      "📋 Processing samples...\n",
      "   Processed 0 / 200,000 records\n",
      "   Processed 1,000 / 200,000 records\n",
      "   Processed 2,000 / 200,000 records\n",
      "   Processed 3,000 / 200,000 records\n",
      "   Processed 4,000 / 200,000 records\n",
      "   Processed 5,000 / 200,000 records\n",
      "   Processed 6,000 / 200,000 records\n",
      "   Processed 7,000 / 200,000 records\n",
      "   Processed 8,000 / 200,000 records\n",
      "   Processed 9,000 / 200,000 records\n",
      "   Processed 10,000 / 200,000 records\n",
      "   Processed 11,000 / 200,000 records\n",
      "   Processed 12,000 / 200,000 records\n",
      "   Processed 13,000 / 200,000 records\n",
      "   Processed 14,000 / 200,000 records\n",
      "   Processed 15,000 / 200,000 records\n",
      "   Processed 16,000 / 200,000 records\n",
      "   Processed 17,000 / 200,000 records\n",
      "   Processed 18,000 / 200,000 records\n",
      "   Processed 19,000 / 200,000 records\n",
      "   Processed 20,000 / 200,000 records\n",
      "   Processed 21,000 / 200,000 records\n",
      "   Processed 22,000 / 200,000 records\n",
      "   Processed 23,000 / 200,000 records\n",
      "   Processed 24,000 / 200,000 records\n",
      "   Processed 25,000 / 200,000 records\n",
      "   Processed 26,000 / 200,000 records\n",
      "   Processed 27,000 / 200,000 records\n",
      "   Processed 28,000 / 200,000 records\n",
      "   Processed 29,000 / 200,000 records\n",
      "   Processed 30,000 / 200,000 records\n",
      "   Processed 31,000 / 200,000 records\n",
      "   Processed 32,000 / 200,000 records\n",
      "   Processed 33,000 / 200,000 records\n",
      "   Processed 34,000 / 200,000 records\n",
      "   Processed 35,000 / 200,000 records\n",
      "   Processed 36,000 / 200,000 records\n",
      "   Processed 37,000 / 200,000 records\n",
      "   Processed 38,000 / 200,000 records\n",
      "   Processed 39,000 / 200,000 records\n",
      "   Processed 40,000 / 200,000 records\n",
      "   Processed 41,000 / 200,000 records\n",
      "   Processed 42,000 / 200,000 records\n",
      "   Processed 43,000 / 200,000 records\n",
      "   Processed 44,000 / 200,000 records\n",
      "   Processed 45,000 / 200,000 records\n",
      "   Processed 46,000 / 200,000 records\n",
      "   Processed 47,000 / 200,000 records\n",
      "   Processed 48,000 / 200,000 records\n",
      "   Processed 49,000 / 200,000 records\n",
      "   Processed 50,000 / 200,000 records\n",
      "   Processed 51,000 / 200,000 records\n",
      "   Processed 52,000 / 200,000 records\n",
      "   Processed 53,000 / 200,000 records\n",
      "   Processed 54,000 / 200,000 records\n",
      "   Processed 55,000 / 200,000 records\n",
      "   Processed 56,000 / 200,000 records\n",
      "   Processed 57,000 / 200,000 records\n",
      "   Processed 58,000 / 200,000 records\n",
      "   Processed 59,000 / 200,000 records\n",
      "   Processed 60,000 / 200,000 records\n",
      "   Processed 61,000 / 200,000 records\n",
      "   Processed 62,000 / 200,000 records\n",
      "   Processed 63,000 / 200,000 records\n",
      "   Processed 64,000 / 200,000 records\n",
      "   Processed 65,000 / 200,000 records\n",
      "   Processed 66,000 / 200,000 records\n",
      "   Processed 67,000 / 200,000 records\n",
      "   Processed 68,000 / 200,000 records\n",
      "   Processed 69,000 / 200,000 records\n",
      "   Processed 70,000 / 200,000 records\n",
      "   Processed 71,000 / 200,000 records\n",
      "   Processed 72,000 / 200,000 records\n",
      "   Processed 73,000 / 200,000 records\n",
      "   Processed 74,000 / 200,000 records\n",
      "   Processed 75,000 / 200,000 records\n",
      "   Processed 76,000 / 200,000 records\n",
      "   Processed 77,000 / 200,000 records\n",
      "   Processed 78,000 / 200,000 records\n",
      "   Processed 79,000 / 200,000 records\n",
      "   Processed 80,000 / 200,000 records\n",
      "   Processed 81,000 / 200,000 records\n",
      "   Processed 82,000 / 200,000 records\n",
      "   Processed 83,000 / 200,000 records\n",
      "   Processed 84,000 / 200,000 records\n",
      "   Processed 85,000 / 200,000 records\n",
      "   Processed 86,000 / 200,000 records\n",
      "   Processed 87,000 / 200,000 records\n",
      "   Processed 88,000 / 200,000 records\n",
      "   Processed 89,000 / 200,000 records\n",
      "   Processed 90,000 / 200,000 records\n",
      "   Processed 91,000 / 200,000 records\n",
      "   Processed 92,000 / 200,000 records\n",
      "   Processed 93,000 / 200,000 records\n",
      "   Processed 94,000 / 200,000 records\n",
      "   Processed 95,000 / 200,000 records\n",
      "   Processed 96,000 / 200,000 records\n",
      "   Processed 97,000 / 200,000 records\n",
      "   Processed 98,000 / 200,000 records\n",
      "   Processed 99,000 / 200,000 records\n",
      "   Processed 100,000 / 200,000 records\n",
      "   Processed 101,000 / 200,000 records\n",
      "   Processed 102,000 / 200,000 records\n",
      "   Processed 103,000 / 200,000 records\n",
      "   Processed 104,000 / 200,000 records\n",
      "   Processed 105,000 / 200,000 records\n",
      "   Processed 106,000 / 200,000 records\n",
      "   Processed 107,000 / 200,000 records\n",
      "   Processed 108,000 / 200,000 records\n",
      "   Processed 109,000 / 200,000 records\n",
      "   Processed 110,000 / 200,000 records\n",
      "   Processed 111,000 / 200,000 records\n",
      "   Processed 112,000 / 200,000 records\n",
      "   Processed 113,000 / 200,000 records\n",
      "   Processed 114,000 / 200,000 records\n",
      "   Processed 115,000 / 200,000 records\n",
      "   Processed 116,000 / 200,000 records\n",
      "   Processed 117,000 / 200,000 records\n",
      "   Processed 118,000 / 200,000 records\n",
      "   Processed 119,000 / 200,000 records\n",
      "   Processed 120,000 / 200,000 records\n",
      "   Processed 121,000 / 200,000 records\n",
      "   Processed 122,000 / 200,000 records\n",
      "   Processed 123,000 / 200,000 records\n",
      "   Processed 124,000 / 200,000 records\n",
      "   Processed 125,000 / 200,000 records\n",
      "   Processed 126,000 / 200,000 records\n",
      "   Processed 127,000 / 200,000 records\n",
      "   Processed 128,000 / 200,000 records\n",
      "   Processed 129,000 / 200,000 records\n",
      "   Processed 130,000 / 200,000 records\n",
      "   Processed 131,000 / 200,000 records\n",
      "   Processed 132,000 / 200,000 records\n",
      "   Processed 133,000 / 200,000 records\n",
      "   Processed 134,000 / 200,000 records\n",
      "   Processed 135,000 / 200,000 records\n",
      "   Processed 136,000 / 200,000 records\n",
      "   Processed 137,000 / 200,000 records\n",
      "   Processed 138,000 / 200,000 records\n",
      "   Processed 139,000 / 200,000 records\n",
      "   Processed 140,000 / 200,000 records\n",
      "   Processed 141,000 / 200,000 records\n",
      "   Processed 142,000 / 200,000 records\n",
      "   Processed 143,000 / 200,000 records\n",
      "   Processed 144,000 / 200,000 records\n",
      "   Processed 145,000 / 200,000 records\n",
      "   Processed 146,000 / 200,000 records\n",
      "   Processed 147,000 / 200,000 records\n",
      "   Processed 148,000 / 200,000 records\n",
      "   Processed 149,000 / 200,000 records\n",
      "   Processed 150,000 / 200,000 records\n",
      "   Processed 151,000 / 200,000 records\n",
      "   Processed 152,000 / 200,000 records\n",
      "   Processed 153,000 / 200,000 records\n",
      "   Processed 154,000 / 200,000 records\n",
      "   Processed 155,000 / 200,000 records\n",
      "   Processed 156,000 / 200,000 records\n",
      "   Processed 157,000 / 200,000 records\n",
      "   Processed 158,000 / 200,000 records\n",
      "   Processed 159,000 / 200,000 records\n",
      "   Processed 160,000 / 200,000 records\n",
      "   Processed 161,000 / 200,000 records\n",
      "   Processed 162,000 / 200,000 records\n",
      "   Processed 163,000 / 200,000 records\n",
      "   Processed 164,000 / 200,000 records\n",
      "   Processed 165,000 / 200,000 records\n",
      "   Processed 166,000 / 200,000 records\n",
      "   Processed 167,000 / 200,000 records\n",
      "   Processed 168,000 / 200,000 records\n",
      "   Processed 169,000 / 200,000 records\n",
      "   Processed 170,000 / 200,000 records\n",
      "   Processed 171,000 / 200,000 records\n",
      "   Processed 172,000 / 200,000 records\n",
      "   Processed 173,000 / 200,000 records\n",
      "   Processed 174,000 / 200,000 records\n",
      "   Processed 175,000 / 200,000 records\n",
      "   Processed 176,000 / 200,000 records\n",
      "   Processed 177,000 / 200,000 records\n",
      "   Processed 178,000 / 200,000 records\n",
      "   Processed 179,000 / 200,000 records\n",
      "   Processed 180,000 / 200,000 records\n",
      "   Processed 181,000 / 200,000 records\n",
      "   Processed 182,000 / 200,000 records\n",
      "   Processed 183,000 / 200,000 records\n",
      "   Processed 184,000 / 200,000 records\n",
      "   Processed 185,000 / 200,000 records\n",
      "   Processed 186,000 / 200,000 records\n",
      "   Processed 187,000 / 200,000 records\n",
      "   Processed 188,000 / 200,000 records\n",
      "   Processed 189,000 / 200,000 records\n",
      "   Processed 190,000 / 200,000 records\n",
      "   Processed 191,000 / 200,000 records\n",
      "   Processed 192,000 / 200,000 records\n",
      "   Processed 193,000 / 200,000 records\n",
      "   Processed 194,000 / 200,000 records\n",
      "   Processed 195,000 / 200,000 records\n",
      "   Processed 196,000 / 200,000 records\n",
      "   Processed 197,000 / 200,000 records\n",
      "   Processed 198,000 / 200,000 records\n",
      "   Processed 199,000 / 200,000 records\n",
      "✅ Fingerprinting complete!\n",
      "📊 STRUCTURE ANALYSIS RESULTS:\n",
      "   • Unique structure patterns found: 14\n",
      "   • Records analyzed: 200,000\n",
      "   • Most common pattern frequency: 67,260 records\n",
      "\n",
      "🏷️  PATTERN CLASSIFICATION:\n",
      "   • OUTLIER: 9 patterns\n",
      "   • SECONDARY_SCHEMA: 3 patterns\n",
      "   • RARE_VARIANT: 2 patterns\n"
     ]
    }
   ],
   "source": [
    "with capture_output(\"Structure Pattern Detection\", 4):\n",
    "    print(\"🔍 GENERATING STRUCTURE FINGERPRINTS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Generate fingerprints for all samples\n",
    "    structure_fingerprints = {}\n",
    "    structure_examples = {}\n",
    "    structure_counts = Counter()\n",
    "    \n",
    "    print(\"📋 Processing samples...\")\n",
    "    for i, record in enumerate(sample_records):\n",
    "        if i % 1000 == 0:\n",
    "            print(f\"   Processed {i:,} / {len(sample_records):,} records\")\n",
    "        \n",
    "        # Generate both hash and full structure\n",
    "        structure_hash, structure_detail = generate_schema_fingerprint(record, max_depth=5)\n",
    "        \n",
    "        # Store first example of each structure type\n",
    "        if structure_hash not in structure_examples:\n",
    "            structure_examples[structure_hash] = record\n",
    "            structure_fingerprints[structure_hash] = structure_detail\n",
    "        \n",
    "        # Count occurrences\n",
    "        structure_counts[structure_hash] += 1\n",
    "    \n",
    "    print(\"✅ Fingerprinting complete!\")\n",
    "    print(\"📊 STRUCTURE ANALYSIS RESULTS:\")\n",
    "    print(f\"   • Unique structure patterns found: {len(structure_counts)}\")\n",
    "    print(f\"   • Records analyzed: {len(sample_records):,}\")\n",
    "    print(f\"   • Most common pattern frequency: {structure_counts.most_common(1)[0][1]:,} records\")\n",
    "    \n",
    "    # Classify patterns by frequency\n",
    "    classifications = classify_structure_variations(structure_counts, len(sample_records))\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"🏷️  PATTERN CLASSIFICATION:\")\n",
    "    classification_summary = Counter()\n",
    "    for _, info in classifications.items():\n",
    "        classification_summary[info['classification']] += 1\n",
    "    \n",
    "    for class_type, count in classification_summary.most_common():\n",
    "        print(f\"   • {class_type}: {count} patterns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Detailed Pattern Analysis: Understanding Each Structure Type\n",
    "\n",
    "**What this section does**: Examines each discovered pattern in detail, showing frequency, characteristics, and example records to understand what makes each pattern unique.\n",
    "\n",
    "**Simple explanation**: \n",
    "- Sorts patterns from most common to least common\n",
    "- Shows detailed characteristics for each pattern (top-level fields, optional elements)\n",
    "- Identifies differences between patterns (what fields are missing or extra)\n",
    "- Provides compact structure examples for visual inspection\n",
    "\n",
    "**Paraphrasing tips**:\n",
    "- \"What are the characteristics and differences of each pattern?\" → Detailed breakdown of pattern features\n",
    "- \"Which patterns are most important?\" → Frequency-based ranking shows priority\n",
    "- \"How do patterns differ from each other?\" → Comparative analysis reveals structural variations\n",
    "\n",
    "**Key insight**: This deep dive helps you understand not just THAT patterns exist, but WHY they're different and what those differences mean for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SECTION 5: DETAILED PATTERN ANALYSIS\n",
      "================================================================================\n",
      "📋 DETAILED PATTERN ANALYSIS\n",
      "============================================================\n",
      "🔍 PATTERN #1 - SECONDARY_SCHEMA\n",
      "----------------------------------------\n",
      "📊 Frequency: 67,260 records (33.63%)\n",
      "🔑 Structure Hash: 936748b80e5c31b9...\n",
      "📝 Top-level fields (12): @timestamp, agent, data_stream, destination, ecs, elastic_agent, event, host, network, network_traffic, process, source\n",
      "🔧 Notable characteristics: process: dict, source.process: present\n",
      "\n",
      "🔍 PATTERN #2 - SECONDARY_SCHEMA\n",
      "----------------------------------------\n",
      "📊 Frequency: 67,235 records (33.62%)\n",
      "🔑 Structure Hash: 0e116a9cdc0848e1...\n",
      "📝 Top-level fields (11): @timestamp, agent, data_stream, destination, ecs, elastic_agent, event, host, network, network_traffic, source\n",
      "🔄 Differences from Pattern #1:\n",
      "   Missing: 9 structural elements\n",
      "   Extra: 1 structural elements\n",
      "\n",
      "🔍 PATTERN #3 - SECONDARY_SCHEMA\n",
      "----------------------------------------\n",
      "📊 Frequency: 54,406 records (27.20%)\n",
      "🔑 Structure Hash: 955584995c34862b...\n",
      "📝 Top-level fields (12): @timestamp, agent, data_stream, destination, ecs, elastic_agent, event, host, network, network_traffic, process, source\n",
      "🔧 Notable characteristics: process: dict, source.process: present\n",
      "🔄 Differences from Pattern #1:\n",
      "   Extra: 1 structural elements\n",
      "\n",
      "🔍 PATTERN #4 - RARE_VARIANT\n",
      "----------------------------------------\n",
      "📊 Frequency: 5,253 records (2.63%)\n",
      "🔑 Structure Hash: 80a781e5764ccf69...\n",
      "📝 Top-level fields (12): @timestamp, agent, data_stream, destination, ecs, elastic_agent, event, host, network, network_traffic, process, source\n",
      "🔧 Notable characteristics: process: dict\n",
      "🔄 Differences from Pattern #1:\n",
      "   Missing: 1 structural elements\n",
      "   Extra: 1 structural elements\n",
      "\n",
      "🔍 PATTERN #5 - RARE_VARIANT\n",
      "----------------------------------------\n",
      "📊 Frequency: 3,037 records (1.52%)\n",
      "🔑 Structure Hash: 01fa29904dfd16b6...\n",
      "📝 Top-level fields (11): @timestamp, agent, data_stream, destination, ecs, elastic_agent, event, host, network, network_traffic, source\n",
      "🔄 Differences from Pattern #1:\n",
      "   Missing: 9 structural elements\n",
      "   Extra: 2 structural elements\n",
      "\n",
      "🔍 PATTERN #6 - OUTLIER\n",
      "----------------------------------------\n",
      "📊 Frequency: 708 records (0.35%)\n",
      "🔑 Structure Hash: 763734186cb99967...\n",
      "📝 Top-level fields (11): @timestamp, agent, data_stream, destination, ecs, elastic_agent, event, host, network, network_traffic, source\n",
      "🔄 Differences from Pattern #1:\n",
      "   Missing: 12 structural elements\n",
      "   Extra: 1 structural elements\n",
      "\n",
      "🔍 PATTERN #7 - OUTLIER\n",
      "----------------------------------------\n",
      "📊 Frequency: 583 records (0.29%)\n",
      "🔑 Structure Hash: 2cf7d900439025d6...\n",
      "📝 Top-level fields (12): @timestamp, agent, data_stream, destination, ecs, elastic_agent, event, host, network, network_traffic, process, source\n",
      "🔧 Notable characteristics: process: dict, source.process: present\n",
      "🔄 Differences from Pattern #1:\n",
      "   Extra: 1 structural elements\n",
      "\n",
      "🔍 PATTERN #8 - OUTLIER\n",
      "----------------------------------------\n",
      "📊 Frequency: 549 records (0.27%)\n",
      "🔑 Structure Hash: 324bdf95229bbb19...\n",
      "📝 Top-level fields (11): @timestamp, agent, data_stream, destination, ecs, elastic_agent, event, host, network, network_traffic, source\n",
      "🔄 Differences from Pattern #1:\n",
      "   Missing: 12 structural elements\n",
      "   Extra: 2 structural elements\n",
      "\n",
      "🔍 PATTERN #9 - OUTLIER\n",
      "----------------------------------------\n",
      "📊 Frequency: 323 records (0.16%)\n",
      "🔑 Structure Hash: 99d7da5a61d96d9c...\n",
      "📝 Top-level fields (11): @timestamp, agent, data_stream, destination, ecs, elastic_agent, event, host, network, network_traffic, source\n",
      "🔄 Differences from Pattern #1:\n",
      "   Missing: 11 structural elements\n",
      "   Extra: 2 structural elements\n",
      "\n",
      "🔍 PATTERN #10 - OUTLIER\n",
      "----------------------------------------\n",
      "📊 Frequency: 294 records (0.15%)\n",
      "🔑 Structure Hash: c11024de9efb3afa...\n",
      "📝 Top-level fields (12): @timestamp, agent, data_stream, destination, ecs, elastic_agent, event, host, network, network_traffic, process, source\n",
      "🔧 Notable characteristics: process: dict\n",
      "🔄 Differences from Pattern #1:\n",
      "   Missing: 1 structural elements\n",
      "   Extra: 2 structural elements\n",
      "\n",
      "\n",
      "📄 COMPLETE STRUCTURE EXAMPLES\n",
      "============================================================\n",
      "🏗️  PATTERN #1 EXAMPLE RECORD:\n",
      "------------------------------\n",
      "process: {dict with 7 fields}\n",
      "  args: [array with 1 items, first: str]\n",
      "  parent: {dict with 1 fields}\n",
      "    pid: int = 588\n",
      "  start: str = 2025-05-04T11:29:33.250Z\n",
      "  name: str = dns.exe\n",
      "  working_directory: str = \n",
      "  pid: int = 2608\n",
      "  executable: str = C:\\Windows\\System32\\dns.exe\n",
      "agent: {dict with 5 fields}\n",
      "  name: str = diskjockey\n",
      "  id: str = 15638b02-c77f-4aed-9566-1c5c68...\n",
      "  type: str = packetbeat\n",
      "  ephemeral_id: str = bb3f2762-d059-48c3-8da2-f6b7c9...\n",
      "  version: str = 8.18.0\n",
      "destination: {dict with 5 fields}\n",
      "  port: int = 53\n",
      "  bytes: int = 1237\n",
      "  ip: str = 192.112.36.4\n",
      "  mac: str = 08-00-27-26-33-A5\n",
      "  packets: int = 1\n",
      "elastic_agent: {dict with 3 fields}\n",
      "  id: str = 15638b02-c77f-4aed-9566-1c5c68...\n",
      "  version: str = 8.18.0\n",
      "  snapshot: bool = False\n",
      "network_traffic: {dict with 1 fields}\n",
      "  flow: {dict with 2 fields}\n",
      "    final: bool = False\n",
      "    id: str = EQIA////DP////8U//8BAAEIACcmM6...\n",
      "source: {dict with 6 fields}\n",
      "  process: {dict with 7 fields}\n",
      "    args: [array with 1 items, first: str]\n",
      "    name: str = dns.exe\n",
      "    start: str = 2025-05-04T11:29:33.250Z\n",
      "    working_directory: str = \n",
      "    pid: int = 2608\n",
      "    executable: str = C:\\Windows\\System32\\dns.exe\n",
      "    ppid: int = 588\n",
      "  port: int = 65288\n",
      "  bytes: int = 103\n",
      "  ip: str = 10.1.0.4\n",
      "  packets: int = 1\n",
      "  mac: str = 08-00-27-57-05-2B\n",
      "network: {dict with 5 fields}\n",
      "  community_id: str = 1:Xu7A2oEQej6MZ5BloXOrb0Kyqls=\n",
      "  bytes: int = 1340\n",
      "  transport: str = udp\n",
      "  type: str = ipv4\n",
      "  packets: int = 2\n",
      "@timestamp: str = 2025-05-04T11:30:08.613Z\n",
      "ecs: {dict with 1 fields}\n",
      "  version: str = 8.11.0\n",
      "data_stream: {dict with 3 fields}\n",
      "  namespace: str = default\n",
      "  type: str = logs\n",
      "  dataset: str = network_traffic.flow\n",
      "host: {dict with 7 fields}\n",
      "  hostname: str = diskjockey\n",
      "  os: {dict with 7 fields}\n",
      "    build: str = 17763.3650\n",
      "    kernel: str = 10.0.17763.3650 (WinBuild.1601...\n",
      "    name: str = Windows Server 2019 Datacenter...\n",
      "    family: str = windows\n",
      "    type: str = windows\n",
      "    version: str = 10.0\n",
      "    platform: str = windows\n",
      "  ip: [array with 1 items, first: str]\n",
      "  name: str = diskjockey\n",
      "  id: str = acb80d05-87f8-427b-9b79-726fd9...\n",
      "  mac: [array with 1 items, first: str]\n",
      "  architecture: str = x86_64\n",
      "event: {dict with 9 fields}\n",
      "  duration: int = 58173400\n",
      "  agent_id_status: str = verified\n",
      "  ingested: str = 2025-05-04T11:30:11Z\n",
      "  kind: str = event\n",
      "  start: str = 2025-05-04T11:29:58.901Z\n",
      "  action: str = network_flow\n",
      "  end: str = 2025-05-04T11:29:58.958Z\n",
      "  category: [array with 1 items, first: str]\n",
      "  type: [array with 1 items, first: str]\n",
      "\n",
      "🏗️  PATTERN #2 EXAMPLE RECORD:\n",
      "------------------------------\n",
      "agent: {dict with 5 fields}\n",
      "  name: str = diskjockey\n",
      "  id: str = 15638b02-c77f-4aed-9566-1c5c68...\n",
      "  type: str = packetbeat\n",
      "  ephemeral_id: str = bb3f2762-d059-48c3-8da2-f6b7c9...\n",
      "  version: str = 8.18.0\n",
      "destination: {dict with 5 fields}\n",
      "  port: int = 9200\n",
      "  bytes: int = 10872\n",
      "  ip: str = 10.2.0.20\n",
      "  mac: str = 08-00-27-26-33-A5\n",
      "  packets: int = 22\n",
      "elastic_agent: {dict with 3 fields}\n",
      "  id: str = 15638b02-c77f-4aed-9566-1c5c68...\n",
      "  version: str = 8.18.0\n",
      "  snapshot: bool = False\n",
      "network_traffic: {dict with 1 fields}\n",
      "  flow: {dict with 2 fields}\n",
      "    final: bool = False\n",
      "    id: str = EQQA////DP//////FP8BAAEIACcmM6...\n",
      "source: {dict with 5 fields}\n",
      "  port: int = 49782\n",
      "  bytes: int = 7130\n",
      "  ip: str = 10.1.0.4\n",
      "  mac: str = 08-00-27-57-05-2B\n",
      "  packets: int = 25\n",
      "network: {dict with 5 fields}\n",
      "  community_id: str = 1:JYKOU3xntvVYio6JAicdfxMOgGo=\n",
      "  bytes: int = 18002\n",
      "  transport: str = tcp\n",
      "  type: str = ipv4\n",
      "  packets: int = 47\n",
      "@timestamp: str = 2025-05-04T11:30:08.613Z\n",
      "ecs: {dict with 1 fields}\n",
      "  version: str = 8.11.0\n",
      "data_stream: {dict with 3 fields}\n",
      "  namespace: str = default\n",
      "  type: str = logs\n",
      "  dataset: str = network_traffic.flow\n",
      "host: {dict with 7 fields}\n",
      "  hostname: str = diskjockey\n",
      "  os: {dict with 7 fields}\n",
      "    build: str = 17763.3650\n",
      "    kernel: str = 10.0.17763.3650 (WinBuild.1601...\n",
      "    name: str = Windows Server 2019 Datacenter...\n",
      "    family: str = windows\n",
      "    type: str = windows\n",
      "    version: str = 10.0\n",
      "    platform: str = windows\n",
      "  ip: [array with 1 items, first: str]\n",
      "  name: str = diskjockey\n",
      "  id: str = acb80d05-87f8-427b-9b79-726fd9...\n",
      "  mac: [array with 1 items, first: str]\n",
      "  architecture: str = x86_64\n",
      "event: {dict with 9 fields}\n",
      "  duration: int = 15374108700\n",
      "  agent_id_status: str = verified\n",
      "  ingested: str = 2025-05-04T11:30:11Z\n",
      "  kind: str = event\n",
      "  start: str = 2025-05-04T11:29:52.525Z\n",
      "  action: str = network_flow\n",
      "  end: str = 2025-05-04T11:30:06.755Z\n",
      "  category: [array with 1 items, first: str]\n",
      "  type: [array with 1 items, first: str]\n",
      "\n",
      "🏗️  PATTERN #3 EXAMPLE RECORD:\n",
      "------------------------------\n",
      "agent: {dict with 5 fields}\n",
      "  name: str = endofroad\n",
      "  id: str = e2299459-1b0c-429c-a6f0-5b3f18...\n",
      "  type: str = packetbeat\n",
      "  ephemeral_id: str = 7fd5e56a-db88-4b5b-b1d8-3ee0fc...\n",
      "  version: str = 8.18.0\n",
      "process: {dict with 7 fields}\n",
      "  args: [empty array]\n",
      "  parent: {dict with 1 fields}\n",
      "    pid: int = 0\n",
      "  start: str = 2025-05-04T11:30:29.122Z\n",
      "  name: str = .\n",
      "  working_directory: str = \n",
      "  pid: int = 4\n",
      "  executable: str = \n",
      "destination: {dict with 5 fields}\n",
      "  port: int = 445\n",
      "  bytes: int = 252\n",
      "  ip: str = 10.1.0.4\n",
      "  mac: str = 08-00-27-57-05-2B\n",
      "  packets: int = 2\n",
      "elastic_agent: {dict with 3 fields}\n",
      "  id: str = e2299459-1b0c-429c-a6f0-5b3f18...\n",
      "  version: str = 8.18.0\n",
      "  snapshot: bool = False\n",
      "network_traffic: {dict with 1 fields}\n",
      "  flow: {dict with 2 fields}\n",
      "    final: bool = False\n",
      "    id: str = EQQA////DP//////FP8BAAEIACdXBS...\n",
      "source: {dict with 6 fields}\n",
      "  process: {dict with 7 fields}\n",
      "    args: [empty array]\n",
      "    name: str = .\n",
      "    start: str = 2025-05-04T11:30:29.122Z\n",
      "    pid: int = 4\n",
      "    working_directory: str = \n",
      "    executable: str = \n",
      "    ppid: int = 0\n",
      "  port: int = 49691\n",
      "  bytes: int = 306\n",
      "  ip: str = 10.1.0.7\n",
      "  packets: int = 3\n",
      "  mac: str = 08-00-27-83-7A-4E\n",
      "network: {dict with 5 fields}\n",
      "  community_id: str = 1:PvxdIQguO02SNuZ5eMPNp+2+k2M=\n",
      "  bytes: int = 558\n",
      "  transport: str = tcp\n",
      "  type: str = ipv4\n",
      "  packets: int = 5\n",
      "@timestamp: str = 2025-05-04T11:31:03.445Z\n",
      "ecs: {dict with 1 fields}\n",
      "  version: str = 8.11.0\n",
      "data_stream: {dict with 3 fields}\n",
      "  namespace: str = default\n",
      "  type: str = logs\n",
      "  dataset: str = network_traffic.flow\n",
      "host: {dict with 7 fields}\n",
      "  hostname: str = endofroad\n",
      "  os: {dict with 7 fields}\n",
      "    build: str = 17763.3650\n",
      "    kernel: str = 10.0.17763.3650 (WinBuild.1601...\n",
      "    name: str = Windows Server 2019 Datacenter...\n",
      "    type: str = windows\n",
      "    family: str = windows\n",
      "    version: str = 10.0\n",
      "    platform: str = windows\n",
      "  ip: [array with 2 items, first: str]\n",
      "  name: str = endofroad\n",
      "  id: str = e92b35f5-40a7-4d64-8860-e6b2ec...\n",
      "  mac: [array with 1 items, first: str]\n",
      "  architecture: str = x86_64\n",
      "event: {dict with 9 fields}\n",
      "  duration: int = 0\n",
      "  agent_id_status: str = verified\n",
      "  ingested: str = 2025-05-04T11:31:06Z\n",
      "  kind: str = event\n",
      "  start: str = 2025-05-04T11:31:01.962Z\n",
      "  action: str = network_flow\n",
      "  end: str = 2025-05-04T11:31:01.962Z\n",
      "  category: [array with 1 items, first: str]\n",
      "  type: [array with 1 items, first: str]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with capture_output(\"Detailed Pattern Analysis\", 5):\n",
    "    print(\"📋 DETAILED PATTERN ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Sort patterns by frequency for analysis\n",
    "    sorted_patterns = structure_counts.most_common()\n",
    "    \n",
    "    for i, (structure_hash, count) in enumerate(sorted_patterns[:10], 1):\n",
    "        classification_info = classifications[structure_hash]\n",
    "        \n",
    "        print(f\"🔍 PATTERN #{i} - {classification_info['classification']}\")\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"📊 Frequency: {count:,} records ({classification_info['percentage']:.2f}%)\")\n",
    "        print(f\"🔑 Structure Hash: {structure_hash[:16]}...\")\n",
    "        \n",
    "        # Get example record\n",
    "        example_record = structure_examples[structure_hash]\n",
    "        \n",
    "        # Analyze top-level structure\n",
    "        top_level_fields = list(example_record.keys())\n",
    "        print(f\"📝 Top-level fields ({len(top_level_fields)}): {', '.join(sorted(top_level_fields))}\")\n",
    "        \n",
    "        # Check for optional fields\n",
    "        optional_indicators = []\n",
    "        for field in top_level_fields:\n",
    "            if field == 'process':\n",
    "                optional_indicators.append(f\"process: {type(example_record[field]).__name__}\")\n",
    "            elif isinstance(example_record[field], dict) and not example_record[field]:\n",
    "                optional_indicators.append(f\"{field}: empty_dict\")\n",
    "            elif field == 'source' and isinstance(example_record[field], dict) and 'process' in example_record[field]:\n",
    "                optional_indicators.append(\"source.process: present\")\n",
    "        \n",
    "        if optional_indicators:\n",
    "            print(f\"🔧 Notable characteristics: {', '.join(optional_indicators)}\")\n",
    "        \n",
    "        # Show structure differences for secondary patterns\n",
    "        if i > 1:\n",
    "            primary_structure = structure_fingerprints[sorted_patterns[0][0]]\n",
    "            current_structure = structure_fingerprints[structure_hash]\n",
    "            \n",
    "            # Simple difference detection (can be enhanced)\n",
    "            primary_fields = set(str(primary_structure).split())\n",
    "            current_fields = set(str(current_structure).split())\n",
    "            \n",
    "            missing_fields = primary_fields - current_fields\n",
    "            extra_fields = current_fields - primary_fields\n",
    "            \n",
    "            if missing_fields or extra_fields:\n",
    "                print(\"🔄 Differences from Pattern #1:\")\n",
    "                if missing_fields:\n",
    "                    print(f\"   Missing: {len(missing_fields)} structural elements\")\n",
    "                if extra_fields:\n",
    "                    print(f\"   Extra: {len(extra_fields)} structural elements\")\n",
    "        \n",
    "        print(\"\")  # Add spacing between patterns\n",
    "    \n",
    "    # Show complete structure for top 3 patterns (compact view)\n",
    "    print(\"\")\n",
    "    print(\"📄 COMPLETE STRUCTURE EXAMPLES\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    def show_compact_structure(obj, indent=0, max_depth=2):\n",
    "        \"\"\"Show a compact view of structure for logging\"\"\"\n",
    "        prefix = \"  \" * indent\n",
    "        if isinstance(obj, dict):\n",
    "            for key, value in obj.items():\n",
    "                if isinstance(value, dict):\n",
    "                    print(f\"{prefix}{key}: {{dict with {len(value)} fields}}\")\n",
    "                    if indent < max_depth:  # Limit depth for readability\n",
    "                        show_compact_structure(value, indent + 1, max_depth)\n",
    "                elif isinstance(value, list):\n",
    "                    if value:\n",
    "                        print(f\"{prefix}{key}: [array with {len(value)} items, first: {type(value[0]).__name__}]\")\n",
    "                    else:\n",
    "                        print(f\"{prefix}{key}: [empty array]\")\n",
    "                else:\n",
    "                    value_preview = str(value)[:30] + \"...\" if len(str(value)) > 30 else str(value)\n",
    "                    print(f\"{prefix}{key}: {type(value).__name__} = {value_preview}\")\n",
    "        elif isinstance(obj, list):\n",
    "            print(f\"{prefix}[{len(obj)} items]\")\n",
    "    \n",
    "    for i, (structure_hash, count) in enumerate(sorted_patterns[:3], 1):\n",
    "        print(f\"🏗️  PATTERN #{i} EXAMPLE RECORD:\")\n",
    "        print(\"-\" * 30)\n",
    "        example = structure_examples[structure_hash]\n",
    "        show_compact_structure(example)\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Field Co-occurrence Analysis: Discovering Field Relationships\n",
    "\n",
    "**What this section does**: Analyzes which fields appear together in records and identifies conditional field relationships to understand data dependencies.\n",
    "\n",
    "**Simple explanation**: \n",
    "- Extracts all possible field paths from nested JSON structures\n",
    "- Counts how often each field appears across all records\n",
    "- Identifies which fields are always present vs. optional/conditional\n",
    "- Discovers field combinations that appear together\n",
    "\n",
    "**Paraphrasing tips**:\n",
    "- \"Which fields always appear together?\" → Field co-occurrence reveals data relationships\n",
    "- \"What fields are optional vs. required?\" → Presence analysis shows field reliability\n",
    "- \"Are there conditional dependencies?\" → Some fields only appear when others are present\n",
    "\n",
    "**Key insight**: Understanding field relationships is crucial for robust data processing - you need to know which fields you can count on and which require null handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SECTION 6: FIELD CO-OCCURRENCE ANALYSIS\n",
      "================================================================================\n",
      "🔗 FIELD CO-OCCURRENCE ANALYSIS\n",
      "==================================================\n",
      "📊 Field presence analysis:\n",
      "   • Total unique field paths: 89\n",
      "   • Unique field combinations: 11\n",
      "\n",
      "📈 Most common fields (top 20):\n",
      "   agent                                    200,000 (100.0%)\n",
      "   agent.name                               200,000 (100.0%)\n",
      "   agent.id                                 200,000 (100.0%)\n",
      "   agent.type                               200,000 (100.0%)\n",
      "   agent.ephemeral_id                       200,000 (100.0%)\n",
      "   agent.version                            200,000 (100.0%)\n",
      "   destination                              200,000 (100.0%)\n",
      "   destination.mac                          200,000 (100.0%)\n",
      "   elastic_agent                            200,000 (100.0%)\n",
      "   elastic_agent.id                         200,000 (100.0%)\n",
      "   elastic_agent.version                    200,000 (100.0%)\n",
      "   elastic_agent.snapshot                   200,000 (100.0%)\n",
      "   network_traffic                          200,000 (100.0%)\n",
      "   network_traffic.flow                     200,000 (100.0%)\n",
      "   network_traffic.flow.final               200,000 (100.0%)\n",
      "   network_traffic.flow.id                  200,000 (100.0%)\n",
      "   source                                   200,000 (100.0%)\n",
      "   source.bytes                             200,000 (100.0%)\n",
      "   source.packets                           200,000 (100.0%)\n",
      "   source.mac                               200,000 (100.0%)\n",
      "\n",
      "🔧 Conditional/Optional fields (present in <95% of records):\n",
      "   destination.process                       5,575 (  2.8%)\n",
      "   destination.process.args                  5,575 (  2.8%)\n",
      "   destination.process.start                 5,575 (  2.8%)\n",
      "   destination.process.name                  5,575 (  2.8%)\n",
      "   destination.process.working_directory     5,575 (  2.8%)\n",
      "   destination.process.pid                   5,575 (  2.8%)\n",
      "   destination.process.executable            5,575 (  2.8%)\n",
      "   destination.process.ppid                  5,575 (  2.8%)\n",
      "   source.process                           122,460 ( 61.2%)\n",
      "   source.process.args                      122,460 ( 61.2%)\n",
      "   source.process.name                      122,460 ( 61.2%)\n",
      "   source.process.start                     122,460 ( 61.2%)\n",
      "   source.process.working_directory         122,460 ( 61.2%)\n",
      "   source.process.pid                       122,460 ( 61.2%)\n",
      "   source.process.executable                122,460 ( 61.2%)\n",
      "   source.process.ppid                      122,460 ( 61.2%)\n",
      "   process                                  128,035 ( 64.0%)\n",
      "   process.args                             128,035 ( 64.0%)\n",
      "   process.parent                           128,035 ( 64.0%)\n",
      "   process.parent.pid                       128,035 ( 64.0%)\n",
      "   process.start                            128,035 ( 64.0%)\n",
      "   process.name                             128,035 ( 64.0%)\n",
      "   process.working_directory                128,035 ( 64.0%)\n",
      "   process.pid                              128,035 ( 64.0%)\n",
      "   process.executable                       128,035 ( 64.0%)\n",
      "\n",
      "🎯 Most common field combinations (top 10):\n",
      "   Combination with 81 fields: 121,666 records ( 60.8%)\n",
      "   Combination with 64 fields: 67,235 records ( 33.6%)\n",
      "   Combination with 81 fields:  5,547 records (  2.8%)\n",
      "   Combination with 62 fields:  3,037 records (  1.5%)\n",
      "   Combination with 79 fields:    794 records (  0.4%)\n",
      "   Combination with 57 fields:    708 records (  0.4%)\n",
      "   Combination with 55 fields:    549 records (  0.3%)\n",
      "   Combination with 60 fields:    323 records (  0.2%)\n",
      "   Combination with 58 fields:     83 records (  0.0%)\n",
      "   Combination with 62 fields:     30 records (  0.0%)\n",
      "\n",
      "🔍 Key insights:\n",
      "   • 'process' field appears in 64.0% of records\n",
      "   • 'source.process' appears in 61.2% of records\n",
      "   • Process fields show correlation - investigating structural relationship\n"
     ]
    }
   ],
   "source": [
    "with capture_output(\"Field Co-occurrence Analysis\", 6):\n",
    "    print(\"🔗 FIELD CO-OCCURRENCE ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Analyze field presence patterns\n",
    "    field_combinations, field_counts = analyze_field_presence_patterns(sample_records)\n",
    "    \n",
    "    print(\"📊 Field presence analysis:\")\n",
    "    print(f\"   • Total unique field paths: {len(field_counts)}\")\n",
    "    print(f\"   • Unique field combinations: {len(field_combinations)}\")\n",
    "    \n",
    "    # Show most common fields\n",
    "    print(\"\")\n",
    "    print(\"📈 Most common fields (top 20):\")\n",
    "    total_samples = len(sample_records)\n",
    "    for field, count in field_counts.most_common(20):\n",
    "        percentage = (count / total_samples) * 100\n",
    "        print(f\"   {field:<40} {count:>6,} ({percentage:>5.1f}%)\")\n",
    "    \n",
    "    # Identify conditional fields\n",
    "    print(\"\")\n",
    "    print(\"🔧 Conditional/Optional fields (present in <95% of records):\")\n",
    "    conditional_fields = []\n",
    "    for field, count in field_counts.items():\n",
    "        percentage = (count / total_samples) * 100\n",
    "        if percentage < 95:\n",
    "            conditional_fields.append((field, count, percentage))\n",
    "    \n",
    "    # Sort conditional fields by rarity\n",
    "    conditional_fields.sort(key=lambda x: x[2])\n",
    "    \n",
    "    for field, count, percentage in conditional_fields:\n",
    "        print(f\"   {field:<40} {count:>6,} ({percentage:>5.1f}%)\")\n",
    "    \n",
    "    # Analyze field combinations for insights\n",
    "    print(\"\")\n",
    "    print(\"🎯 Most common field combinations (top 10):\")\n",
    "    for combination, count in field_combinations.most_common(10):\n",
    "        percentage = (count / total_samples) * 100\n",
    "        combination_size = len(combination)\n",
    "        print(f\"   Combination with {combination_size:2d} fields: {count:>6,} records ({percentage:>5.1f}%)\")\n",
    "    \n",
    "    # Process field relationships\n",
    "    print(\"\")\n",
    "    print(\"🔍 Key insights:\")\n",
    "    \n",
    "    # Check process field correlation\n",
    "    process_present = field_counts.get('process', 0)\n",
    "    process_percentage = (process_present / total_samples) * 100\n",
    "    print(f\"   • 'process' field appears in {process_percentage:.1f}% of records\")\n",
    "    \n",
    "    # Check source.process correlation  \n",
    "    source_process_present = field_counts.get('source.process', 0)\n",
    "    source_process_percentage = (source_process_present / total_samples) * 100\n",
    "    print(f\"   • 'source.process' appears in {source_process_percentage:.1f}% of records\")\n",
    "    \n",
    "    # Calculate conditional probability\n",
    "    if process_present > 0 and source_process_present > 0:\n",
    "        print(\"   • Process fields show correlation - investigating structural relationship\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Schema Consistency Report: Overall Data Quality Assessment\n",
    "\n",
    "**What this section does**: Generates a comprehensive report that evaluates overall data consistency and provides specific recommendations for processing strategies.\n",
    "\n",
    "**Simple explanation**: \n",
    "- Calculates consistency metrics (how uniform is the data structure?)\n",
    "- Determines coverage percentages (how much data do top patterns represent?)\n",
    "- Assigns an overall consistency rating (HIGHLY CONSISTENT → HIGHLY VARIABLE)\n",
    "- Provides specific processing recommendations based on findings\n",
    "\n",
    "**Paraphrasing tips**:\n",
    "- \"How consistent is my data overall?\" → Consistency metrics and coverage analysis\n",
    "- \"Should I use one pipeline or multiple?\" → Processing strategy recommendations\n",
    "- \"What's the best approach for this dataset?\" → Executive summary with actionable insights\n",
    "\n",
    "**Key insight**: This section translates technical analysis into business decisions - telling you exactly how to approach processing this specific dataset based on its structural characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SECTION 7: SCHEMA CONSISTENCY REPORT\n",
      "================================================================================\n",
      "📊 SCHEMA CONSISTENCY REPORT\n",
      "============================================================\n",
      "🔍 CONSISTENCY METRICS:\n",
      "   • Total unique structure patterns: 14\n",
      "   • Primary/Secondary patterns: 3\n",
      "   • Structure diversity ratio: 0.007%\n",
      "\n",
      "📈 COVERAGE ANALYSIS:\n",
      "   • Top 3 patterns cover: 94.5% of data\n",
      "   • Top 5 patterns cover: 98.6% of data\n",
      "   • Top 10 patterns cover: 99.8% of data\n",
      "\n",
      "🟡 OVERALL ASSESSMENT: MODERATELY CONSISTENT\n",
      "\n",
      "💡 PROCESSING RECOMMENDATIONS:\n",
      "   🟡 Good consistency - recommend dual processing pipelines\n",
      "   🟡 Primary pipeline for top 3 patterns\n",
      "   🟡 Secondary pipeline for patterns 4-5\n",
      "   🟡 Error handling for remaining variants\n",
      "\n",
      "🛠️  FIELD HANDLING STRATEGIES:\n",
      "   • Always present fields (64): Standard extraction\n",
      "   • Conditional fields (17): Null handling required\n",
      "   • Rare fields (8): Consider exclusion or special handling\n",
      "\n",
      "🎯 EXECUTIVE SUMMARY:\n",
      "   Dataset shows moderately consistent structure with 14 unique patterns.\n",
      "   Top 3 patterns handle 94.5% of records.\n",
      "   Recommended approach: Multi-pattern pipeline processing.\n",
      "\n",
      "💾 Saving detailed analysis results...\n",
      "✅ Results saved to: outputs/3b-network-flows/3b-network-flows_structure_results_20250629_065016.json\n",
      "📁 Output directory: outputs/3b-network-flows\n",
      "🎉 Network-flows structure consistency analysis complete!\n"
     ]
    }
   ],
   "source": [
    "with capture_output(\"Schema Consistency Report\", 7):\n",
    "    print(\"📊 SCHEMA CONSISTENCY REPORT\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Calculate consistency metrics\n",
    "    total_patterns = len(structure_counts)\n",
    "    total_samples = len(sample_records)\n",
    "    primary_patterns = sum(1 for _, info in classifications.items() \n",
    "                          if info['classification'] in ['PRIMARY_SCHEMA', 'SECONDARY_SCHEMA'])\n",
    "    \n",
    "    # Calculate coverage of top patterns\n",
    "    sorted_patterns = structure_counts.most_common()\n",
    "    top_3_coverage = sum(count for _, count in sorted_patterns[:3])\n",
    "    top_5_coverage = sum(count for _, count in sorted_patterns[:5])\n",
    "    top_10_coverage = sum(count for _, count in sorted_patterns[:10])\n",
    "    \n",
    "    print(\"🔍 CONSISTENCY METRICS:\")\n",
    "    print(f\"   • Total unique structure patterns: {total_patterns}\")\n",
    "    print(f\"   • Primary/Secondary patterns: {primary_patterns}\")\n",
    "    print(f\"   • Structure diversity ratio: {(total_patterns/total_samples)*100:.3f}%\")\n",
    "    print(\"\")\n",
    "    print(\"📈 COVERAGE ANALYSIS:\")\n",
    "    print(f\"   • Top 3 patterns cover: {(top_3_coverage/total_samples)*100:.1f}% of data\")\n",
    "    print(f\"   • Top 5 patterns cover: {(top_5_coverage/total_samples)*100:.1f}% of data\")\n",
    "    print(f\"   • Top 10 patterns cover: {(top_10_coverage/total_samples)*100:.1f}% of data\")\n",
    "    \n",
    "    # Determine overall consistency level\n",
    "    if (top_3_coverage/total_samples) > 0.95:\n",
    "        consistency_level = \"HIGHLY CONSISTENT\"\n",
    "        consistency_color = \"🟢\"\n",
    "    elif (top_5_coverage/total_samples) > 0.90:\n",
    "        consistency_level = \"MODERATELY CONSISTENT\"\n",
    "        consistency_color = \"🟡\"\n",
    "    elif (top_10_coverage/total_samples) > 0.80:\n",
    "        consistency_level = \"SOMEWHAT VARIABLE\"\n",
    "        consistency_color = \"🟠\"\n",
    "    else:\n",
    "        consistency_level = \"HIGHLY VARIABLE\"\n",
    "        consistency_color = \"🔴\"\n",
    "    \n",
    "    print(\"\")\n",
    "    print(f\"{consistency_color} OVERALL ASSESSMENT: {consistency_level}\")\n",
    "    \n",
    "    # Generate recommendations based on findings\n",
    "    print(\"\")\n",
    "    print(\"💡 PROCESSING RECOMMENDATIONS:\")\n",
    "    \n",
    "    if (top_3_coverage/total_samples) > 0.95:\n",
    "        print(\"   ✅ Excellent consistency - can use single processing pipeline\")\n",
    "        print(\"   ✅ Focus on top 3 patterns for 95%+ coverage\")\n",
    "        print(\"   ✅ Simple fallback handling for rare variants\")\n",
    "    elif (top_5_coverage/total_samples) > 0.90:\n",
    "        print(\"   🟡 Good consistency - recommend dual processing pipelines\")\n",
    "        print(\"   🟡 Primary pipeline for top 3 patterns\")\n",
    "        print(\"   🟡 Secondary pipeline for patterns 4-5\")\n",
    "        print(\"   🟡 Error handling for remaining variants\")\n",
    "    else:\n",
    "        print(\"   🟠 Variable structure - requires flexible processing\")\n",
    "        print(\"   🟠 Implement schema-agnostic field extraction\")\n",
    "        print(\"   🟠 Use pattern matching for different record types\")\n",
    "        print(\"   🟠 Extensive error handling and validation needed\")\n",
    "    \n",
    "    # Specific field handling recommendations\n",
    "    print(\"\")\n",
    "    print(\"🛠️  FIELD HANDLING STRATEGIES:\")\n",
    "    \n",
    "    # Always present fields\n",
    "    always_present_fields = [field for field, count in field_counts.items() \n",
    "                            if (count / total_samples) >= 0.95]\n",
    "    print(f\"   • Always present fields ({len(always_present_fields)}): Standard extraction\")\n",
    "    \n",
    "    # Conditional fields  \n",
    "    conditional_field_count = len([field for field, count in field_counts.items() \n",
    "                                  if 0.05 < (count / total_samples) < 0.95])\n",
    "    print(f\"   • Conditional fields ({conditional_field_count}): Null handling required\")\n",
    "    \n",
    "    # Rare fields\n",
    "    rare_field_count = len([field for field, count in field_counts.items() \n",
    "                           if (count / total_samples) <= 0.05])\n",
    "    print(f\"   • Rare fields ({rare_field_count}): Consider exclusion or special handling\")\n",
    "    \n",
    "    # Generate final summary\n",
    "    print(\"\")\n",
    "    print(\"🎯 EXECUTIVE SUMMARY:\")\n",
    "    print(f\"   Dataset shows {consistency_level.lower()} structure with {total_patterns} unique patterns.\")\n",
    "    print(f\"   Top {min(3, total_patterns)} patterns handle {(top_3_coverage/total_samples)*100:.1f}% of records.\")\n",
    "    print(f\"   Recommended approach: {'Single pipeline' if total_patterns <= 3 else 'Multi-pattern pipeline'} processing.\")\n",
    "    \n",
    "    # Save detailed results for reference\n",
    "    print(\"\")\n",
    "    print(\"💾 Saving detailed analysis results...\")\n",
    "    analysis_results = {\n",
    "        'analysis_metadata': {\n",
    "            'analysis_type': ANALYSIS_TYPE,\n",
    "            'timestamp': timestamp,\n",
    "            'total_file_records': total_file_records,\n",
    "            'target_file': jsonl_file_path\n",
    "        },\n",
    "        'consistency_metrics': {\n",
    "            'total_patterns': total_patterns,\n",
    "            'total_samples': total_samples,\n",
    "            'consistency_level': consistency_level,\n",
    "            'diversity_ratio': (total_patterns/total_samples)*100\n",
    "        },\n",
    "        'coverage_analysis': {\n",
    "            'top_3_coverage_pct': (top_3_coverage/total_samples)*100,\n",
    "            'top_5_coverage_pct': (top_5_coverage/total_samples)*100,\n",
    "            'top_10_coverage_pct': (top_10_coverage/total_samples)*100\n",
    "        },\n",
    "        'top_patterns': [\n",
    "            {\n",
    "                'pattern_hash': hash_val,\n",
    "                'count': count,\n",
    "                'percentage': classifications[hash_val]['percentage'],\n",
    "                'classification': classifications[hash_val]['classification']\n",
    "            } for hash_val, count in sorted_patterns[:10]\n",
    "        ],\n",
    "        'field_statistics': dict(list(field_counts.most_common(50))),\n",
    "        'processing_recommendations': {\n",
    "            'pipeline_strategy': 'Single pipeline' if total_patterns <= 3 else 'Multi-pattern pipeline',\n",
    "            'always_present_field_count': len(always_present_fields),\n",
    "            'conditional_field_count': conditional_field_count,\n",
    "            'rare_field_count': rare_field_count\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save to organized output directory\n",
    "    with open(results_filename, 'w') as f:\n",
    "        json.dump(analysis_results, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"✅ Results saved to: {results_filename}\")\n",
    "    print(f\"📁 Output directory: {analysis_outputs_dir}\")\n",
    "    print(\"🎉 Network-flows structure consistency analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Pattern-Specific Processing Strategies: Implementation Roadmap\n",
    "\n",
    "**What this section does**: Generates concrete implementation strategies for each major pattern, providing a detailed roadmap for building data processing pipelines.\n",
    "\n",
    "**Simple explanation**: \n",
    "- Creates specific extraction strategies for each pattern type\n",
    "- Identifies core fields vs. optional fields for each pattern\n",
    "- Assigns processing priorities (HIGH, MEDIUM, LOW, MINIMAL)\n",
    "- Provides step-by-step implementation roadmap\n",
    "\n",
    "**Paraphrasing tips**:\n",
    "- \"How should I process each pattern type?\" → Pattern-specific extraction strategies\n",
    "- \"What fields are important for each pattern?\" → Core vs. optional field identification\n",
    "- \"What order should I implement support?\" → Priority-based implementation roadmap\n",
    "\n",
    "**Key insight**: This section transforms analysis insights into actionable engineering tasks - giving you a concrete plan for building robust data processing pipelines that handle all discovered patterns appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🛠️  PATTERN-SPECIFIC PROCESSING STRATEGIES\n",
      "============================================================\n",
      "\n",
      "📋 PATTERN #1 PROCESSING STRATEGY\n",
      "----------------------------------------\n",
      "📊 Coverage: 67,260 records (33.63%)\n",
      "🏷️  Classification: SECONDARY_SCHEMA\n",
      "🔧 Recommended extraction strategy:\n",
      "   • Core fields (6): @timestamp, agent, destination, source, network, event\n",
      "   • Optional fields: process, source.process (null handling required)\n",
      "   • Nested extractions: host.os.*, network_traffic.flow.*\n",
      "⭐ Processing priority: MEDIUM - Secondary processing pipeline\n",
      "\n",
      "📋 PATTERN #2 PROCESSING STRATEGY\n",
      "----------------------------------------\n",
      "📊 Coverage: 67,235 records (33.62%)\n",
      "🏷️  Classification: SECONDARY_SCHEMA\n",
      "🔧 Recommended extraction strategy:\n",
      "   • Core fields (6): @timestamp, agent, destination, source, network, event\n",
      "   • Nested extractions: host.os.*, network_traffic.flow.*\n",
      "⭐ Processing priority: MEDIUM - Secondary processing pipeline\n",
      "\n",
      "📋 PATTERN #3 PROCESSING STRATEGY\n",
      "----------------------------------------\n",
      "📊 Coverage: 54,406 records (27.20%)\n",
      "🏷️  Classification: SECONDARY_SCHEMA\n",
      "🔧 Recommended extraction strategy:\n",
      "   • Core fields (6): @timestamp, agent, destination, source, network, event\n",
      "   • Optional fields: process, source.process (null handling required)\n",
      "   • Nested extractions: host.os.*, network_traffic.flow.*\n",
      "⭐ Processing priority: MEDIUM - Secondary processing pipeline\n",
      "\n",
      "📋 PATTERN #4 PROCESSING STRATEGY\n",
      "----------------------------------------\n",
      "📊 Coverage: 5,253 records (2.63%)\n",
      "🏷️  Classification: RARE_VARIANT\n",
      "🔧 Recommended extraction strategy:\n",
      "   • Core fields (6): @timestamp, agent, destination, source, network, event\n",
      "   • Optional fields: process (null handling required)\n",
      "   • Nested extractions: host.os.*, network_traffic.flow.*\n",
      "⭐ Processing priority: MINIMAL - Error handling only\n",
      "\n",
      "📋 PATTERN #5 PROCESSING STRATEGY\n",
      "----------------------------------------\n",
      "📊 Coverage: 3,037 records (1.52%)\n",
      "🏷️  Classification: RARE_VARIANT\n",
      "🔧 Recommended extraction strategy:\n",
      "   • Core fields (6): @timestamp, agent, destination, source, network, event\n",
      "   • Nested extractions: host.os.*, network_traffic.flow.*\n",
      "⭐ Processing priority: MINIMAL - Error handling only\n",
      "\n",
      "🚀 IMPLEMENTATION ROADMAP:\n",
      "   1. Implement primary pipeline for Pattern #1 (67,260 records)\n",
      "   2. Add secondary handling for Pattern #2 (67,235 records)\n",
      "   3. Consider Pattern #3 support (54,406 records)\n",
      "   4. Implement fallback processing for remaining 11 patterns\n",
      "   5. Add comprehensive error logging and schema validation\n",
      "\n",
      "✅ Analysis complete! Use these insights to design your processing pipeline.\n"
     ]
    }
   ],
   "source": [
    "print(\"🛠️  PATTERN-SPECIFIC PROCESSING STRATEGIES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Generate processing strategies for top patterns\n",
    "for i, (structure_hash, count) in enumerate(sorted_patterns[:5], 1):\n",
    "    classification_info = classifications[structure_hash]\n",
    "    percentage = classification_info['percentage']\n",
    "    \n",
    "    print(f\"\\n📋 PATTERN #{i} PROCESSING STRATEGY\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"📊 Coverage: {count:,} records ({percentage:.2f}%)\")\n",
    "    print(f\"🏷️  Classification: {classification_info['classification']}\")\n",
    "    \n",
    "    # Get example for analysis\n",
    "    example = structure_examples[structure_hash]\n",
    "    \n",
    "    # Generate field extraction strategy\n",
    "    print(f\"🔧 Recommended extraction strategy:\")\n",
    "    \n",
    "    # Core fields that should always be extracted\n",
    "    core_fields = ['@timestamp', 'agent', 'destination', 'source', 'network', 'event']\n",
    "    present_core = [field for field in core_fields if field in example]\n",
    "    print(f\"   • Core fields ({len(present_core)}): {', '.join(present_core)}\")\n",
    "    \n",
    "    # Optional fields with null handling\n",
    "    optional_fields = []\n",
    "    if 'process' in example:\n",
    "        optional_fields.append('process')\n",
    "    if 'source' in example and isinstance(example['source'], dict) and 'process' in example['source']:\n",
    "        optional_fields.append('source.process')\n",
    "    \n",
    "    if optional_fields:\n",
    "        print(f\"   • Optional fields: {', '.join(optional_fields)} (null handling required)\")\n",
    "    \n",
    "    # Nested extraction recommendations\n",
    "    nested_extractions = []\n",
    "    if 'host' in example and 'os' in example['host']:\n",
    "        nested_extractions.append('host.os.*')\n",
    "    if 'network_traffic' in example and 'flow' in example['network_traffic']:\n",
    "        nested_extractions.append('network_traffic.flow.*')\n",
    "    \n",
    "    if nested_extractions:\n",
    "        print(f\"   • Nested extractions: {', '.join(nested_extractions)}\")\n",
    "    \n",
    "    # Priority level for processing\n",
    "    if percentage >= 50:\n",
    "        priority = \"HIGH - Primary processing pipeline\"\n",
    "    elif percentage >= 20:\n",
    "        priority = \"MEDIUM - Secondary processing pipeline\"\n",
    "    elif percentage >= 5:\n",
    "        priority = \"LOW - Specialized handling\"\n",
    "    else:\n",
    "        priority = \"MINIMAL - Error handling only\"\n",
    "    \n",
    "    print(f\"⭐ Processing priority: {priority}\")\n",
    "\n",
    "print(f\"\\n🚀 IMPLEMENTATION ROADMAP:\")\n",
    "print(f\"   1. Implement primary pipeline for Pattern #1 ({sorted_patterns[0][1]:,} records)\")\n",
    "if len(sorted_patterns) > 1:\n",
    "    print(f\"   2. Add secondary handling for Pattern #2 ({sorted_patterns[1][1]:,} records)\")\n",
    "if len(sorted_patterns) > 2:\n",
    "    print(f\"   3. Consider Pattern #3 support ({sorted_patterns[2][1]:,} records)\")\n",
    "print(f\"   4. Implement fallback processing for remaining {total_patterns-3} patterns\")\n",
    "print(f\"   5. Add comprehensive error logging and schema validation\")\n",
    "\n",
    "print(f\"\\n✅ Analysis complete! Use these insights to design your processing pipeline.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataset-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
