{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sysmon JSONL to CSV Converter\n",
    "\n",
    "## üìñ Overview\n",
    "\n",
    "This notebook transforms Windows Sysmon events from JSONL format (extracted from Elasticsearch) into structured CSV datasets for machine learning analysis. It processes raw Windows Event Log XML embedded within JSONL files and converts them into tabular format with proper field mapping per event type.\n",
    "\n",
    "### üéØ Purpose\n",
    "\n",
    "- **Parse** Windows Event Log XML from JSONL files  \n",
    "- **Extract** structured data using event-specific field schemas\n",
    "- **Transform** multi-format security events into unified CSV structure\n",
    "- **Clean** and validate data for machine learning pipeline readiness\n",
    "\n",
    "### üìä Input/Output\n",
    "\n",
    "- **Input**: JSONL file with Windows Sysmon events (from notebook #1)\n",
    "- **Output**: Structured CSV file with event-specific columns and proper field mapping\n",
    "\n",
    "### üîß Key Features\n",
    "\n",
    "- **Schema-based parsing** for 18+ different Sysmon event types\n",
    "- **Robust XML handling** with sanitization and error recovery\n",
    "- **Detailed logging** for debugging and quality assessment\n",
    "- **Field mapping validation** with missing field tracking\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Required Libraries\n",
    "\n",
    "Import essential libraries for XML parsing, data manipulation, and file processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Sysmon Event Schema Mapping\n",
    "\n",
    "Define field mappings for each Sysmon EventID based on official documentation and data analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fields obtained from https://www.ultimatewindowssecurity.com/securitylog/encyclopedia/ and manual inspection of the data\n",
    "fields_per_eventid = {\n",
    "1: ['UtcTime', 'ProcessGuid', 'ProcessId', 'Image', 'CommandLine', 'CurrentDirectory', 'User', 'Hashes', 'ParentProcessGuid', 'ParentProcessId', 'ParentImage', 'ParentCommandLine'],\n",
    "2: ['UtcTime', 'ProcessGuid', 'ProcessId', 'Image', 'TargetFilename', 'CreationUtcTime', 'PreviousCreationUtcTime', 'User'],\n",
    "3: ['UtcTime', 'ProcessGuid', 'ProcessId', 'Image', 'User', 'Protocol', 'SourceIsIpv6', 'SourceIp', 'SourceHostname', 'SourcePort', 'SourcePortName', 'DestinationIsIpv6', 'DestinationIp', 'DestinationHostname', 'DestinationPort', 'DestinationPortName'],\n",
    "5: ['UtcTime', 'ProcessGuid', 'ProcessId', 'Image', 'User'],\n",
    "6: ['UtcTime', 'ImageLoaded', 'Hashes', 'User'],\n",
    "7: ['UtcTime', 'ProcessGuid', 'ProcessId', 'Image', 'ImageLoaded', 'OriginalFileName', 'Hashes', 'User'],\n",
    "8: ['UtcTime', 'SourceProcessGuid', 'SourceProcessId', 'SourceImage', 'TargetProcessGuid', 'TargetProcessId', 'TargetImage', 'NewThreadId', 'SourceUser', 'TargetUser'],\n",
    "9: ['UtcTime', 'ProcessGuid', 'ProcessId', 'Image', 'Device', 'User'],\n",
    "10: ['UtcTime', 'SourceProcessGUID', 'SourceProcessId', 'SourceImage', 'TargetProcessGUID', 'TargetProcessId', 'TargetImage', 'SourceThreadId', 'SourceUser', 'TargetUser'],\n",
    "11: ['UtcTime', 'ProcessGuid', 'ProcessId', 'Image', 'TargetFilename', 'CreationUtcTime', 'User'],\n",
    "12: ['EventType', 'UtcTime', 'ProcessGuid', 'ProcessId', 'Image', 'TargetObject', 'User'],\n",
    "13: ['EventType', 'UtcTime', 'ProcessGuid', 'ProcessId', 'Image', 'TargetObject', 'User'],\n",
    "14: ['EventType', 'UtcTime', 'ProcessGuid', 'ProcessId', 'Image', 'TargetObject', 'User'],\n",
    "15: ['UtcTime', 'ProcessGuid', 'ProcessId', 'Image', 'TargetFilename', 'CreationUtcTime', 'Hash', 'User'],\n",
    "16: ['UtcTime', 'Configuration', 'ConfigurationFileHash', 'User'],\n",
    "17: ['EventType', 'UtcTime', 'ProcessGuid', 'ProcessId', 'PipeName', 'Image', 'User'],\n",
    "18: ['EventType', 'UtcTime', 'ProcessGuid', 'ProcessId', 'PipeName', 'Image', 'User'],\n",
    "22: ['UtcTime', 'ProcessGuid', 'ProcessId', 'Image', 'QueryName', 'QueryStatus', 'QueryResults', 'User'],\n",
    "23: ['UtcTime', 'ProcessGuid', 'ProcessId', 'User', 'Image', 'TargetFilename', 'Hashes'],\n",
    "24: ['UtcTime', 'ProcessGuid', 'ProcessId', 'User', 'Image', 'Hashes'],\n",
    "25: ['UtcTime', 'ProcessGuid', 'ProcessId', 'User', 'Image']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßπ XML Sanitization Function\n",
    "\n",
    "Clean and repair malformed XML strings before parsing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_xml(xml_str):\n",
    "    \"\"\"Clean invalid characters and repair XML structure\"\"\"\n",
    "    # Remove non-printable characters\n",
    "    cleaned = ''.join(c for c in xml_str if 31 < ord(c) < 127 or c in '\\t\\n\\r')\n",
    "    # Fix common XML issues using BeautifulSoup's parser\n",
    "    return BeautifulSoup(cleaned, \"xml\").prettify()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç XML Parser Function\n",
    "\n",
    "Parse Windows Event Log XML to extract EventID, Computer, and event-specific fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sysmon_event(xml_str):\n",
    "    \"\"\"Parse XML with enhanced error handling\"\"\"\n",
    "    try:\n",
    "        # Clean XML first\n",
    "        clean_xml = sanitize_xml(xml_str)\n",
    "        \n",
    "        # Parse with explicit namespace\n",
    "        namespaces = {'ns': 'http://schemas.microsoft.com/win/2004/08/events/event'}\n",
    "        root = ET.fromstring(clean_xml)\n",
    "        \n",
    "        # System section - with null checks\n",
    "        system = root.find('ns:System', namespaces)\n",
    "        if not system:\n",
    "            return None, None, None\n",
    "\n",
    "        event_id_elem = system.find('ns:EventID', namespaces)\n",
    "        computer_elem = system.find('ns:Computer', namespaces)\n",
    "        \n",
    "        # event_id = event_id_elem.text.strip() if (event_id_elem and event_id_elem.text) else None\n",
    "        event_id = int(event_id_elem.text) if event_id_elem is not None else None\n",
    "        \n",
    "        # computer = computer_elem.text.strip() if (computer_elem and computer_elem.text) else None\n",
    "        computer = computer_elem.text if computer_elem is not None else None\n",
    "\n",
    "        # EventData section\n",
    "        event_data = root.find('ns:EventData', namespaces)\n",
    "        fields = {}\n",
    "        if event_data:\n",
    "            for data in event_data.findall('ns:Data', namespaces):\n",
    "                name = data.get('Name')\n",
    "                # fields[name] = data.text.strip() if data.text else None\n",
    "                fields[name] = data.text if data.text else None\n",
    "\n",
    "        return event_id, computer, fields\n",
    "\n",
    "    except Exception as e:\n",
    "        # Log problematic XML samples for debugging\n",
    "        with open('bad_xml_samples.txt', 'a') as bad_xml:\n",
    "            bad_xml.write(f\"Error: {str(e)}\\n\")\n",
    "            bad_xml.write(f\"XML: {xml_str[:500]}...\\n\")\n",
    "            bad_xml.write(\"-\" * 50 + \"\\n\")\n",
    "        print(f\"XML parsing failed: {str(e)}\")\n",
    "        return None, None, None  # Crucial: return tuple of 3 Nones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Main Processing Function\n",
    "\n",
    "Process JSONL file and convert Sysmon events to structured DataFrame with comprehensive error tracking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_events(jsonl_path):\n",
    "    \"\"\"Process events with error tracking\"\"\"\n",
    "    records = []\n",
    "    error_count = 0\n",
    "    missing_fields_tracker = {}  # Track missing fields per EventID\n",
    "    eventid_counts = {}  # Track total events per EventID\n",
    "    \n",
    "    with open(jsonl_path, 'r') as f:\n",
    "        for line_number, line in enumerate(f, 1):\n",
    "            try:\n",
    "                event = json.loads(line)\n",
    "                xml_str = event['event']['original']\n",
    "                \n",
    "                event_id, computer, fields = parse_sysmon_event(xml_str)\n",
    "                \n",
    "                # Skip if essential fields missing with detailed logging\n",
    "                if not event_id or not computer:\n",
    "                    with open('parsing_errors.log', 'a') as log:\n",
    "                        log.write(f\"Line {line_number}: Failed - EventID={event_id}, Computer={computer}\\n\")\n",
    "                        log.write(f\"XML sample: {xml_str[:200]}...\\n\\n\")\n",
    "                    error_count += 1\n",
    "                    continue\n",
    "                \n",
    "                # Count total events per EventID\n",
    "                eventid_counts[event_id] = eventid_counts.get(event_id, 0) + 1\n",
    "                \n",
    "                # Build record\n",
    "                record = {\n",
    "                    'EventID': event_id,\n",
    "                    'Computer': computer,\n",
    "                    '@timestamp': event['@timestamp']\n",
    "                }\n",
    "                \n",
    "                # Add fields from mapping with mismatch tracking\n",
    "                expected_fields = fields_per_eventid.get(event_id, [])\n",
    "                for field in expected_fields:\n",
    "                    if field not in fields:  # Field expected but not found in XML\n",
    "                        if event_id not in missing_fields_tracker:\n",
    "                            missing_fields_tracker[event_id] = {}\n",
    "                        if field not in missing_fields_tracker[event_id]:\n",
    "                            missing_fields_tracker[event_id][field] = 0\n",
    "                        missing_fields_tracker[event_id][field] += 1\n",
    "                    record[field] = fields.get(field, None)\n",
    "                \n",
    "                records.append(record)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing line {line_number}: {str(e)}\")\n",
    "                error_count += 1\n",
    "    \n",
    "    print(f\"\\nProcessing complete. Errors: {error_count}, Success: {len(records)}\")\n",
    "    \n",
    "    # Print detailed missing fields summary with statistics\n",
    "    if missing_fields_tracker:\n",
    "        print(\"\\nDetailed missing fields analysis:\")\n",
    "        print(\"-\" * 60)\n",
    "        for event_id, missing_fields in missing_fields_tracker.items():\n",
    "            total_events = eventid_counts[event_id]\n",
    "            print(f\"EventID {event_id}: {total_events} total events\")\n",
    "            for field, missing_count in missing_fields.items():\n",
    "                percentage = (missing_count / total_events) * 100\n",
    "                print(f\"  ‚Ä¢ Field '{field}': {missing_count}/{total_events} missing ({percentage:.1f}%)\")\n",
    "            print()\n",
    "    else:\n",
    "        print(\"\\nNo missing fields detected - all schema mappings match XML structure.\")\n",
    "    \n",
    "    # Print EventID distribution summary\n",
    "    print(\"EventID distribution:\")\n",
    "    print(\"-\" * 40)\n",
    "    for event_id in sorted(eventid_counts.keys()):\n",
    "        print(f\"EventID {event_id}: {eventid_counts[event_id]:,} events\")\n",
    "    \n",
    "    return pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÅ File Configuration\n",
    "\n",
    "Set input and output file paths for processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"-ds-logs-windows-sysmon_operational-default-2025-05-04-000001.jsonl\"\n",
    "output_file = \"sysmon-2025-05-04-000001.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Execute Processing\n",
    "\n",
    "Run the main processing function to convert JSONL to DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing complete. Errors: 0, Success: 570078\n",
      "\n",
      "Detailed missing fields analysis:\n",
      "------------------------------------------------------------\n",
      "EventID 6: 481 total events\n",
      "  ‚Ä¢ Field 'User': 481/481 missing (100.0%)\n",
      "\n",
      "EventID distribution:\n",
      "----------------------------------------\n",
      "EventID 1: 1,461 events\n",
      "EventID 2: 57 events\n",
      "EventID 3: 16,918 events\n",
      "EventID 4: 6 events\n",
      "EventID 5: 965 events\n",
      "EventID 6: 481 events\n",
      "EventID 7: 63,892 events\n",
      "EventID 9: 1,158 events\n",
      "EventID 10: 57,814 events\n",
      "EventID 11: 4,271 events\n",
      "EventID 12: 289,812 events\n",
      "EventID 13: 129,227 events\n",
      "EventID 15: 46 events\n",
      "EventID 17: 488 events\n",
      "EventID 18: 1,491 events\n",
      "EventID 23: 1,979 events\n",
      "EventID 24: 5 events\n",
      "EventID 25: 7 events\n"
     ]
    }
   ],
   "source": [
    "df = process_events(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßΩ Data Cleaning Function\n",
    "\n",
    "Clean DataFrame by removing whitespace and normalizing empty values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataframe(df):\n",
    "    \"\"\"Clean whitespace and newlines from all string columns\"\"\"\n",
    "    # Trim whitespace for string columns\n",
    "    str_cols = df.select_dtypes(['object']).columns\n",
    "    df[str_cols] = df[str_cols].apply(lambda x: x.str.strip())\n",
    "    \n",
    "    # Replace empty strings with None\n",
    "    df.replace({'': None}, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßº Apply Data Cleaning\n",
    "\n",
    "Clean the processed DataFrame to prepare for CSV export:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clean_dataframe(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Export to CSV\n",
    "\n",
    "Save the cleaned DataFrame as CSV file for machine learning pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Optional Data Exploration\n",
    "\n",
    "The following commented cells provide various data exploration and validation options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df = pd.read_csv(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check rows where the Utctime is null\n",
    "# new_df[new_df['UtcTime'].isnull()].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df[new_df['UtcTime'].isnull()]['EventID'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df['EventID'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df[new_df['EventID'] == 1].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df[new_df['EventID'] == 3].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df[new_df['EventID'] == 5].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df[new_df['EventID'] == 6].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df[new_df['EventID'] == 6]['ImageLoaded'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df[new_df['EventID'] == 7].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df[new_df['EventID'] == 8].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df[new_df['EventID'] == 9].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df[new_df['EventID'] == 10].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df[new_df['EventID'] == 10]['TargetProcessGUID'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df[new_df['EventID'] == 11].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df[new_df['EventID'] == 12].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df[new_df['EventID'] == 13].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df[new_df['EventID'] == 14].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df[new_df['EventID'] == 15].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df[new_df['EventID'] == 16].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df[new_df['EventID'] == 17].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df[new_df['EventID'] == 18].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df[new_df['EventID'] == 22].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df[new_df['EventID'] == 23].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df[new_df['EventID'] == 24].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df[new_df['EventID'] == 25].head(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataset-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
