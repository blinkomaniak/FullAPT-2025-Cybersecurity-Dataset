{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": "# Elasticsearch Index Downloader\n\n## Overview\n\nThis notebook implements the **first stage** of our cybersecurity dataset creation pipeline. It provides a systematic approach to connect to an Elasticsearch cluster, discover relevant indices, and export time-ranged data to JSONL (JSON Lines) format for downstream processing.\n\n## Pipeline Context\n\nThis is **Notebook 1 of 7** in the dataset generation workflow:\n1. **[Current]** Elasticsearch Index Downloader - Raw data extraction\n2. Sysmon Dataset CSV Creator - Windows event log processing  \n3. Network Traffic Flow CSV Creator - Network data processing\n4. Caldera Report Analyzer - Attack timeline analysis\n5. Sysmon Event Tracking - Advanced event correlation\n6. Event Timeline Plotter - Temporal visualization\n7. Network Event Tracking - Network behavior analysis\n\n## Process Workflow\n\n```mermaid\ngraph TB\n    A[Connect to Elasticsearch] --> B[Discover Available Indices]\n    B --> C[Filter by Keywords: 'sysmon', 'network_traffic']\n    C --> D[Display Index Information<br/>Size, Creation Date]\n    D --> E[Interactive Index Selection]\n    E --> F[Define Time Range<br/>Start & End DateTime]\n    F --> G[Export Query Execution<br/>@timestamp filtering]\n    G --> H[JSONL File Generation<br/>One file per index]\n    H --> I[Pipeline Ready for Stage 2]\n```\n\n## Key Technical Features\n\n- **Secure Connection**: SSL-disabled connection suitable for internal network environments\n- **Index Discovery**: Automatic identification of relevant indices using keyword filtering\n- **Time Range Filtering**: Precise timestamp-based data extraction using Elasticsearch range queries\n- **Efficient Scanning**: Uses `elasticsearch.helpers.scan()` for memory-efficient large dataset processing\n- **JSONL Output**: Structured line-delimited JSON format optimized for data pipeline integration\n\n## Output Artifacts\n\nGenerated JSONL files follow the naming convention: `{sanitized-index-name}.jsonl`\n- Example: `.ds-logs-windows.sysmon_operational-default-2025.05.04-000001` ‚Üí `-ds-logs-windows-sysmon_operational-default-2025-05-04-000001.jsonl`"
  },
  {
   "cell_type": "code",
   "source": "# Core Elasticsearch connectivity\nfrom elasticsearch import Elasticsearch\nfrom elasticsearch.helpers import scan  # Memory-efficient large dataset iteration\n\n# Time handling with timezone support\nfrom datetime import datetime, timezone\n\n# Data serialization and file system operations\nimport json\nimport os",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Elasticsearch cluster connection settings\nes_host = \"https://10.2.0.20:9200\"  # Internal network Elasticsearch endpoint\nusername = \"elastic\"                # Cluster admin username\npassword = \"hiYqiU21LVg0F8krD=XN\"   # Cluster admin password (secure in production)\n\n# Index filtering patterns for cybersecurity data sources\nkeywords = ['sysmon', 'network_traffic']  # Target data types for analysis\n\n# Output configuration\noutput_dir = \"./\"  # Current directory for JSONL file generation\n\n# Timestamp parsing format (matches Elasticsearch Kibana format)\nTIMESTAMP_FORMAT = \"%b %d, %Y @ %H:%M:%S.%f\"  # Example: \"May 04, 2025 @ 11:30:00.000\"",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def connect_elasticsearch():\n    \"\"\"Create secure connection to Elasticsearch cluster with authentication\"\"\"\n    return Elasticsearch(\n        hosts=[es_host],                    # Target cluster endpoint\n        basic_auth=(username, password),   # Username/password authentication\n        verify_certs=False,                # Disable SSL verification for internal networks\n        ssl_show_warn=False                # Suppress SSL warning messages\n    )",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def test_connection(es):\n    \"\"\"Validate Elasticsearch connection with cluster health check\"\"\"\n    try:\n        return es.ping()  # Lightweight cluster connectivity test\n    except Exception as e:\n        print(f\"üî• Connection failed: {e}\")  # User-friendly error reporting\n        return False",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def list_relevant_indices(es, keywords):\n    \"\"\"Retrieve indices containing keywords with storage size and creation metadata\"\"\"\n    try:\n        # Fetch index metadata: name, size, creation timestamp\n        response = es.cat.indices(format=\"json\", h=\"index,store.size,creation.date\")\n        \n        return [\n            {\n                \"name\": idx[\"index\"],                                                    # Full index name\n                \"size\": idx.get(\"store.size\", \"0b\"),                                   # Storage size (human-readable)\n                \"created\": datetime.fromtimestamp(int(idx[\"creation.date\"])/1000, tz=timezone.utc)  # UTC creation time\n            }\n            for idx in response\n            if any(kw in idx[\"index\"] for kw in keywords)  # Filter by keyword patterns\n        ]\n    except Exception as e:\n        print(f\"üö® Error listing indices: {e}\")  # Handle API failures gracefully\n        return []",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def display_indices_selector(indices):\n    \"\"\"Interactive index selection with user-friendly interface and validation\"\"\"\n    print(f\"\\nüìÇ Found {len(indices)} relevant indices:\")\n    \n    # Display numbered list with metadata for informed selection\n    for i, idx in enumerate(indices, 1):\n        print(f\"{i:>3}. {idx['name']} ({idx['size']}) [Created: {idx['created'].strftime('%Y-%m-%d')}]\")\n\n    while True:  # Continue until valid selection or exit\n        selection = input(\"\\nüî¢ Select indices (comma-separated numbers, 'all', or 'exit'): \").strip().lower()\n        \n        # Handle exit condition\n        if selection == \"exit\":\n            return []\n            \n        # Handle bulk selection\n        if selection == \"all\":\n            return [idx[\"name\"] for idx in indices]\n        \n        try:\n            # Parse comma-separated numbers and convert to index names\n            selected_indices = [\n                indices[int(num)-1][\"name\"]  # Convert 1-based input to 0-based indexing\n                for num in selection.split(\",\") \n                if num.strip().isdigit()    # Validate numeric input\n            ]\n            \n            if selected_indices:\n                return list(set(selected_indices))  # Remove duplicates and return\n            \n            print(\"‚ö†Ô∏è No valid selection. Please try again.\")\n            \n        except (IndexError, ValueError):\n            print(\"‚õî Invalid input format. Use numbers separated by commas.\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def parse_utc_time(time_str):\n    \"\"\"Parse time string into UTC datetime object for Elasticsearch compatibility\"\"\"\n    try:\n        # Clean input: remove timezone suffix if present (assume UTC)\n        time_str = time_str.split(\" (UTC)\")[0].strip()\n        \n        # Parse using predefined format and enforce UTC timezone\n        return datetime.strptime(time_str, TIMESTAMP_FORMAT).replace(tzinfo=timezone.utc)\n        \n    except ValueError as e:\n        # Provide user-friendly error feedback with format example\n        print(f\"‚è∞ Time parsing error: {e}\")\n        print(f\"üìÖ Expected format: {TIMESTAMP_FORMAT} (UTC)\")\n        return None",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def export_index_data(es, index_name, start_time, end_time):\n    \"\"\"Export index data within time range to JSONL file with memory-efficient scanning\"\"\"\n    \n    # Ensure output directory exists\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Sanitize index name for filesystem compatibility\n    safe_name = index_name.replace(\":\", \"_\").replace(\".\", \"-\")\n    filename = os.path.join(output_dir, f\"{safe_name}.jsonl\")\n    \n    # Construct Elasticsearch range query for temporal filtering\n    query = {\n        \"query\": {\n            \"range\": {\n                \"@timestamp\": {                         # Standard Elasticsearch timestamp field\n                    \"gte\": start_time.isoformat(),      # Greater than or equal (start)\n                    \"lte\": end_time.isoformat(),        # Less than or equal (end)\n                    \"format\": \"strict_date_optional_time\"  # ISO datetime format\n                }\n            }\n        }\n    }\n\n    try:\n        with open(filename, \"w\") as f:\n            count = 0\n            \n            # Memory-efficient iteration through large result sets\n            for hit in scan(es, index=index_name, query=query):\n                # Write each document as a single JSON line\n                f.write(json.dumps(hit[\"_source\"]) + \"\\n\")  # Extract document source only\n                count += 1\n                \n        print(f\"‚úÖ Success: {count} documents from {index_name} -> {filename}\")\n        return True\n        \n    except Exception as e:\n        print(f\"‚ùå Failed to export {index_name}: {e}\")  # Handle export failures gracefully\n        return False",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def main():\n    \"\"\"Main pipeline orchestrator for Elasticsearch data extraction workflow\"\"\"\n    \n    # Stage 1: Establish cluster connection\n    print(\"\\nüîó Connecting to Elasticsearch...\")\n    es = connect_elasticsearch()\n    \n    # Stage 2: Validate connection before proceeding\n    if not test_connection(es):\n        print(\"üö® Could not establish connection to Elasticsearch\")\n        return\n\n    # Stage 3: Discover available indices matching keywords\n    print(\"\\nüîç Searching for relevant indices...\")\n    indices = list_relevant_indices(es, keywords)\n    \n    if not indices:\n        print(\"ü§∑ No matching indices found\")\n        return\n\n    # Stage 4: Interactive index selection\n    selected_indices = display_indices_selector(indices)\n    if not selected_indices:\n        print(\"üö™ Exiting without download\")\n        return\n\n    # Stage 5: Time range specification for temporal filtering\n    print(\"\\nüïí Time Range Selection (UTC)\")\n    print(\"üí° Example format: 'Jan 29, 2025 @ 04:24:54.863'\")\n    start_time = parse_utc_time(input(\"‚è±Ô∏è  Start time: \"))\n    end_time = parse_utc_time(input(\"‚è∞ End time: \"))\n    \n    # Validate time parsing results\n    if not all([start_time, end_time]):\n        print(\"‚õî Invalid time parameters\")\n        return\n\n    # Stage 6: Execute data export for each selected index\n    print(\"\\n‚è≥ Starting data export...\")\n    for index in selected_indices:\n        export_index_data(es, index, start_time, end_time)\n\n    # Display summary for verification and downstream reference\n    print(f'start time: {start_time}')\n    print(f'end time: {end_time}')",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Execute the complete Elasticsearch data extraction pipeline\nif __name__ == \"__main__\":\n    main()  # Run interactive data extraction workflow",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "es_host = \"https://10.2.0.20:9200\"\n",
    "username = \"elastic\"\n",
    "password = \"hiYqiU21LVg0F8krD=XN\"\n",
    "keywords = ['sysmon', 'network_traffic']\n",
    "output_dir = \"./\"\n",
    "TIMESTAMP_FORMAT = \"%b %d, %Y @ %H:%M:%S.%f\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_elasticsearch():\n",
    "    \"\"\"Create secure connection to Elasticsearch (with SSL verification disabled)\"\"\"\n",
    "    return Elasticsearch(\n",
    "        hosts=[es_host],\n",
    "        basic_auth=(username, password),\n",
    "        verify_certs=False,\n",
    "        ssl_show_warn=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_connection(es):\n",
    "    \"\"\"Validate ES connection with cluster health check\"\"\"\n",
    "    try:\n",
    "        return es.ping()\n",
    "    except Exception as e:\n",
    "        print(f\"üî• Connection failed: {e}\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_relevant_indices(es, keywords):\n",
    "    \"\"\"Retrieve indices containing keywords with storage size\"\"\"\n",
    "    try:\n",
    "        response = es.cat.indices(format=\"json\", h=\"index,store.size,creation.date\")\n",
    "        return [\n",
    "            {\n",
    "                \"name\": idx[\"index\"],\n",
    "                \"size\": idx.get(\"store.size\", \"0b\"),\n",
    "                \"created\": datetime.fromtimestamp(int(idx[\"creation.date\"])/1000, tz=timezone.utc)\n",
    "            }\n",
    "            for idx in response\n",
    "            if any(kw in idx[\"index\"] for kw in keywords)\n",
    "        ]\n",
    "    except Exception as e:\n",
    "        print(f\"üö® Error listing indices: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_indices_selector(indices):\n",
    "    \"\"\"Interactive index selection with pagination\"\"\"\n",
    "    print(f\"\\nüìÇ Found {len(indices)} relevant indices:\")\n",
    "    for i, idx in enumerate(indices, 1):\n",
    "        print(f\"{i:>3}. {idx['name']} ({idx['size']}) [Created: {idx['created'].strftime('%Y-%m-%d')}]\")\n",
    "\n",
    "    while True:\n",
    "        selection = input(\"\\nüî¢ Select indices (comma-separated numbers, 'all', or 'exit'): \").strip().lower()\n",
    "        \n",
    "        if selection == \"exit\":\n",
    "            return []\n",
    "        if selection == \"all\":\n",
    "            return [idx[\"name\"] for idx in indices]\n",
    "        \n",
    "        try:\n",
    "            selected_indices = [\n",
    "                indices[int(num)-1][\"name\"] \n",
    "                for num in selection.split(\",\") \n",
    "                if num.strip().isdigit()\n",
    "            ]\n",
    "            if selected_indices:\n",
    "                return list(set(selected_indices))  # Remove duplicates\n",
    "            print(\"‚ö†Ô∏è No valid selection. Please try again.\")\n",
    "        except (IndexError, ValueError):\n",
    "            print(\"‚õî Invalid input format. Use numbers separated by commas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_utc_time(time_str):  # Corrected function name\n",
    "    \"\"\"Parse time string into UTC datetime object\"\"\"\n",
    "    try:\n",
    "        # Strip any trailing timezone identifiers (we assume UTC)\n",
    "        time_str = time_str.split(\" (UTC)\")[0].strip()\n",
    "        return datetime.strptime(time_str, TIMESTAMP_FORMAT).replace(tzinfo=timezone.utc)\n",
    "    except ValueError as e:\n",
    "        print(f\"‚è∞ Time parsing error: {e}\")\n",
    "        print(f\"üìÖ Expected format: {TIMESTAMP_FORMAT} (UTC)\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_index_data(es, index_name, start_time, end_time):\n",
    "    \"\"\"Export index data within time range to JSONL file\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    safe_name = index_name.replace(\":\", \"_\").replace(\".\", \"-\")\n",
    "    filename = os.path.join(output_dir, f\"{safe_name}.jsonl\")\n",
    "    \n",
    "    query = {\n",
    "        \"query\": {\n",
    "            \"range\": {\n",
    "                \"@timestamp\": {\n",
    "                    \"gte\": start_time.isoformat(),\n",
    "                    \"lte\": end_time.isoformat(),\n",
    "                    \"format\": \"strict_date_optional_time\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        with open(filename, \"w\") as f:\n",
    "            count = 0\n",
    "            for hit in scan(es, index=index_name, query=query):\n",
    "                f.write(json.dumps(hit[\"_source\"]) + \"\\n\")\n",
    "                count += 1\n",
    "        print(f\"‚úÖ Success: {count} documents from {index_name} -> {filename}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to export {index_name}: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"\\nüîó Connecting to Elasticsearch...\")\n",
    "    es = connect_elasticsearch()\n",
    "    \n",
    "    if not test_connection(es):\n",
    "        print(\"üö® Could not establish connection to Elasticsearch\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nüîç Searching for relevant indices...\")\n",
    "    indices = list_relevant_indices(es, keywords)\n",
    "    \n",
    "    if not indices:\n",
    "        print(\"ü§∑ No matching indices found\")\n",
    "        return\n",
    "\n",
    "    selected_indices = display_indices_selector(indices)\n",
    "    if not selected_indices:\n",
    "        print(\"üö™ Exiting without download\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nüïí Time Range Selection (UTC)\")\n",
    "    print(\"üí° Example format: 'Jan 29, 2025 @ 04:24:54.863'\")\n",
    "    start_time = parse_utc_time(input(\"‚è±Ô∏è  Start time: \"))\n",
    "    end_time = parse_utc_time(input(\"‚è∞ End time: \"))\n",
    "    \n",
    "    if not all([start_time, end_time]):\n",
    "        print(\"‚õî Invalid time parameters\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n‚è≥ Starting data export...\")\n",
    "    for index in selected_indices:\n",
    "        export_index_data(es, index, start_time, end_time)\n",
    "\n",
    "    print(f'start time: {start_time}')\n",
    "    print(f'end time: {end_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîó Connecting to Elasticsearch...\n",
      "\n",
      "üîç Searching for relevant indices...\n",
      "\n",
      "üìÇ Found 13 relevant indices:\n",
      "  1. .ds-logs-network_traffic.dns-default-2025.03.10-000001 (114.2mb) [Created: 2025-03-10]\n",
      "  2. .ds-logs-network_traffic.tls-default-2025.03.10-000001 (52.7mb) [Created: 2025-03-10]\n",
      "  3. .ds-logs-network_traffic.icmp-default-2025.03.10-000001 (2mb) [Created: 2025-03-10]\n",
      "  4. .ds-logs-windows.sysmon_operational-default-2025.04.15-000002 (17.2gb) [Created: 2025-04-15]\n",
      "  5. .ds-logs-network_traffic.dhcpv4-default-2025.04.17-000001 (86.2kb) [Created: 2025-04-17]\n",
      "  6. .ds-logs-network_traffic.tls-default-2025.04.15-000002 (45.3mb) [Created: 2025-04-15]\n",
      "  7. .ds-logs-network_traffic.flow-default-2025.04.15-000002 (25.3gb) [Created: 2025-04-15]\n",
      "  8. .ds-logs-network_traffic.dns-default-2025.04.15-000002 (255.4mb) [Created: 2025-04-15]\n",
      "  9. .ds-logs-windows.sysmon_operational-default-2025.03.10-000001 (8.4gb) [Created: 2025-03-10]\n",
      " 10. .ds-logs-network_traffic.icmp-default-2025.04.15-000002 (7.6mb) [Created: 2025-04-15]\n",
      " 11. .ds-logs-network_traffic.http-default-2025.04.15-000002 (137.2mb) [Created: 2025-04-15]\n",
      " 12. .ds-logs-network_traffic.http-default-2025.03.10-000001 (30.1mb) [Created: 2025-03-10]\n",
      " 13. .ds-logs-network_traffic.flow-default-2025.03.10-000001 (4.8gb) [Created: 2025-03-10]\n",
      "\n",
      "üïí Time Range Selection (UTC)\n",
      "üí° Example format: 'Jan 29, 2025 @ 04:24:54.863'\n",
      "\n",
      "‚è≥ Starting data export...\n",
      "‚úÖ Success: 0 documents from .ds-logs-network_traffic.dns-default-2025.03.10-000001 -> ./-ds-logs-network_traffic-dns-default-2025-03-10-000001.jsonl\n",
      "‚úÖ Success: 0 documents from .ds-logs-network_traffic.tls-default-2025.03.10-000001 -> ./-ds-logs-network_traffic-tls-default-2025-03-10-000001.jsonl\n",
      "‚úÖ Success: 0 documents from .ds-logs-network_traffic.icmp-default-2025.03.10-000001 -> ./-ds-logs-network_traffic-icmp-default-2025-03-10-000001.jsonl\n",
      "‚úÖ Success: 570078 documents from .ds-logs-windows.sysmon_operational-default-2025.04.15-000002 -> ./-ds-logs-windows-sysmon_operational-default-2025-04-15-000002.jsonl\n",
      "‚úÖ Success: 0 documents from .ds-logs-network_traffic.dhcpv4-default-2025.04.17-000001 -> ./-ds-logs-network_traffic-dhcpv4-default-2025-04-17-000001.jsonl\n",
      "‚úÖ Success: 293 documents from .ds-logs-network_traffic.tls-default-2025.04.15-000002 -> ./-ds-logs-network_traffic-tls-default-2025-04-15-000002.jsonl\n",
      "‚úÖ Success: 1090212 documents from .ds-logs-network_traffic.flow-default-2025.04.15-000002 -> ./-ds-logs-network_traffic-flow-default-2025-04-15-000002.jsonl\n",
      "‚úÖ Success: 4928 documents from .ds-logs-network_traffic.dns-default-2025.04.15-000002 -> ./-ds-logs-network_traffic-dns-default-2025-04-15-000002.jsonl\n",
      "‚úÖ Success: 0 documents from .ds-logs-windows.sysmon_operational-default-2025.03.10-000001 -> ./-ds-logs-windows-sysmon_operational-default-2025-03-10-000001.jsonl\n",
      "‚úÖ Success: 249 documents from .ds-logs-network_traffic.icmp-default-2025.04.15-000002 -> ./-ds-logs-network_traffic-icmp-default-2025-04-15-000002.jsonl\n",
      "‚úÖ Success: 193 documents from .ds-logs-network_traffic.http-default-2025.04.15-000002 -> ./-ds-logs-network_traffic-http-default-2025-04-15-000002.jsonl\n",
      "‚úÖ Success: 0 documents from .ds-logs-network_traffic.http-default-2025.03.10-000001 -> ./-ds-logs-network_traffic-http-default-2025-03-10-000001.jsonl\n",
      "‚úÖ Success: 0 documents from .ds-logs-network_traffic.flow-default-2025.03.10-000001 -> ./-ds-logs-network_traffic-flow-default-2025-03-10-000001.jsonl\n",
      "start time: 2025-05-04 11:30:00+00:00\n",
      "end time: 2025-05-04 12:40:00+00:00\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataset-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}