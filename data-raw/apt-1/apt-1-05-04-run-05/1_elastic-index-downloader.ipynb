{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Elasticsearch Index Downloader for Cybersecurity Data Collection\n\n## üìñ Academic Overview\n\nThis notebook implements the **first stage** of a comprehensive cybersecurity dataset creation pipeline designed for machine learning research in anomaly detection and threat hunting. The pipeline extracts real-world cybersecurity event data from an Elasticsearch cluster that captures live network traffic and host-based security events during Advanced Persistent Threat (APT) attack simulations.\n\n### üéØ Research Context\n\n- **Domain**: Cybersecurity Machine Learning, Anomaly Detection\n- **Application**: APT Attack Detection, Security Information and Event Management (SIEM)\n- **Data Source**: Elasticsearch cluster with live cybersecurity telemetry\n- **Pipeline Stage**: Stage 1 of 7 (Data Extraction ‚Üí Feature Engineering ‚Üí Model Training)\n\n### üìä Data Collection Architecture\n\nThe Elasticsearch cluster collects multi-modal cybersecurity data:\n\n1. **Windows Security Events** (Sysmon): Process creation, network connections, file modifications\n2. **Network Traffic Flows**: DNS queries, HTTP requests, TLS handshakes, ICMP packets\n3. **Host-based Logs**: Authentication events, system calls, file access patterns\n4. **Attack Simulation Data**: Caldera framework APT emulation with ground truth labels\n\n### üîÑ Pipeline Architecture\n\nThis notebook is part of a **7-stage cybersecurity dataset creation pipeline**:\n\n```mermaid\ngraph TD\n    A[\"üîç Stage 1: Elasticsearch Data Extraction<br/>(This Notebook)\"] --> B[\"üìä Stage 2: Sysmon Dataset Creation\"]\n    B --> C[\"üåê Stage 3: Network Flow Dataset Creation\"]\n    C --> D[\"üìã Stage 4: Caldera Report Analysis\"]\n    D --> E[\"üéØ Stage 5: Event Tracking & Labeling\"]\n    E --> F[\"üìà Stage 6: Timeline Visualization\"]\n    F --> G[\"üîó Stage 7: Network Event Correlation\"]\n    \n    subgraph \"Data Sources\"\n        H[\"Elasticsearch Cluster\"]\n        I[\"Sysmon Logs\"]\n        J[\"Network Flows\"]\n        K[\"Caldera Reports\"]\n    end\n    \n    H --> A\n    I --> B\n    J --> C\n    K --> D\n    \n    subgraph \"Output Formats\"\n        L[\"JSONL Files\"]\n        M[\"CSV Datasets\"]\n        N[\"Labeled Data\"]\n        O[\"Visualizations\"]\n    end\n    \n    A --> L\n    B --> M\n    E --> N\n    F --> O\n```\n\n### üéØ Stage 1 Objectives (This Notebook)\n\n1. **Connect** to Elasticsearch cluster with security telemetry\n2. **Discover** available indices containing cybersecurity data\n3. **Select** relevant indices through interactive user interface\n4. **Extract** structured security events within specified time ranges\n5. **Serialize** data in JSONL format for efficient downstream processing\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## üõ†Ô∏è Environment Setup & Dependencies\n\n### Required Libraries\n\nThis notebook requires several Python libraries for Elasticsearch integration, data processing, and file I/O operations. Each library serves a specific purpose in the data extraction pipeline:\n\n- **elasticsearch**: Official Python client for Elasticsearch API interactions and search operations\n- **json**: Built-in JSON serialization for data handling and JSONL output format\n- **datetime**: Timestamp processing and time-based query filtering for temporal data extraction\n- **os**: File system operations and path management for output directory handling\n\n### üîß Technical Configuration\n\nThe notebook implements an **interactive approach** for:\n- **Manual index selection**: User chooses which security indices to process\n- **Custom time range input**: User specifies exact temporal boundaries for data extraction\n- **Real-time feedback**: Progress updates and status messages during processing\n- **Flexible output**: JSONL format for efficient downstream ML pipeline integration\n\n### üéì Educational Value\n\nThis implementation demonstrates:\n- **Production Elasticsearch integration** patterns for cybersecurity data\n- **Interactive data exploration** techniques for security researchers\n- **JSONL streaming** for memory-efficient large dataset processing\n- **Error handling** best practices for distributed system interactions\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## ‚öôÔ∏è Configuration Management\n\n### üîß Elasticsearch Connection Parameters\n\nThe configuration section defines critical parameters for connecting to the Elasticsearch cluster and extracting cybersecurity data. These parameters are optimized for interactive data collection from APT simulation environments:\n\n#### Connection Settings\n- **Host**: Elasticsearch cluster endpoint with HTTPS encryption\n- **Authentication**: Username/password authentication for secure access\n- **SSL Configuration**: Disabled certificate verification for lab environments\n- **Keywords**: Filter criteria for discovering relevant security indices\n\n#### Data Discovery\n- **Target Keywords**: `['sysmon', 'network_traffic']` to identify cybersecurity-relevant indices\n- **Output Directory**: Local directory for storing extracted JSONL files\n- **Timestamp Format**: Human-readable format for interactive time range specification\n\n#### Security Considerations\n- **Lab Environment**: Configuration optimized for research/testing environments\n- **Production Adaptation**: For production use, enable SSL verification and use secure credential management\n- **Access Control**: Ensure appropriate Elasticsearch user permissions for read-only data access\n\n### üéØ Interactive Workflow Design\n\nThis configuration supports an **interactive data extraction workflow**:\n1. **Automatic Discovery**: Find relevant indices based on keywords\n2. **Manual Selection**: User chooses specific indices for processing\n3. **Time Range Input**: User specifies extraction window with human-readable format\n4. **Batch Processing**: Sequential extraction from selected indices\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## üîå Elasticsearch Connection Management\n\n### üõ°Ô∏è Connection Architecture\n\nThe connection management functions implement a simple yet robust approach to Elasticsearch cluster interaction for cybersecurity data extraction:\n\n#### Connection Features\n- **Basic Authentication**: Username/password authentication for lab environments\n- **SSL Handling**: Disabled certificate verification for development clusters\n- **Health Validation**: Connection testing with ping functionality\n- **Error Reporting**: Clear error messages for troubleshooting connection issues\n\n#### Design Philosophy\nThis implementation prioritizes **simplicity and clarity** over complex enterprise features:\n- **Direct Connection**: Single-node connection suitable for research environments\n- **Minimal Configuration**: Straightforward setup without complex pooling or load balancing\n- **Interactive Feedback**: Clear status messages for user understanding\n- **Research Focus**: Optimized for data extraction rather than production monitoring\n\n#### Connection Functions\n\n1. **`connect_elasticsearch()`**: Establishes secure connection to the cluster\n2. **`test_connection(es)`**: Validates connectivity with basic ping test\n\nThese functions provide the foundation for all subsequent data discovery and extraction operations, ensuring reliable access to the cybersecurity telemetry stored in the Elasticsearch cluster.\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import scan\n",
    "from datetime import datetime, timezone  # Add timezone to the import\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## üîç Index Discovery & Interactive Selection\n\n### üìä Intelligent Index Discovery\n\nThe index discovery system identifies cybersecurity-relevant data stores within the Elasticsearch cluster using keyword-based filtering. This approach efficiently narrows down the search space from potentially hundreds of indices to those containing security telemetry.\n\n#### Discovery Process\n1. **Comprehensive Scan**: Query all indices in the cluster using `cat.indices` API\n2. **Keyword Filtering**: Match index names containing `'sysmon'` and `'network_traffic'` patterns\n3. **Metadata Collection**: Extract storage size and creation timestamps for each relevant index\n4. **User Presentation**: Display indices with human-readable information for selection\n\n#### Interactive Selection Interface\nThe system implements a **user-friendly selection mechanism**:\n- **Numbered List**: Clear enumeration of available indices with metadata\n- **Flexible Input**: Support for individual numbers, comma-separated lists, or 'all' selection\n- **Size Information**: Display storage size to help users understand data volume\n- **Creation Dates**: Timestamp information for temporal context\n\n#### Index Functions\n\n1. **`list_relevant_indices(es, keywords)`**: Discovers and filters indices based on security keywords\n2. **`display_indices_selector(indices)`**: Interactive user interface for index selection\n\nThis approach balances **automation** (keyword-based discovery) with **user control** (manual selection), allowing researchers to focus on specific data sources relevant to their analysis objectives.\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "es_host = \"https://10.2.0.20:9200\"\n",
    "username = \"elastic\"\n",
    "password = \"hiYqiU21LVg0F8krD=XN\"\n",
    "keywords = ['sysmon', 'network_traffic']\n",
    "output_dir = \"./\"\n",
    "TIMESTAMP_FORMAT = \"%b %d, %Y @ %H:%M:%S.%f\""
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## üìÖ Temporal Processing & Data Extraction\n\n### ‚è∞ Time Range Management\n\nThe temporal processing system handles time-based filtering for cybersecurity event extraction. This is critical for focusing on specific attack simulation windows or investigation timeframes.\n\n#### Time Format Design\n- **Human-Readable Input**: Format like `'Jan 29, 2025 @ 04:24:54.863'` for intuitive user interaction\n- **UTC Standardization**: All timestamps converted to UTC for consistent processing\n- **Precision Support**: Microsecond precision for high-resolution security event correlation\n- **Timezone Handling**: Automatic UTC conversion regardless of input timezone indicators\n\n#### Data Extraction Architecture\nThe extraction system implements **streaming processing** for memory-efficient handling of large security datasets:\n\n##### Extraction Features\n- **Time-Range Queries**: Elasticsearch range queries on `@timestamp` field\n- **Streaming Output**: Direct write to JSONL files without loading entire datasets in memory\n- **Progress Tracking**: Real-time document count reporting during extraction\n- **Error Resilience**: Individual index failures don't stop the entire process\n\n##### JSONL Format Benefits\n- **Line-by-Line Processing**: Each security event on a separate line for easy streaming\n- **Fault Tolerance**: Partial files remain valid if extraction is interrupted\n- **ML Pipeline Ready**: Direct compatibility with pandas, ML frameworks, and data processing tools\n- **Human Readable**: JSON format allows manual inspection and debugging\n\n#### Processing Functions\n\n1. **`parse_utc_time(time_str)`**: Converts human-readable timestamps to UTC datetime objects\n2. **`export_index_data(es, index_name, start_time, end_time)`**: Streams security events to JSONL files\n\nThis design prioritizes **usability** for researchers while maintaining **efficiency** for large-scale cybersecurity data processing.\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_elasticsearch():\n",
    "    \"\"\"Create secure connection to Elasticsearch (with SSL verification disabled)\"\"\"\n",
    "    return Elasticsearch(\n",
    "        hosts=[es_host],\n",
    "        basic_auth=(username, password),\n",
    "        verify_certs=False,\n",
    "        ssl_show_warn=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_connection(es):\n",
    "    \"\"\"Validate ES connection with cluster health check\"\"\"\n",
    "    try:\n",
    "        return es.ping()\n",
    "    except Exception as e:\n",
    "        print(f\"üî• Connection failed: {e}\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## üöÄ Orchestration & Execution Workflow\n\n### üéØ Interactive Data Extraction Pipeline\n\nThe main orchestration function coordinates the entire data extraction workflow, implementing a **user-guided approach** for cybersecurity data collection. This design empowers researchers to make informed decisions about which data to extract while maintaining automated efficiency for the actual processing.\n\n#### Workflow Stages\n\n1. **üîó Connection Establishment**\n   - Initialize Elasticsearch client with configured parameters\n   - Validate connectivity to ensure cluster accessibility\n   - Provide clear feedback on connection status\n\n2. **üîç Index Discovery**\n   - Scan cluster for cybersecurity-relevant indices\n   - Present findings to user with metadata (size, creation date)\n   - Handle cases where no relevant indices are found\n\n3. **üìã Interactive Selection**\n   - Display numbered list of available indices\n   - Accept user input for index selection (individual, multiple, or all)\n   - Validate selections and handle user choices\n\n4. **üìÖ Time Range Configuration**\n   - Prompt user for start and end timestamps\n   - Parse human-readable time format into UTC datetime objects\n   - Validate time range parameters for logical consistency\n\n5. **üì• Data Extraction**\n   - Process each selected index sequentially\n   - Stream data directly to JSONL files for memory efficiency\n   - Provide real-time progress feedback and final statistics\n\n#### Design Principles\n\n- **User Control**: Researchers decide which data to extract and when\n- **Transparency**: Clear feedback at each stage of the process\n- **Flexibility**: Support for various selection patterns and time ranges\n- **Reliability**: Graceful error handling and process continuation\n- **Efficiency**: Streaming processing for large datasets\n\nThis interactive approach balances **automation** with **research flexibility**, allowing security practitioners to focus on their analysis objectives while the system handles the technical complexities of large-scale data extraction.\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_relevant_indices(es, keywords):\n",
    "    \"\"\"Retrieve indices containing keywords with storage size\"\"\"\n",
    "    try:\n",
    "        response = es.cat.indices(format=\"json\", h=\"index,store.size,creation.date\")\n",
    "        return [\n",
    "            {\n",
    "                \"name\": idx[\"index\"],\n",
    "                \"size\": idx.get(\"store.size\", \"0b\"),\n",
    "                \"created\": datetime.fromtimestamp(int(idx[\"creation.date\"])/1000, tz=timezone.utc)\n",
    "            }\n",
    "            for idx in response\n",
    "            if any(kw in idx[\"index\"] for kw in keywords)\n",
    "        ]\n",
    "    except Exception as e:\n",
    "        print(f\"üö® Error listing indices: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## üìù Summary & Next Steps\n\n### üéØ Stage 1 Completion Summary\n\nThis notebook successfully implements the **first stage** of the cybersecurity dataset creation pipeline, providing an interactive and user-friendly approach to extracting security telemetry from Elasticsearch clusters for machine learning research.\n\n#### ‚úÖ Key Accomplishments\n\n1. **üîå Robust Elasticsearch Integration**\n   - Simple yet reliable connection management for research environments\n   - Clear error handling and user feedback for troubleshooting\n   - Optimized for APT simulation data collection workflows\n\n2. **üîç Intelligent Data Discovery**\n   - Keyword-based filtering to identify cybersecurity-relevant indices\n   - Metadata presentation (size, creation date) for informed decision-making\n   - Scalable approach for clusters with hundreds of indices\n\n3. **üéÆ Interactive User Experience**\n   - Intuitive index selection with flexible input options\n   - Human-readable timestamp format for easy time range specification\n   - Real-time progress feedback during data extraction\n\n4. **üì• Efficient Data Extraction**\n   - Streaming JSONL output for memory-efficient processing\n   - Individual index error isolation to prevent workflow interruption\n   - Direct compatibility with downstream ML pipeline tools\n\n#### üìä Output Format\n\nThe extracted JSONL files contain structured cybersecurity events ready for the next pipeline stages:\n- **Windows Security Events**: Sysmon process creation, network connections, file operations\n- **Network Traffic Data**: DNS queries, HTTP requests, TLS handshakes, flow metadata\n- **Temporal Precision**: Microsecond-level timestamps for accurate event correlation\n- **Structured Fields**: JSON format preserving all original event metadata\n\n---\n\n### üîÑ Pipeline Continuity\n\nThe extracted JSONL files serve as input for the remaining pipeline stages:\n\n#### üìã Stage 2: Sysmon Dataset Creation (`2_elastic_sysmon-ds_csv_creator.ipynb`)\n- **Purpose**: Transform Windows Sysmon events into structured CSV format for ML training\n- **Input**: Sysmon JSONL files from this extraction\n- **Output**: Labeled CSV datasets with process behavior features\n\n#### üåê Stage 3: Network Flow Dataset Creation (`3_elastic_network-traffic-flow-ds_csv_creator.ipynb`)\n- **Purpose**: Process network traffic into flow-based statistical features\n- **Input**: Network traffic JSONL files from this extraction  \n- **Output**: Network flow CSV datasets with connection patterns\n\n#### üìã Stage 4: Caldera Report Analysis (`4_caldera-report-analyzer.ipynb`)\n- **Purpose**: Extract attack ground truth and TTPs mapping from simulation reports\n- **Input**: Caldera JSON reports + extracted security events\n- **Output**: Attack timeline correlation and ground truth labels\n\n---\n\n### üöÄ Usage Instructions\n\n#### For Security Researchers:\n1. **Configure** the Elasticsearch connection parameters for your environment\n2. **Run** the notebook cells sequentially to establish connection and discover indices\n3. **Select** relevant indices based on your research focus (Sysmon, network traffic, etc.)\n4. **Specify** time ranges corresponding to your attack simulation or investigation window\n5. **Monitor** extraction progress and verify output JSONL files\n\n#### For ML Practitioners:\n- The extracted JSONL files are immediately usable with pandas: `pd.read_json('file.jsonl', lines=True)`\n- Each line represents a single security event with standardized timestamp and metadata fields\n- Files can be processed in streaming fashion for large datasets that exceed memory capacity\n\n#### For Students & Educators:\n- This notebook demonstrates production-grade techniques for cybersecurity data collection\n- Interactive design allows hands-on learning about Elasticsearch, security data structures, and ETL processes\n- Code serves as reference implementation for distributed security data processing\n\n---\n\n### üìö Educational Value\n\nThis implementation showcases several important concepts:\n\n- **üîê Cybersecurity Data Engineering**: Practical techniques for handling real-world security telemetry\n- **üîç Distributed Search Systems**: Elasticsearch integration patterns for large-scale data retrieval  \n- **üéÆ Interactive Data Science**: User-guided workflows that balance automation with human decision-making\n- **üìä Streaming Data Processing**: Memory-efficient techniques for handling datasets larger than available RAM\n\nThe simple, well-commented code serves as an excellent foundation for understanding how to build robust data collection systems for cybersecurity machine learning research.\n\n---\n\n**üéâ Stage 1 Complete - Ready for Stage 2 Processing!**\n\n*The extracted JSONL files are now available for transformation into structured ML datasets in the next pipeline stage.*",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_indices_selector(indices):\n",
    "    \"\"\"Interactive index selection with pagination\"\"\"\n",
    "    print(f\"\\nüìÇ Found {len(indices)} relevant indices:\")\n",
    "    for i, idx in enumerate(indices, 1):\n",
    "        print(f\"{i:>3}. {idx['name']} ({idx['size']}) [Created: {idx['created'].strftime('%Y-%m-%d')}]\")\n",
    "\n",
    "    while True:\n",
    "        selection = input(\"\\nüî¢ Select indices (comma-separated numbers, 'all', or 'exit'): \").strip().lower()\n",
    "        \n",
    "        if selection == \"exit\":\n",
    "            return []\n",
    "        if selection == \"all\":\n",
    "            return [idx[\"name\"] for idx in indices]\n",
    "        \n",
    "        try:\n",
    "            selected_indices = [\n",
    "                indices[int(num)-1][\"name\"] \n",
    "                for num in selection.split(\",\") \n",
    "                if num.strip().isdigit()\n",
    "            ]\n",
    "            if selected_indices:\n",
    "                return list(set(selected_indices))  # Remove duplicates\n",
    "            print(\"‚ö†Ô∏è No valid selection. Please try again.\")\n",
    "        except (IndexError, ValueError):\n",
    "            print(\"‚õî Invalid input format. Use numbers separated by commas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_utc_time(time_str):  # Corrected function name\n",
    "    \"\"\"Parse time string into UTC datetime object\"\"\"\n",
    "    try:\n",
    "        # Strip any trailing timezone identifiers (we assume UTC)\n",
    "        time_str = time_str.split(\" (UTC)\")[0].strip()\n",
    "        return datetime.strptime(time_str, TIMESTAMP_FORMAT).replace(tzinfo=timezone.utc)\n",
    "    except ValueError as e:\n",
    "        print(f\"‚è∞ Time parsing error: {e}\")\n",
    "        print(f\"üìÖ Expected format: {TIMESTAMP_FORMAT} (UTC)\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_index_data(es, index_name, start_time, end_time):\n",
    "    \"\"\"Export index data within time range to JSONL file\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    safe_name = index_name.replace(\":\", \"_\").replace(\".\", \"-\")\n",
    "    filename = os.path.join(output_dir, f\"{safe_name}.jsonl\")\n",
    "    \n",
    "    query = {\n",
    "        \"query\": {\n",
    "            \"range\": {\n",
    "                \"@timestamp\": {\n",
    "                    \"gte\": start_time.isoformat(),\n",
    "                    \"lte\": end_time.isoformat(),\n",
    "                    \"format\": \"strict_date_optional_time\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        with open(filename, \"w\") as f:\n",
    "            count = 0\n",
    "            for hit in scan(es, index=index_name, query=query):\n",
    "                f.write(json.dumps(hit[\"_source\"]) + \"\\n\")\n",
    "                count += 1\n",
    "        print(f\"‚úÖ Success: {count} documents from {index_name} -> {filename}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to export {index_name}: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"\\nüîó Connecting to Elasticsearch...\")\n",
    "    es = connect_elasticsearch()\n",
    "    \n",
    "    if not test_connection(es):\n",
    "        print(\"üö® Could not establish connection to Elasticsearch\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nüîç Searching for relevant indices...\")\n",
    "    indices = list_relevant_indices(es, keywords)\n",
    "    \n",
    "    if not indices:\n",
    "        print(\"ü§∑ No matching indices found\")\n",
    "        return\n",
    "\n",
    "    selected_indices = display_indices_selector(indices)\n",
    "    if not selected_indices:\n",
    "        print(\"üö™ Exiting without download\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nüïí Time Range Selection (UTC)\")\n",
    "    print(\"üí° Example format: 'Jan 29, 2025 @ 04:24:54.863'\")\n",
    "    start_time = parse_utc_time(input(\"‚è±Ô∏è  Start time: \"))\n",
    "    end_time = parse_utc_time(input(\"‚è∞ End time: \"))\n",
    "    \n",
    "    if not all([start_time, end_time]):\n",
    "        print(\"‚õî Invalid time parameters\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n‚è≥ Starting data export...\")\n",
    "    for index in selected_indices:\n",
    "        export_index_data(es, index, start_time, end_time)\n",
    "\n",
    "    print(f'start time: {start_time}')\n",
    "    print(f'end time: {end_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîó Connecting to Elasticsearch...\n",
      "\n",
      "üîç Searching for relevant indices...\n",
      "\n",
      "üìÇ Found 13 relevant indices:\n",
      "  1. .ds-logs-network_traffic.dns-default-2025.03.10-000001 (114.2mb) [Created: 2025-03-10]\n",
      "  2. .ds-logs-network_traffic.tls-default-2025.03.10-000001 (52.7mb) [Created: 2025-03-10]\n",
      "  3. .ds-logs-network_traffic.icmp-default-2025.03.10-000001 (2mb) [Created: 2025-03-10]\n",
      "  4. .ds-logs-windows.sysmon_operational-default-2025.04.15-000002 (17.2gb) [Created: 2025-04-15]\n",
      "  5. .ds-logs-network_traffic.dhcpv4-default-2025.04.17-000001 (86.2kb) [Created: 2025-04-17]\n",
      "  6. .ds-logs-network_traffic.tls-default-2025.04.15-000002 (45.3mb) [Created: 2025-04-15]\n",
      "  7. .ds-logs-network_traffic.flow-default-2025.04.15-000002 (25.3gb) [Created: 2025-04-15]\n",
      "  8. .ds-logs-network_traffic.dns-default-2025.04.15-000002 (255.4mb) [Created: 2025-04-15]\n",
      "  9. .ds-logs-windows.sysmon_operational-default-2025.03.10-000001 (8.4gb) [Created: 2025-03-10]\n",
      " 10. .ds-logs-network_traffic.icmp-default-2025.04.15-000002 (7.6mb) [Created: 2025-04-15]\n",
      " 11. .ds-logs-network_traffic.http-default-2025.04.15-000002 (137.2mb) [Created: 2025-04-15]\n",
      " 12. .ds-logs-network_traffic.http-default-2025.03.10-000001 (30.1mb) [Created: 2025-03-10]\n",
      " 13. .ds-logs-network_traffic.flow-default-2025.03.10-000001 (4.8gb) [Created: 2025-03-10]\n",
      "\n",
      "üïí Time Range Selection (UTC)\n",
      "üí° Example format: 'Jan 29, 2025 @ 04:24:54.863'\n",
      "\n",
      "‚è≥ Starting data export...\n",
      "‚úÖ Success: 0 documents from .ds-logs-network_traffic.dns-default-2025.03.10-000001 -> ./-ds-logs-network_traffic-dns-default-2025-03-10-000001.jsonl\n",
      "‚úÖ Success: 0 documents from .ds-logs-network_traffic.tls-default-2025.03.10-000001 -> ./-ds-logs-network_traffic-tls-default-2025-03-10-000001.jsonl\n",
      "‚úÖ Success: 0 documents from .ds-logs-network_traffic.icmp-default-2025.03.10-000001 -> ./-ds-logs-network_traffic-icmp-default-2025-03-10-000001.jsonl\n",
      "‚úÖ Success: 570078 documents from .ds-logs-windows.sysmon_operational-default-2025.04.15-000002 -> ./-ds-logs-windows-sysmon_operational-default-2025-04-15-000002.jsonl\n",
      "‚úÖ Success: 0 documents from .ds-logs-network_traffic.dhcpv4-default-2025.04.17-000001 -> ./-ds-logs-network_traffic-dhcpv4-default-2025-04-17-000001.jsonl\n",
      "‚úÖ Success: 293 documents from .ds-logs-network_traffic.tls-default-2025.04.15-000002 -> ./-ds-logs-network_traffic-tls-default-2025-04-15-000002.jsonl\n",
      "‚úÖ Success: 1090212 documents from .ds-logs-network_traffic.flow-default-2025.04.15-000002 -> ./-ds-logs-network_traffic-flow-default-2025-04-15-000002.jsonl\n",
      "‚úÖ Success: 4928 documents from .ds-logs-network_traffic.dns-default-2025.04.15-000002 -> ./-ds-logs-network_traffic-dns-default-2025-04-15-000002.jsonl\n",
      "‚úÖ Success: 0 documents from .ds-logs-windows.sysmon_operational-default-2025.03.10-000001 -> ./-ds-logs-windows-sysmon_operational-default-2025-03-10-000001.jsonl\n",
      "‚úÖ Success: 249 documents from .ds-logs-network_traffic.icmp-default-2025.04.15-000002 -> ./-ds-logs-network_traffic-icmp-default-2025-04-15-000002.jsonl\n",
      "‚úÖ Success: 193 documents from .ds-logs-network_traffic.http-default-2025.04.15-000002 -> ./-ds-logs-network_traffic-http-default-2025-04-15-000002.jsonl\n",
      "‚úÖ Success: 0 documents from .ds-logs-network_traffic.http-default-2025.03.10-000001 -> ./-ds-logs-network_traffic-http-default-2025-03-10-000001.jsonl\n",
      "‚úÖ Success: 0 documents from .ds-logs-network_traffic.flow-default-2025.03.10-000001 -> ./-ds-logs-network_traffic-flow-default-2025-03-10-000001.jsonl\n",
      "start time: 2025-05-04 11:30:00+00:00\n",
      "end time: 2025-05-04 12:40:00+00:00\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataset-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}